<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/G128.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/G32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/G16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"njughr.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="PaperTCiteAbstractKeywords &#x2F; Index TermsCommentsPaperLink">
<meta property="og:type" content="article">
<meta property="og:title" content="Pipes">
<meta property="og:url" content="https://njughr.github.io/2021/10/29/papers/index.html">
<meta property="og:site_name" content="GHR&#39;s Blog">
<meta property="og:description" content="PaperTCiteAbstractKeywords &#x2F; Index TermsCommentsPaperLink">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://njughr.github.io/2021/10/29/papers/NanoPutian.png">
<meta property="og:image" content="https://loop.frontiersin.org/images/profile/1228973/24">
<meta property="og:image" content="https://loop.frontiersin.org/images/profile/1229999/24">
<meta property="og:image" content="https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg">
<meta property="og:image" content="https://njughr.github.io/2021/10/29/papers/1.png">
<meta property="og:image" content="https://njughr.github.io/2021/10/29/papers/qiufen.jpg">
<meta property="article:published_time" content="2021-10-29T11:35:57.000Z">
<meta property="article:modified_time" content="2022-02-07T05:26:05.091Z">
<meta property="article:author" content="Haoran Geng">
<meta property="article:tag" content="ic">
<meta property="article:tag" content="cs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://njughr.github.io/2021/10/29/papers/NanoPutian.png">

<link rel="canonical" href="https://njughr.github.io/2021/10/29/papers/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Pipes | GHR's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">GHR's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">骐骥一跃，不能十步<br>驽马十驾，功在不舍</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">22</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">19</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://njughr.github.io/2021/10/29/papers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/paperboat.jpg">
      <meta itemprop="name" content="Haoran Geng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GHR's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pipes
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-29 19:35:57" itemprop="dateCreated datePublished" datetime="2021-10-29T19:35:57+08:00">2021-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-07 13:26:05" itemprop="dateModified" datetime="2022-02-07T13:26:05+08:00">2022-02-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="PaperT"><a href="#PaperT" class="headerlink" title="PaperT"></a>PaperT</h1><h2 id="Cite"><a href="#Cite" class="headerlink" title="Cite"></a>Cite</h2><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h2 id="Keywords-Index-Terms"><a href="#Keywords-Index-Terms" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><h2 id="PaperLink"><a href="#PaperLink" class="headerlink" title="PaperLink"></a>PaperLink</h2> <span id="more"></span>
<hr>
<p>尚需补充的：</p>
<p>Paper98-0207，可以参考<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/393850095/answer/1531276843">怎么查所在领域的优秀课题组? - mengyi sun的回答 - 知乎</a></p>
<p>Paper97-0206，可以参考<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1QY411b7QY?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click">剪影视错觉是什么？可凭意念操纵舞女旋转方向，这是什么原理？</a></p>
<p>Paper96-0205，可以参考<a target="_blank" rel="noopener" href="https://www.163.com/dy/article/F2OQJIRQ05329456.html">聚焦腹侧苍白球，冷泉港李波团队揭秘大脑平衡压力和疼痛的新机制</a>，这个可以作为效率心理学的理论基础之一。</p>
<p>Paper95-0204，知网搜索《影响情绪的化学物质》</p>
<p>Paper94-0203</p>
<p>Paper93-0202</p>
<p>Paper90-0130</p>
<p>Paper89-0129</p>
<p>Paper88-0128，可以参考<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/376039235/answer/1162845603">有哪些不易察觉的错误证明？ - Ephraim的回答 - 知乎</a></p>
<p>Paper87-0127</p>
<p>Paper86-0126，可以参考<a target="_blank" rel="noopener" href="https://ese.nju.edu.cn/83/00/c22537a557824/page.htm">电子学院邢钟文教授和物理学院李绍春教授等发现反常超导邻近效应</a></p>
<p>Paper85-0125，可以参考<a target="_blank" rel="noopener" href="https://ese.nju.edu.cn/65/7c/c22537a550268/page.htm">南京大学电子学院研究成功超导单探测器光谱仪</a></p>
<p>Paper77-0117</p>
<p>Paper59-1225</p>
<p>Paper58-1224</p>
<p>Paper56-1222</p>
<p>Paper55-1221</p>
<p>Paper54-1220</p>
<p>Paper48-1215</p>
<p>Paper40-1210</p>
<p>Paper38-1208</p>
<p>Paper33-1203</p>
<p>Paper32-1114</p>
<p>Paper31-1129</p>
<p>Paper26-1112</p>
<p>Paper25-1117</p>
<p>Paper22-1116</p>
<h1 id="Paper92-0201"><a href="#Paper92-0201" class="headerlink" title="Paper92-0201"></a>Paper92-0201</h1><h2 id="Cite-1"><a href="#Cite-1" class="headerlink" title="Cite"></a>Cite</h2><p>王思北,任沁沁,高蕾. 团结奋斗书写新的美好未来[N]. 人民日报,2022-02-01(001).</p>
<h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p>“围绕明确奋斗目标形成的团结才是最牢固的团结，依靠紧密团结进行的奋斗才是最有力的奋斗。我们靠团结奋斗创造了辉煌历史，还要靠团结奋斗开辟美好未来。”习近平总书记1月30日在春节团拜会上的重要讲话，激荡起全党全国各族人民同心协力书写美好未来的豪情和信心。</p>
<h2 id="Keywords-Index-Terms-1"><a href="#Keywords-Index-Terms-1" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>人民幸福；习近平总书记</p>
<h2 id="Comments-1"><a href="#Comments-1" class="headerlink" title="Comments"></a>Comments</h2><p><strong>以下为摘抄</strong></p>
<p>团结就是力量，奋斗开创未来。</p>
<p>对百年奋斗历史最好的致敬，是书写新的奋斗历史。未来属于青年，希望寄予青年。新的奋斗历史正等着我们去书写。不负时代，不负韶华。激昂青春，激荡梦想。</p>
<p>巍巍蒙山高，亲亲沂水长。美丽沂蒙以每一寸红色热土诉说“中国共产党根基在人民、血脉在人民、力量在人民”。</p>
<p>此时此刻，中国特色社会主义伟大事业的历史新篇章正在徐徐展开。让我们手拉着手、心连着心，一起开辟美好未来，在新的赶考之路上继续创造令人刮目相看的奇迹！</p>
<h2 id="PaperLink-1"><a href="#PaperLink-1" class="headerlink" title="PaperLink"></a>PaperLink</h2><p><a target="_blank" rel="noopener" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CCND&amp;dbname=CCNDTEMPDAY&amp;filename=RMRB202202010014&amp;uniplatform=NZKPT&amp;v=LUc_SYAsz8CE3XPDvFDtNjiuyNSwg-KdxyWGnro7A07vxfEDQKE263_DUjcQFPoelvAN4pYV07Q%3d">团结奋斗书写新的美好未来</a></p>
<hr>
<h1 id="Paper91-0131"><a href="#Paper91-0131" class="headerlink" title="Paper91-0131"></a>Paper91-0131</h1><h2 id="Cite-2"><a href="#Cite-2" class="headerlink" title="Cite"></a>Cite</h2><p>习近平. 在二〇二二年春节团拜会上的讲话[N]. 人民日报,2022-01-31(001).</p>
<h2 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h2><p>同志们，朋友们：在农历壬寅虎年春节即将到来之际，我们在这里欢聚一堂，辞旧岁迎新春，感到格外高兴。首先，我代表党中央和国务院，向大家致以节日的美好祝福！向全国各族人民，向香港特别行政区同胞、澳门特别行政区同胞、台湾同胞和海外侨胞拜年！祝大家新春愉快！</p>
<h2 id="Keywords-Index-Terms-2"><a href="#Keywords-Index-Terms-2" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>春节团拜会；疫情防控；奋斗目标；共产党</p>
<h2 id="Comments-2"><a href="#Comments-2" class="headerlink" title="Comments"></a>Comments</h2><p><strong>身体健康，万事如意，阖家欢乐，虎年大吉！</strong></p>
<h2 id="PaperLink-2"><a href="#PaperLink-2" class="headerlink" title="PaperLink"></a>PaperLink</h2><p><a target="_blank" rel="noopener" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CCND&amp;dbname=CCNDTEMPDAY&amp;filename=RMRB202201310012&amp;uniplatform=NZKPT&amp;v=LUc_SYAsz8DoNHb0t9GIfd9IYuTgue7WMavSrBe-PtZWAZF6O0zKb2EC_iCUQCoh2TzPFpdxUN8%3d">在二〇二二年春节团拜会上的讲话</a></p>
<hr>
<h1 id="Paper84-0119"><a href="#Paper84-0119" class="headerlink" title="Paper84-0119"></a>Paper84-0119</h1><h2 id="Cite-3"><a href="#Cite-3" class="headerlink" title="Cite"></a>Cite</h2><p>Balaji A, Adiraju P, Kashyap HJ, Das A, Krichmar JL, Dutt ND, Catthoor F. PyCARL: A PyNN interface for hardware-software co-simulation of spiking neural network. arXiv preprint arXiv:2003.09696. 2020 Mar 21.</p>
<h2 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h2><p>We present PyCARL, a PyNN-based common Python programming interface for hardware-software co-simulation of spiking neural network (SNN). Through PyCARL, we make the following two key contributions. First, we provide an interface of PyNN to CARLsim, a computationally-efficient, GPU-accelerated and biophysically-detailed SNN simulator. PyCARL facilitates joint development of machine learning models and code sharing between CARLsim and PyNN users, promoting an integrated and larger neuromorphic community. Second, we integrate cycle-accurate models of state-of-the-art neuromorphic hardware such as TrueNorth, Loihi, and DynapSE in PyCARL, to accurately model hardware latencies that delay spikes between communicating neurons and degrade performance. PyCARL allows users to analyze and optimize the performance difference between software-only simulation and hardware-software co-simulation of their machine learning models. We show that system designers can also use PyCARL to perform design-space exploration early in the product development stage, facilitating faster time-to-deployment of neuromorphic products. We evaluate the memory usage and simulation time of PyCARL using functionality tests, synthetic SNNs, and realistic applications. Our results demonstrate that for large SNNs, PyCARL does not lead to any significant overhead compared to CARLsim. We also use PyCARL to analyze these SNNs for a state-of-the-art neuromorphic hardware and demonstrate a significant performance deviation from software-only simulations. PyCARL allows to evaluate and minimize such differences early during model development.</p>
<p>我们提出了 PyCARL，一个基于 PyNN 的通用 Python 编程接口，用于尖峰神经网络 (SNN) 的硬件-软件协同仿真。通过 PyCARL，我们做出了以下两个关键贡献。首先，我们提供了一个 PyNN 到 CARLsim 的接口，CARLsim 是一个计算效率高、GPU 加速和生物物理详细的 SNN 模拟器。 PyCARL 促进了机器学习模型的联合开发和 CARLsim 和 PyNN 用户之间的代码共享，促进了一个集成的和更大的神经形态社区。其次，我们在 PyCARL 中集成了最先进的神经形态硬件（如 TrueNorth、Loihi 和 DynapSE）的周期精确模型，以准确模拟延迟通信神经元之间的尖峰并降低性能的硬件延迟。 PyCARL 允许用户分析和优化其机器学习模型的纯软件仿真和硬件-软件协同仿真之间的性能差异。我们表明，系统设计人员还可以在产品开发阶段的早期使用 PyCARL 进行设计空间探索，从而加快神经形态产品的部署时间。我们使用功能测试、合成 SNN 和实际应用程序评估 PyCARL 的内存使用和模拟时间。我们的结果表明，对于大型 SNN，与 CARLsim 相比，PyCARL 不会导致任何重大开销。我们还使用 PyCARL 分析这些 SNN 以获得最先进的神经形态硬件，并展示与纯软件模拟的显着性能偏差。 PyCARL 允许在模型开发的早期评估和最小化这些差异。</p>
<h2 id="Comments-3"><a href="#Comments-3" class="headerlink" title="Comments"></a>Comments</h2><p>本文有28个引用。</p>
<h2 id="PaperLink-3"><a href="#PaperLink-3" class="headerlink" title="PaperLink"></a>PaperLink</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.09696">PyCARL: A PyNN Interface for Hardware-Software Co-Simulation of Spiking Neural Network</a></p>
<hr>
<h1 id="Paper83-0120"><a href="#Paper83-0120" class="headerlink" title="Paper83-0120"></a>Paper83-0120</h1><h2 id="Cite-4"><a href="#Cite-4" class="headerlink" title="Cite"></a>Cite</h2><p>Xiang S, Zhang Y, Gong J, Guo X, Lin L, Hao Y. STDP-based unsupervised spike pattern learning in a photonic spiking neural network with VCSELs and VCSOAs. IEEE Journal of Selected Topics in Quantum Electronics. 2019 Apr 17;25(6):1-9.</p>
<h2 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h2><p>We propose a photonic spiking neural network (SNN) consisting of photonic spiking neurons based on vertical-cavity surface-emitting lasers (VCSELs). The photonic spike timing dependent plasticity (STDP) is implemented in a vertical-cavity semiconductor optical amplifier (VCSOA). A versatile computational model of the photonic SNN is presented based on the rate equation models. Through numerical simulation, a spike pattern learning and recognition task is performed based on the photonic STDP. The results show that the post-synaptic spike timing (PST) is eventually converged iteratively to the first spike timing of the input spike pattern via unsupervised learning. Additionally, the convergence rate of the PST can be accelerated for a photonic SNN with more pre-synaptic neurons. The effects of VCSOA parameters on the convergence performance of the unsupervised spike learning are also considered. To the best of our knowledge, such a versatile computational model of photonic SNN for unsupervised learning and recognition of arbitrary spike pattern has not yet been reported, which would contribute one step forward toward numerical implementation of a large-scale energy-efficient photonic SNN, and hence is interesting for neuromorphic photonic systems and spiking information processing.</p>
<p>我们提出了一种光子脉冲神经网络 (SNN)，该网络由基于垂直腔面发射激光器 (VCSEL) 的光子脉冲神经元组成。光子尖峰时间相关可塑性 (STDP) 在垂直腔半导体光放大器 (VCSOA) 中实现。基于速率方程模型提出了一种通用的光子 SNN 计算模型。通过数值模拟，基于光子STDP执行尖峰模式学习和识别任务。结果表明，突触后尖峰时间（PST）最终通过无监督学习迭代收敛到输入尖峰模式的第一个尖峰时间。此外，对于具有更多突触前神经元的光子 SNN，可以加快 PST 的收敛速度。还考虑了 VCSOA 参数对无监督尖峰学习收敛性能的影响。据我们所知，这种用于无监督学习和任意尖峰模式识别的光子 SNN 通用计算模型尚未被报道，这将有助于朝着大规模节能光子 SNN 的数值实现迈出一步，因此对于神经形态光子系统和尖峰信息处理很有趣。</p>
<h2 id="Comments-4"><a href="#Comments-4" class="headerlink" title="Comments"></a>Comments</h2><p>本文有56个引用。</p>
<h2 id="PaperLink-4"><a href="#PaperLink-4" class="headerlink" title="PaperLink"></a>PaperLink</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8693533">STDP-Based Unsupervised Spike Pattern Learning in a Photonic Spiking Neural Network With VCSELs and VCSOAs</a></p>
<hr>
<h1 id="Paper82-0121"><a href="#Paper82-0121" class="headerlink" title="Paper82-0121"></a>Paper82-0121</h1><h2 id="Cite-5"><a href="#Cite-5" class="headerlink" title="Cite"></a>Cite</h2><p>Andreev AV, Ivanchenko MV, Pisarchik AN, Hramov AE. Stimulus classification using chimera-like states in a spiking neural network. Chaos, Solitons &amp; Fractals. 2020 Oct 1;139:110061.</p>
<h2 id="Abstract-5"><a href="#Abstract-5" class="headerlink" title="Abstract"></a>Abstract</h2><p>A complex network of bistable Hodgkin-Huxley (HH) neurons with excitatory coupling can exhibit a partially spiking chimera behavior. We propose to use this chimera-like state for classification of the entering stimulus amplitude in the neural network with coexisting resting and spiking states. Due to different additive noise applied to each neuron in the network, the neurons are nonidentical. Therefore, depending on the amplitude of the external current, a part of the neurons stays in the resting state, while another part oscillates. Keeping fixed the coupling strength between neurons inside the network, we train the neural network on external pulses with two different amplitudes to adjust the coupling strength between the network neurons and two output neurons. We consider two variants of the classifier, in the presence and in the absence of inhibitory coupling between output neurons, and study how the output neurons respond to the external pulses of different amplitudes. The accuracy of the proposed classifier reaches 100% when the output neurons are inhibitory coupled, so that only one of these neurons is activated.</p>
<p>具有兴奋性耦合的双稳态霍奇金-赫胥黎 (HH) 神经元的复杂网络可以表现出部分尖峰嵌合行为。我们建议使用这种嵌合体状态来对神经网络中的进入刺激幅度进行分类，并具有共存的静息和尖峰状态。由于不同的加性噪声应用于网络中的每个神经元，神经元是不相同的。因此，根据外部电流的幅度，一部分神经元处于静止状态，而另一部分则在振荡。保持网络内部神经元之间的耦合强度不变，我们在两个不同幅度的外部脉冲上训练神经网络，以调整网络神经元和两个输出神经元之间的耦合强度。我们考虑分类器的两种变体，在输出神经元之间存在和不存在抑制耦合的情况下，并研究输出神经元如何响应不同幅度的外部脉冲。当输出神经元被抑制耦合时，所提出的分类器的准确率达到 100%，因此这些神经元中只有一个被激活。</p>
<h2 id="Highlights"><a href="#Highlights" class="headerlink" title="Highlights"></a>Highlights</h2><ul>
<li>The threshold classifier based on the chimera-like states in the network of bistable Hodgkin-Huxley neurons allows the classification of amplitudes of the external pulsed current applied to the neural network.</li>
<li>基于双稳态霍奇金-赫胥黎神经元网络中的嵌合体状态的阈值分类器允许对施加到神经网络的外部脉冲电流的幅度进行分类。</li>
<li>The classifier is trained on two external pulses with different amplitudes to adapt the coupling strength of the main network with the output neurons, so that one of the output neurons is activated by the lower-amplitude pulse, whereas another neuron is activated by the higher-amplitude pulse.</li>
<li>分类器在两个不同幅度的外部脉冲上进行训练，以适应主网络与输出神经元的耦合强度，使得一个输出神经元被较低幅度的脉冲激活，而另一个神经元被较高幅度的脉冲激活-幅度脉冲。</li>
<li>For both variants of the classifier, with the presence of inhibitory coupling between output neurons and without it, there is a threshold value for classification of the pulse amplitude. The inhibitory coupling decreases the range of uncertainty in the amplitude threshold classification.</li>
<li>对于分类器的两种变体，在输出神经元之间存在和不存在抑制耦合的情况下，存在用于分类脉冲幅度的阈值。抑制耦合减小了幅度阈值分类中的不确定性范围。</li>
<li>The proposed classifier can achieve 100% accuracy in a wide range of the external pulsed current amplitudes. The uncertainty in classification is observed within a very narrow range where both output neurons generate equal number of spikes. Such uncertainty in decision making has analogy with human perception of the bistable Necker cube which can be equally interpreted as left- or right-oriented.</li>
<li>所提出的分类器可以在很宽的外部脉冲电流幅度范围内实现 100% 的准确度。在两个输出神经元产生相同数量的尖峰的非常窄的范围内观察到分类的不确定性。决策中的这种不确定性类似于人类对双稳态内克尔立方体的感知，它可以同样被解释为向左或向右。</li>
</ul>
<h2 id="Comments-5"><a href="#Comments-5" class="headerlink" title="Comments"></a>Comments</h2><p>本文有22个引用。</p>
<h2 id="PaperLink-5"><a href="#PaperLink-5" class="headerlink" title="PaperLink"></a>PaperLink</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0960077920304586">Stimulus classification using chimera-like states in a spiking neural network</a></p>
<hr>
<h1 id="Paper81-0122"><a href="#Paper81-0122" class="headerlink" title="Paper81-0122"></a>Paper81-0122</h1><h2 id="Cite-6"><a href="#Cite-6" class="headerlink" title="Cite"></a>Cite</h2><p>Han B, Srinivasan G, Roy K. Rmp-snn: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2020 (pp. 13558-13567).</p>
<h2 id="Abstract-6"><a href="#Abstract-6" class="headerlink" title="Abstract"></a>Abstract</h2><p>Spiking Neural Networks (SNNs) have recently attracted significant research interest as the third generation of artificial neural networks that can enable low-power event-driven data analytics. The best performing SNNs for image recognition tasks are obtained by converting a trained Analog Neural Network (ANN), consisting of Rectified Linear Units (ReLU), to SNN composed of integrate-and-fire neurons with “proper” firing thresholds. The converted SNNs typically incur loss in accuracy compared to that provided by the original ANN and require sizable number of inference time-steps to achieve the best accuracy. We find that performance degradation in the converted SNN stems from using “hard reset” spiking neuron that is driven to fixed reset potential once its membrane potential exceeds the firing threshold, leading to information loss during SNN inference. We propose ANN-SNN conversion using “soft reset” spiking neuron model, referred to as Residual Membrane Potential (RMP) spiking neuron, which retains the “residual” membrane potential above threshold at the firing instants. We demonstrate near loss-less ANN-SNN conversion using RMP neurons for VGG-16, ResNet-20, and ResNet-34 SNNs on challenging datasets including CIFAR-10 (93.63% top-1), CIFAR-100 (70.93% top-1), and ImageNet (73.09% top-1 accuracy). Our results also show that RMP-SNN surpasses the best inference accuracy provided by the converted SNN with “hard reset” spiking neurons using 2-8 times fewer inference time-steps across network architectures and datasets.</p>
<p>脉冲神经网络 (SNN) 作为第三代人工神经网络，可以实现低功耗事件驱动的数据分析，最近引起了极大的研究兴趣。通过将由整流线性单元 (ReLU) 组成的训练有素的模拟神经网络 (ANN) 转换为由具有“适当”触发阈值的集成和触发神经元组成的 SNN，可以获得用于图像识别任务的最佳性能 SNN。与原始 ANN 提供的相比，转换后的 SNN 通常会导致精度损失，并且需要大量的推理时间步长才能达到最佳精度。我们发现，转换后的 SNN 的性能下降源于使用“硬重置”尖峰神经元，一旦其膜电位超过触发阈值，该神经元就会被驱动到固定的重置电位，从而导致 SNN 推理过程中的信息丢失。我们建议使用“软重置”尖峰神经元模型进行 ANN-SNN 转换，称为残余膜电位 (RMP) 尖峰神经元，该模型在触发瞬间将“残余”膜电位保持在阈值以上。我们在具有挑战性的数据集（包括 CIFAR-10（93.63% top-1）、CIFAR-100（70.93% top- 1）和 ImageNet（73.09% 的 top-1 准确率）。我们的结果还表明，RMP-SNN 超过了转换后的 SNN 提供的最佳推理精度，其中“硬重置”尖峰神经元使用跨网络架构和数据集的推理时间步长减少了 2-8 倍。</p>
<h2 id="Comments-6"><a href="#Comments-6" class="headerlink" title="Comments"></a>Comments</h2><p>本文有44个引用。</p>
<h2 id="PaperLink-6"><a href="#PaperLink-6" class="headerlink" title="PaperLink"></a>PaperLink</h2><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Han_RMP-SNN_Residual_Membrane_Potential_Neuron_for_Enabling_Deeper_High-Accuracy_and_CVPR_2020_paper.html">RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-Latency Spiking Neural Network</a></p>
<hr>
<h1 id="Paper80-0123"><a href="#Paper80-0123" class="headerlink" title="Paper80-0123"></a>Paper80-0123</h1><h2 id="Cite-7"><a href="#Cite-7" class="headerlink" title="Cite"></a>Cite</h2><p>Kim S, Park S, Na B, Yoon S. Spiking-YOLO: Spiking neural network for energy-efficient object detection. InProceedings of the AAAI Conference on Artificial Intelligence 2020 Apr 3 (Vol. 34, No. 07, pp. 11270-11277).</p>
<h2 id="Abstract-7"><a href="#Abstract-7" class="headerlink" title="Abstract"></a>Abstract</h2><p>Over the past decade, deep neural networks (DNNs) have demonstrated remarkable performance in a variety of applications. As we try to solve more advanced problems, increasing demands for computing and power resources has become inevitable. Spiking neural networks (SNNs) have attracted widespread interest as the third-generation of neural networks due to their event-driven and low-powered nature. SNNs, however, are difficult to train, mainly owing to their complex dynamics of neurons and non-differentiable spike operations. Furthermore, their applications have been limited to relatively simple tasks such as image classification. In this study, we investigate the performance degradation of SNNs in a more challenging regression problem (i.e., object detection). Through our in-depth analysis, we introduce two novel methods: channel-wise normalization and signed neuron with imbalanced threshold, both of which provide fast and accurate information transmission for deep SNNs. Consequently, we present a first spiked-based object detection model, called Spiking-YOLO. Our experiments show that Spiking-YOLO achieves remarkable results that are comparable (up to 98%) to those of Tiny YOLO on non-trivial datasets, PASCAL VOC and MS COCO. Furthermore, Spiking-YOLO on a neuromorphic chip consumes approximately 280 times less energy than Tiny YOLO and converges 2.3 to 4 times faster than previous SNN conversion methods.</p>
<p>在过去的十年中，深度神经网络 (DNN) 在各种应用中都表现出了卓越的性能。随着我们试图解决更高级的问题，对计算和电力资源的需求不断增加已成为必然。脉冲神经网络 (SNN) 作为第三代神经网络，由于其事件驱动和低功耗的特性而引起了广泛的兴趣。然而，SNN 难以训练，主要是由于它们复杂的神经元动力学和不可微分的尖峰操作。此外，它们的应用仅限于相对简单的任务，例如图像分类。在这项研究中，我们研究了 SNN 在更具挑战性的回归问题（即对象检测）中的性能下降。通过我们的深入分析，我们介绍了两种新颖的方法：通道归一化和具有不平衡阈值的符号神经元，这两种方法都为深度 SNN 提供了快速准确的信息传输。因此，我们提出了第一个基于尖峰的目标检测模型，称为 Spiking-YOLO。我们的实验表明，Spiking-YOLO 在非平凡数据集、PASCAL VOC 和 MS COCO 上取得了与 Tiny YOLO 相当（高达 98%）的显着结果。此外，神经形态芯片上的 Spiking-YOLO 消耗的能量大约比 Tiny YOLO 少 280 倍，收敛速度比以前的 SNN 转换方法快 2.3 到 4 倍。</p>
<h2 id="Comments-7"><a href="#Comments-7" class="headerlink" title="Comments"></a>Comments</h2><p>本文有62个引用。</p>
<h2 id="PaperLink-7"><a href="#PaperLink-7" class="headerlink" title="PaperLink"></a>PaperLink</h2><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/6787">Spiking-YOLO: Spiking Neural Network for Energy-Efficient Object Detection</a></p>
<hr>
<h1 id="Paper79-0124"><a href="#Paper79-0124" class="headerlink" title="Paper79-0124"></a>Paper79-0124</h1><h2 id="Cite-8"><a href="#Cite-8" class="headerlink" title="Cite"></a>Cite</h2><p>Wang W, Pedretti G, Milo V, Carboni R, Calderoni A, Ramaswamy N, Spinelli AS, Ielmini D. Learning of spatiotemporal patterns in a spiking neural network with resistive switching synapses. Science advances. 2018 Sep 1;4(9):eaat4752.</p>
<h2 id="Abstract-8"><a href="#Abstract-8" class="headerlink" title="Abstract"></a>Abstract</h2><p>The human brain is a complex integrated spatiotemporal system, where space (which neuron fires) and time (when a neuron fires) both carry information to be processed by cognitive functions. To parallel the energy efficiency and computing functionality of the brain, methodologies operating over both the space and time domains are thus essential. Implementing spatiotemporal functions within nanoscale devices capable of synaptic plasticity would contribute a significant step toward constructing a large-scale neuromorphic system that emulates the computing and energy performances of the human brain. We present a neuromorphic approach to brain-like spatiotemporal computing using resistive switching synapses. To process the spatiotemporal spike pattern, time-coded spikes are reshaped into exponentially decaying signals that are fed to a McCulloch-Pitts neuron. Recognition of spike sequences is demonstrated after supervised training of a multiple-neuron network with resistive switching synapses. Finally, we show that, due to the sensitivity to precise spike timing, the spatiotemporal neural network is able to mimic the sound azimuth detection of the human brain.</p>
<p>人脑是一个复杂的综合时空系统，其中空间（神经元触发）和时间（神经元触发时）都承载着要由认知功能处理的信息。为了并行处理大脑的能量效率和计算功能，在空间和时间域上运行的方法因此是必不可少的。在具有突触可塑性的纳米级设备中实现时空功能将有助于朝着构建模拟人脑计算和能量性能的大规模神经形态系统迈出重要一步。我们提出了一种使用电阻切换突触的类脑时空计算的神经形态方法。为了处理时空尖峰模式，时间编码的尖峰被重新塑造成指数衰减的信号，这些信号被馈送到 McCulloch-Pitts 神经元。在对具有电阻切换突触的多神经元网络进行监督训练后，证明了对尖峰序列的识别。最后，我们表明，由于对精确尖峰时间的敏感性，时空神经网络能够模拟人脑的声音方位角检测。</p>
<h2 id="Comments-8"><a href="#Comments-8" class="headerlink" title="Comments"></a>Comments</h2><p>直接在Google Scholar上搜spiking neural network，然后Since 2018开始慢慢看。</p>
<p>发现一个问题：这篇博文的长度有点过长了，Typora翻页的时候有点卡顿。考虑在Paper100的时候开新档。</p>
<p>本文有118个引用。</p>
<h2 id="PaperLink-8"><a href="#PaperLink-8" class="headerlink" title="PaperLink"></a>PaperLink</h2><p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/sciadv.aat4752">Learning of spatiotemporal patterns in a spiking neural network with resistive switching synapses</a></p>
<hr>
<h1 id="Paper78-0118"><a href="#Paper78-0118" class="headerlink" title="Paper78-0118"></a>Paper78-0118</h1><h2 id="Published-in"><a href="#Published-in" class="headerlink" title="Published in"></a>Published in</h2><p>Bochkovskiy A, Wang CY, Liao HY. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934. 2020 Apr 23.</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.10934">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></p>
<h2 id="Authors"><a href="#Authors" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bochkovskiy%2C+A">Alexey Bochkovskiy</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+C">Chien-Yao Wang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liao%2C+H+M">Hong-Yuan Mark Liao</a></p>
<p>没有YOLO之父Joseph Redmon，这个v4的引用也“只”有不到3000次。</p>
<h2 id="Abstract-9"><a href="#Abstract-9" class="headerlink" title="Abstract"></a>Abstract</h2><p>There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100. Source code is at <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet">this https URL</a></p>
<p>据说有大量功能可以提高卷积神经网络 (CNN) 的准确性。需要在大型数据集上对这些特征的组合进行实际测试，并对结果进行理论证明。某些功能专门针对某些模型和某些问题专门操作，或仅针对小规模数据集；而一些特性，例如批量归一化和残差连接，适用于大多数模型、任务和数据集。我们假设这些通用特征包括加权残差连接 (WRC)、跨阶段部分连接 (CSP)、交叉小批量标准化 (CmBN)、自我对抗训练 (SAT) 和 Mish 激活。我们使用新功能：WRC、CSP、CmBN、SAT、Mish 激活、Mosaic 数据增强、CmBN、DropBlock 正则化和 CIoU 损失，并结合其中一些以实现最先进的结果：43.5% AP (65.7 % AP50) 用于 MS COCO 数据集，在 Tesla V100 上以约 65 FPS 的实时速度。源代码在 <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet">this https URL</a></p>
<h2 id="Comments-9"><a href="#Comments-9" class="headerlink" title="Comments"></a>Comments</h2><p><strong>YOLO v4!</strong></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>期刊档次注意看中科院发布的《国际期刊预警名单》。</p>
<hr>
<h2 id="Published-in-1"><a href="#Published-in-1" class="headerlink" title="Published in"></a>Published in</h2><p>Redmon J, Farhadi A. YOLO9000: better, faster, stronger. InProceedings of the IEEE conference on computer vision and pattern recognition 2017 (pp. 7263-7271).</p>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.08242">YOLO9000: Better, Faster, Stronger</a></p>
<h2 id="Authors-1"><a href="#Authors-1" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Redmon%2C+J">Joseph Redmon</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Farhadi%2C+A">Ali Farhadi</a></p>
<h2 id="Abstract-10"><a href="#Abstract-10" class="headerlink" title="Abstract"></a>Abstract</h2><p>We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don’t have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.</p>
<p>我们介绍了 YOLO9000，这是一种最先进的实时对象检测系统，可以检测超过 9000 个对象类别。首先，我们提出了对 YOLO 检测方法的各种改进，无论是新颖的还是从先前的工作中汲取的。改进后的模型 YOLOv2 在标准检测任务（如 PASCAL VOC 和 COCO）上是最先进的。在 67 FPS 时，YOLOv2 在 VOC 2007 上获得 76.8 mAP。在 40 FPS 时，YOLOv2 获得 78.6 mAP，优于使用 ResNet 和 SSD 的 Faster RCNN 等最先进的方法，同时仍然运行得更快。最后，我们提出了一种联合训练对象检测和分类的方法。使用这种方法，我们在 COCO 检测数据集和 ImageNet 分类数据集上同时训练 YOLO9000。我们的联合训练允许 YOLO9000 预测没有标记检测数据的对象类的检测。我们在 ImageNet 检测任务上验证了我们的方法。 YOLO9000 在 ImageNet 检测验证集上获得 19.7 mAP，尽管只有 200 个类别中的 44 个类别的检测数据。在 COCO 以外的 156 个类上，YOLO9000 获得 16.0 mAP。但 YOLO 可以检测的不仅仅是 200 多个类；它预测超过 9000 个不同对象类别的检测。它仍然实时运行。</p>
<h2 id="Comments-10"><a href="#Comments-10" class="headerlink" title="Comments"></a>Comments</h2><p>YOLO v2</p>
<h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lemonbit/article/details/109281590">YOLO算法最全综述：从YOLOv1到YOLOv5</a></p>
<hr>
<h1 id="Paper76-0116"><a href="#Paper76-0116" class="headerlink" title="Paper76-0116"></a>Paper76-0116</h1><h2 id="Published-in-2"><a href="#Published-in-2" class="headerlink" title="Published in"></a>Published in</h2><p>Annales Mathematicae et Informaticae</p>
<p>Bremner A, Macleod A. An unusual cubic representation problem. InAnnales Mathematicae et Informaticae 2014 Jan 1 (pp. 29-41).</p>
<h2 id="Title-2"><a href="#Title-2" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ami.uni-eszterhazy.hu/uploads/papers/finalpdf/AMI_43_from29to41.pdf">An unusual cubic representation problem</a></p>
<h2 id="Comments-11"><a href="#Comments-11" class="headerlink" title="Comments"></a>Comments</h2><p>文章提出了一个形式非常简单但是正整数解的值非常大（注意整数解的值相对很小）的问题。该问题被好事者伪装成小学题骗人。详见<a target="_blank" rel="noopener" href="https://open.toutiao.com/a6644426000639721988/?utm_campaign=open&amp;utm_medium=webview&amp;utm_source=vivoliulanqi_12&amp;item_id=6644426000639721988&amp;req_id=2019011012173501001802609450960&amp;latitude=30.545275&amp;imei=864176033955983&amp;label=headline&amp;gy=b72a01fb3f881b0bd789c2ca3342dd53944345c69e676659589e828536d227243143eae515588a31b4686fd64b6920219aaca992b1c085c72e3e98e5e940f096&amp;openudid=de6b385a8111eddb&amp;os=Android&amp;city=%E6%AD%A6%E6%B1%89&amp;longitude=114.362526&amp;crypt=9508&amp;isNews=1&amp;showComments=0&amp;showOriginalComments=true&amp;vivo_news_comment_data=%7B%7D&amp;vivo_news_comment_data_checksum=99914b932bd37a50b983c5e7c90ae93b&amp;vivo_news_source=1">史上最贱的数学题</a>。值得注意的是这篇优质内容来自南京大学科幻爱好者协会。</p>
<p>丢番图方程（Diophantine Equation）：有一个或者几个变量的整系数方程，它们的求解仅仅在整数范围内进行。</p>
<hr>
<h1 id="Paper75-0115"><a href="#Paper75-0115" class="headerlink" title="Paper75-0115"></a>Paper75-0115</h1><h2 id="Published-in-3"><a href="#Published-in-3" class="headerlink" title="Published in"></a>Published in</h2><p>宋文骢,谢品,郑遂,李玉璞.一种小展弦比高升力飞机的气动布局研究[J].中国工程科学,2001(08):70-75.</p>
<h2 id="Title-3"><a href="#Title-3" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFD2001&amp;filename=GCKX200108012&amp;uniplatform=NZKPT&amp;v=zT8yP1U9LOTDv0ATygAzCxRA8e9mSiB1fgxW_3TXRENErywMEXILmqGt16ylWVQF">一种小展弦比高升力飞机的气动布局研究</a></p>
<h2 id="Comments-12"><a href="#Comments-12" class="headerlink" title="Comments"></a>Comments</h2><p>提到歼20的研制，总是绕不开这篇文章。</p>
<hr>
<h1 id="Paper74-0114"><a href="#Paper74-0114" class="headerlink" title="Paper74-0114"></a>Paper74-0114</h1><h2 id="Published-in-4"><a href="#Published-in-4" class="headerlink" title="Published in"></a>Published in</h2><p><strong><em>Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi</em></strong>; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 779-788</p>
<h2 id="Title-4"><a href="#Title-4" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html">You Only Look Once: Unified, Real-Time Object Detection</a></p>
<h2 id="Authors-2"><a href="#Authors-2" class="headerlink" title="Authors"></a>Authors</h2><p>University of Washington</p>
<p>华盛顿大学</p>
<p>Allen Institute for AI</p>
<p>Allen AI研究所</p>
<p>Facebook AI Research</p>
<p>Facebook AI研究所</p>
<h2 id="Abstract-11"><a href="#Abstract-11" class="headerlink" title="Abstract"></a>Abstract</h2><p>We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.</p>
<p>我们提出了 YOLO，一种新的目标检测方法。先前关于对象检测的工作将分类器重新用于执行检测。相反，我们将对象检测框架为空间分离的边界框和相关类概率的回归问题。单个神经网络在一次评估中直接从完整图像预测边界框和类别概率。由于整个检测管道是一个单一的网络，因此可以直接在检测性能上进行端到端的优化。我们的统一架构非常快。我们的基础 YOLO 模型以每秒 45 帧的速度实时处理图像。该网络的较小版本 Fast YOLO 每秒处理惊人的 155 帧，同时仍实现其他实时检测器的两倍 mAP。与最先进的检测系统相比，YOLO 会产生更多的定位错误，但不太可能预测背景上的误报。最后，YOLO 学习了对象的非常一般的表示。当从自然图像推广到艺术品等其他领域时，它优于其他检测方法，包括 DPM 和 R-CNN。</p>
<h2 id="Comments-13"><a href="#Comments-13" class="headerlink" title="Comments"></a>Comments</h2><p><strong>YOLO v1！</strong></p>
<p>离谱的是，好像还有<em>you only look twice</em>和<em>you better look twice</em>。。。这几天可以关注一下。</p>
<h2 id="References-2"><a href="#References-2" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37192554/article/details/81092761">yolov1原文地址以及论文翻译</a></p>
<p>论文原文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.02640.pdf">https://arxiv.org/pdf/1506.02640.pdf</a></p>
<p>Tensorflow版本yolo v1:<a target="_blank" rel="noopener" href="https://github.com/gliese581gg/YOLO_tensorflow">https://github.com/gliese581gg/YOLO_tensorflow</a></p>
<hr>
<h1 id="Paper73-0113"><a href="#Paper73-0113" class="headerlink" title="Paper73-0113"></a>Paper73-0113</h1><h2 id="Published-in-5"><a href="#Published-in-5" class="headerlink" title="Published in"></a>Published in</h2><p>Redmon J, Farhadi A. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767. 2018 Apr 8.</p>
<p>10K引用</p>
<h2 id="Title-5"><a href="#Title-5" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.02767">YOLOv3: An Incremental Improvement</a></p>
<h2 id="Authors-3"><a href="#Authors-3" class="headerlink" title="Authors"></a>Authors</h2><p>Joseph Redmon, University of Washington</p>
<p>Joseph Redmon, 华盛顿大学</p>
<h2 id="Abstract-12"><a href="#Abstract-12" class="headerlink" title="Abstract"></a>Abstract</h2><p>We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that’s pretty swell. It’s a little bigger than last time but more accurate. It’s still fast though, don’t worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at <a target="_blank" rel="noopener" href="https://pjreddie.com/yolo/">this https URL</a></p>
<p>我们为 YOLO 提供了一些更新！ 我们进行了一些小的设计更改以使其更好。 我们还训练了这个非常庞大的新网络。 它比上次大一点，但更准确。 不过还是很快的，不用担心。 在 320x320 分辨率下，YOLOv3 以 28.2 mAP 的速度在 22 毫秒内运行，与 SSD 一样准确，但速度快了三倍。 当我们查看旧的 0.5 IOU mAP 检测指标时，YOLOv3 非常好。 它在 Titan X 上在 51 毫秒内达到 57.9 mAP@50，而 RetinaNet 在 198 毫秒内达到 57.5 mAP@50，性能相似但快了 3.8 倍。 与往常一样，所有代码都在 <a target="_blank" rel="noopener" href="https://pjreddie.com/yolo/">this https URL</a> 上在线</p>
<h2 id="Comments-14"><a href="#Comments-14" class="headerlink" title="Comments"></a>Comments</h2><p><strong>yolo v3</strong>原论文</p>
<p>该论文的解读在百度上一搜一大片，随便看。作者文笔很随意。</p>
<p>未来可以继续读yolo v1 v2 v4等</p>
<h2 id="References-3"><a href="#References-3" class="headerlink" title="References"></a>References</h2><p>参考<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/jSV47eDR9AcIXjzX_8xnqw">YOLO之父宣布退出CV界，坦言无法忽视自己工作带来的负面影响</a>，知乎上也有类似的讨论：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/373762877?from=singlemessage">如何看待「YOLO 之父」Joseph Redmon 宣布退出计算机视觉领域?</a></p>
<hr>
<h1 id="Paper72-0112"><a href="#Paper72-0112" class="headerlink" title="Paper72-0112"></a>Paper72-0112</h1><h2 id="Published-in-6"><a href="#Published-in-6" class="headerlink" title="Published in"></a>Published in</h2><p><strong><em>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</em></strong>; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778</p>
<p>CVPR：CVPR是IEEE Conference on Computer Vision and Pattern Recognition的缩写，即IEEE国际计算机视觉与模式识别会议。该会议是由IEEE举办的计算机视觉和模式识别领域的顶级会议。</p>
<h2 id="Title-6"><a href="#Title-6" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Deep Residual Learning for Image Recognition</a></p>
<h2 id="Authors-4"><a href="#Authors-4" class="headerlink" title="Authors"></a>Authors</h2><p>Kaiming He</p>
<p>何恺明，本科就读于清华大学，博士毕业于香港中文大学多媒体实验室。2011年加入微软亚洲研究院（MSRA）工作，主要研究计算机视觉和深度学习。2016年，加入Facebook AI Research（FAIR）担任研究科学家。2020年1月11日，荣登AI全球最具影响力学者榜单。</p>
<p>Microsoft Research</p>
<p>微软研究院</p>
<h2 id="Abstract-13"><a href="#Abstract-13" class="headerlink" title="Abstract"></a>Abstract</h2><p>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—-8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</p>
<p>更深层次的神经网络更难训练。我们提出了一个残差学习框架，以简化比以前使用的网络更深的网络的训练。我们明确地将层重新定义为参考层输入的学习残差函数，而不是学习未参考的函数。我们提供了全面的经验证据，表明这些残差网络更容易优化，并且可以从显着增加的深度中获得准确性。在 ImageNet 数据集上，我们评估深度高达 152 层的残差网络——比 VGG 网络深 8 倍，但仍然具有较低的复杂度。这些残差网络的集合在 ImageNet 测试集上实现了 3.57% 的误差。该结果在 ILSVRC 2015 分类任务中获得第一名。我们还对具有 100 层和 1000 层的 CIFAR-10 进行了分析。表示的深度对于许多视觉识别任务至关重要。仅仅由于我们极其深入的表示，我们在 COCO 对象检测数据集上获得了 28% 的相对改进。深度残差网络是我们提交 ILSVRC 和 COCO 2015 比赛的基础，我们还在 ImageNet 检测、ImageNet 定位、COCO 检测和 COCO 分割任务中获得了第一名。</p>
<h2 id="Comments-15"><a href="#Comments-15" class="headerlink" title="Comments"></a>Comments</h2><p><strong>十万引用的ResNet！！</strong></p>
<p>好好看，其他不用多说。</p>
<hr>
<h1 id="Paper71-0106"><a href="#Paper71-0106" class="headerlink" title="Paper71-0106"></a>Paper71-0106</h1><h2 id="Published-in-7"><a href="#Published-in-7" class="headerlink" title="Published in"></a>Published in</h2><p>Williams S, Waterman A, Patterson D. Roofline: an insightful visual performance model for multicore architectures. Communications of the ACM. 2009 Apr 1;52(4):65-76.</p>
<h2 id="Title-7"><a href="#Title-7" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/fullHtml/10.1145/1498765.1498785">Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore</a></p>
<h2 id="Authors-5"><a href="#Authors-5" class="headerlink" title="Authors"></a>Authors</h2><p><strong>Samuel Williams</strong> (SWWilliams@lbl.gov) is a research scientist at Lawrence Berkeley National Laboratory, Berkeley, CA.</p>
<p><strong>塞缪尔·威廉姆斯</strong> (SWWilliams@lbl.gov) 是加州伯克利劳伦斯伯克利国家实验室的一名研究科学家。</p>
<p><strong>Andrew Waterman</strong> (waterman@eecs.berkeley.edu) is a graduate student researcher in the Parallel Computing Laboratory of the University of California, Berkeley.</p>
<p><strong>安德鲁·水人</strong> (waterman@eecs.berkeley.edu) 是加州大学伯克利分校并行计算实验室的研究生研究员。</p>
<p><strong>David Patterson</strong> (pattrsn@eecs.berkeley.edu) is Director of the Parallel Computing Laboratory of the University of California, Berkeley, and a past president of ACM.</p>
<p><strong>大卫·帕特之子</strong> (pattrsn@eecs.berkeley.edu) 是加州大学伯克利分校并行计算实验室的主任，也是 ACM 的前任主席。</p>
<h2 id="Abstract-14"><a href="#Abstract-14" class="headerlink" title="Abstract"></a>Abstract</h2><p>Conventional wisdom in computer architecture produced similar designs. Nearly every desktop and server computer uses <strong>caches, pipelining, superscalar instruction issue, and out-of-order execution</strong>. Although the instruction sets varied, the microprocessors were all from the same school of design. The relatively recent switch to multicore means that microprocessors will become more diverse, since no conventional wisdom has yet emerged concerning their design. For example, some offer many simple processors vs. fewer complex processors, some depend on multithreading, and some even replace caches with explicitly addressed local stores. Manufacturers will likely offer multiple products with differing numbers of cores to cover multiple price-performance points, since Moore’s Law will permit the doubling of the number of cores per chip every two years.<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/fullHtml/10.1145/1498765.1498785#R4">4</a> While diversity may be understandable in this time of uncertainty, it exacerbates the already difficult jobs of programmers, compiler writers, and even architects. Hence, an easy-to-understand model that offers performance guidelines would be especially valuable.</p>
<p>Such a model need not be perfect, just <strong>insightful</strong>. The 3Cs (compulsory, capacity, and conflict misses) model for caches is an analogy.<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/fullHtml/10.1145/1498765.1498785#R19">19</a> It is not perfect, as it ignores potentially important factors like block size, block-allocation policy, and block-replacement policy. It also has <strong>quirks</strong>; for example, a miss might be labeled “capacity” in one design and “conflict” in another cache of the same size. Yet the 3Cs model has been popular for nearly 20 years precisely because it offers insight into the behavior of programs, helping programmers, compiler writers, and architects improve their respective designs.</p>
<p>Here, we propose one such model we call Roofline, demonstrating it on four diverse multicore computers using four key floating-point kernels.</p>
<p>计算机体系结构中的传统智慧产生了类似的设计。几乎每台台式机和服务器计算机都使用<strong>缓存、流水线、超标量指令发布和乱序执行</strong>。尽管指令集各不相同，但微处理器都来自同一个设计学院。最近转向多核意味着微处理器将变得更加多样化，因为关于它们的设计还没有出现传统智慧。例如，有些提供许多简单的处理器与较少的复杂处理器，有些依赖于多线程，有些甚至用显式寻址的本地存储替换缓存。制造商可能会提供具有不同内核数量的多种产品，以涵盖多个性价比点，因为摩尔定律允许每两年将每个芯片的内核数量翻一番。 4 虽然在这个充满不确定性的时代，多样性可能是可以理解的，但加剧了程序员、编译器编写者甚至架构师本已困难的工作。因此，提供性能指南的易于理解的模型将特别有价值。</p>
<p>这样的模型不需要完美，只要<strong>有见地</strong>。缓存的 3C（强制、容量和冲突未命中）模型是一个类比。19 它并不完美，因为它忽略了潜在的重要因素，如块大小、块分配策略和块替换策略。它也有<strong>怪癖</strong>；例如，一个未命中可能在一个设计中被标记为“容量”，而在另一个相同大小的缓存中被标记为“冲突”。然而，3Cs 模型已经流行了近 20 年，正是因为它提供了对程序行为的洞察，帮助程序员、编译器编写者和架构师改进各自的设计。</p>
<p>在这里，我们提出了一种我们称之为 Roofline 的模型，它在使用四个关键浮点内核的四台不同的多核计算机上进行了演示。</p>
<h2 id="Comments-16"><a href="#Comments-16" class="headerlink" title="Comments"></a>Comments</h2><p>roofline的意思是roof-line，即屋顶轮廓线的形状</p>
<p><strong>CAQA</strong> P212 Roofline可视性能模型原文</p>
<p>有个问题：什么叫Roofline不随内核改变而改变？是和OS无关吗</p>
<h2 id="References-4"><a href="#References-4" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34204282">Roofline Model与深度学习模型的性能分析</a></p>
<hr>
<h1 id="Paper70-0105"><a href="#Paper70-0105" class="headerlink" title="Paper70-0105"></a>Paper70-0105</h1><h2 id="Published-in-8"><a href="#Published-in-8" class="headerlink" title="Published in"></a>Published in</h2><h2 id="Title-8"><a href="#Title-8" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1106.5736v1">Algorithms for Solving Rubik’s Cubes</a></p>
<h2 id="Authors-6"><a href="#Authors-6" class="headerlink" title="Authors"></a>Authors</h2><p>MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, USA, {edemaine,mdemaine,seisenst }@mit.edu</p>
<p>麻省理工学院计算机科学与人工智能实验室，剑桥，马萨诸塞州 02139，美国，{edemaine,mdemaine,seisenst }@mit.edu</p>
<p>David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario N2L 3G1, Canada, alubiw@uwaterloo.ca</p>
<p>David R. Cheriton 计算机科学学院，滑铁卢大学，安大略省滑铁卢 N2L 3G1，加拿大，alubiw@uwaterloo.ca</p>
<p>Department of Computer Science, Tufts University, Medford, MA 02155, USA, awinslow@cs.tufts.edu</p>
<p>塔夫茨大学计算机科学系，梅德福，MA 02155，美国，awinslow@cs.tufts.edu</p>
<h2 id="Abstract-15"><a href="#Abstract-15" class="headerlink" title="Abstract"></a>Abstract</h2><p>The Rubik’s Cube is perhaps the world’s most famous and iconic puzzle, well-known to have a rich underlying mathematical structure (group theory). In this paper, we show that the Rubik’s Cube also has a rich underlying algorithmic structure. Specifically, we show that the n x n x n Rubik’s Cube, as well as the n x n x 1 variant, has a “God’s Number” (diameter of the configuration space) of Theta(n^2/log n). The upper bound comes from effectively parallelizing standard Theta(n^2) solution algorithms, while the lower bound follows from a counting argument. The upper bound gives an asymptotically optimal algorithm for solving a general Rubik’s Cube in the worst case. Given a specific starting state, we show how to find the shortest solution in an n x O(1) x O(1) Rubik’s Cube. Finally, we show that finding this optimal solution becomes NP-hard in an n x n x 1 Rubik’s Cube when the positions and colors of some of the cubies are ignored (not used in determining whether the cube is solved).</p>
<p>魔方可能是世界上最著名和最具标志性的谜题，众所周知，它具有丰富的潜在数学结构（群论）。 在本文中，我们展示了魔方还具有丰富的底层算法结构。 具体来说，我们展示了 n x n x n 魔方以及 n x n x 1 变体，具有 Theta(n^2/log n) 的“上帝数”（配置空间的直径）。 上限来自有效并行化标准 Theta(n^2) 解决方案算法，而下限来自计数参数。 上限给出了在最坏情况下求解一般魔方的渐近最优算法。 给定一个特定的起始状态，我们展示了如何在 n x O(1) x O(1) 魔方中找到最短解。 最后，我们表明，当一些立方体的位置和颜色被忽略（不用于确定立方体是否被求解）时，在 n x n x 1 魔方中找到这个最优解变得 NP 难。</p>
<h2 id="Keywords-Index-Terms-3"><a href="#Keywords-Index-Terms-3" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>combinatorial puzzles, diameter, God’s number, combinatorial optimization</p>
<p>组合拼图，直径，上帝之数，组合优化</p>
<h2 id="Comments-17"><a href="#Comments-17" class="headerlink" title="Comments"></a>Comments</h2><p>该论文提出了一个结论：上帝之数的增长随着阶数n的上升大致呈O(n<sup>2</sup>/log(n))</p>
<p>详见知乎<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/23619010/answer/29414485">最坏情况下，一个N阶魔方至少需要几步复原？ - 陈霜的回答</a></p>
<h2 id="References-5"><a href="#References-5" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper69-0104"><a href="#Paper69-0104" class="headerlink" title="Paper69-0104"></a>Paper69-0104</h1><h2 id="Published-in-9"><a href="#Published-in-9" class="headerlink" title="Published in"></a>Published in</h2><p>Kim S, Park S, Na B, Yoon S. Spiking-YOLO: Spiking neural network for energy-efficient object detection. InProceedings of the AAAI Conference on Artificial Intelligence 2020 Apr 3 (Vol. 34, No. 07, pp. 11270-11277).</p>
<h2 id="Title-9"><a href="#Title-9" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/6787">Spiking-YOLO Spiking Neural Network for Energy-Efficient Object Detection</a></p>
<h2 id="Authors-7"><a href="#Authors-7" class="headerlink" title="Authors"></a>Authors</h2><p>Seoul National University</p>
<p>首尔大学</p>
<h2 id="Abstract-16"><a href="#Abstract-16" class="headerlink" title="Abstract"></a>Abstract</h2><p>Over the past decade, deep neural networks (DNNs) have demonstrated remarkable performance in a variety of applications. As we try to solve more advanced problems, increasing demands for computing and power resources has become inevitable. Spiking neural networks (SNNs) have attracted widespread interest as the third-generation of neural networks due to their event-driven and low-powered nature. SNNs, however, are difficult to train, mainly owing to their complex dynamics of neurons and non-differentiable spike operations. Furthermore, their applications have been limited to relatively simple tasks such as image classification. In this study, we investigate the performance degradation of SNNs in a more challenging regression problem (i.e., object detection). Through our in-depth analysis, we introduce two novel methods: channel-wise normalization and signed neuron with imbalanced threshold, both of which provide fast and accurate information transmission for deep SNNs. Consequently, we present a first spiked-based object detection model, called Spiking-YOLO. Our experiments show that Spiking-YOLO achieves remarkable results that are comparable (up to 98%) to those of Tiny YOLO on non-trivial datasets, PASCAL VOC and MS COCO. Furthermore, Spiking-YOLO on a neuromorphic chip consumes approximately 280 times less energy than Tiny YOLO and converges 2.3 to 4 times faster than previous SNN conversion methods.</p>
<p>在过去十年中，深度神经网络 (DNN) 在各种应用中表现出卓越的性能。当我们尝试解决更高级的问题时，对计算和电源资源的需求增加已成为必然。尖峰神经网络 (SNN) 作为第三代神经网络，由于其事件驱动和低功耗的特性而引起了广泛的兴趣。然而，SNN 很难训练，主要是因为它们复杂的神经元动力学和不可微的尖峰操作。此外，它们的应用仅限于相对简单的任务，例如图像分类。在这项研究中，我们研究了 SNN 在更具挑战性的回归问题（即对象检测）中的性能下降。通过我们的深入分析，我们引入了两种新方法：通道归一化和具有不平衡阈值的带符号神经元，这两种方法都为深度 SNN 提供了快速准确的信息传输。因此，我们提出了第一个基于尖峰的对象检测模型，称为 Spiking-YOLO。我们的实验表明，Spiking-YOLO 在非平凡数据集 PASCAL VOC 和 MS COCO 上取得了与 Tiny YOLO 相当（高达 98%）的显着结果。此外，神经形态芯片上的 Spiking-YOLO 消耗的能量比 Tiny YOLO 少约 280 倍，收敛速度比以前的 SNN 转换方法快 2.3 到 4 倍。</p>
<h2 id="Comments-18"><a href="#Comments-18" class="headerlink" title="Comments"></a>Comments</h2><p>参见<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34970603/article/details/107140315">SNN论文系列-ANN to SNN 学习算法</a>，未来几天主要看该博客中提到的论文。</p>
<h2 id="References-6"><a href="#References-6" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper68-0103"><a href="#Paper68-0103" class="headerlink" title="Paper68-0103"></a>Paper68-0103</h1><h2 id="Published-in-10"><a href="#Published-in-10" class="headerlink" title="Published in"></a>Published in</h2><h2 id="Title-10"><a href="#Title-10" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.frontiersin.org/articles/10.3389/fnins.2019.00095/full">Going deeper in spiking neural networks Vgg and residual architecture</a></p>
<h2 id="Authors-8"><a href="#Authors-8" class="headerlink" title="Authors"></a>Authors</h2><p>Department of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, United States</p>
<p>美国印第安纳州西拉斐特普渡大学电气与计算机工程系</p>
<p>Facebook Reality Labs, Facebook Research, Redmond, WA, United States</p>
<p>Facebook Reality Labs，Facebook Research，美国华盛顿州雷德蒙德</p>
<p>Only affiliations matters.</p>
<h2 id="Abstract-17"><a href="#Abstract-17" class="headerlink" title="Abstract"></a>Abstract</h2><p>Over the past few years, Spiking Neural Networks (SNNs) have become popular as a possible pathway to enable low-power event-driven neuromorphic hardware. However, their application in machine learning have largely been limited to very shallow neural network architectures for simple problems. In this paper, we propose a novel algorithmic technique for generating an SNN with a deep architecture, and demonstrate its effectiveness on complex visual recognition problems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and Residual network architectures, with significantly better accuracy than the state-of-the-art. Finally, we present analysis of the sparse event-driven computations to demonstrate reduced hardware overhead when operating in the spiking domain.</p>
<p>在过去的几年里，尖峰神经网络 (SNN) 作为一种可能的途径来实现低功耗事件驱动的神经形态硬件已经变得流行起来。 然而，它们在机器学习中的应用很大程度上仅限于针对简单问题的非常浅的神经网络架构。 在本文中，我们提出了一种用于生成具有深度架构的 SNN 的新算法技术，并证明了其在复杂视觉识别问题（如 CIFAR-10 和 ImageNet）上的有效性。 我们的技术适用于 VGG 和 Residual 网络架构，其准确性明显高于最先进的技术。 最后，我们对稀疏事件驱动计算进行了分析，以证明在尖峰域中操作时减少的硬件开销。</p>
<h2 id="Comments-19"><a href="#Comments-19" class="headerlink" title="Comments"></a>Comments</h2><p>参见<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34970603/article/details/107140315">SNN论文系列-ANN to SNN 学习算法</a>，未来几天主要看该博客中提到的论文。</p>
<h2 id="References-7"><a href="#References-7" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper67-0102"><a href="#Paper67-0102" class="headerlink" title="Paper67-0102"></a>Paper67-0102</h1><h2 id="Published-in-11"><a href="#Published-in-11" class="headerlink" title="Published in"></a>Published in</h2><h2 id="Title-11"><a href="#Title-11" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/7280696">Fast-Classifying, High-Accuracy Spiking Deep Networks Through Weight and Threshold Balancing</a></p>
<h2 id="Authors-9"><a href="#Authors-9" class="headerlink" title="Authors"></a>Authors</h2><p>Ignore</p>
<h2 id="Abstract-18"><a href="#Abstract-18" class="headerlink" title="Abstract"></a>Abstract</h2><p>Deep neural networks such as Convolutional Networks (ConvNets) and Deep Belief Networks (DBNs) represent the state-of-the-art for many machine learning and computer vision classification problems. To overcome the large computational cost of deep networks, spiking deep networks have recently been proposed, given the specialized hardware now available for spiking neural networks (SNNs). However, this has come at the cost of performance losses due to the conversion from analog neural networks (ANNs) without a notion of time, to sparsely firing, event-driven SNNs. Here we analyze the effects of converting deep ANNs into SNNs with respect to the choice of parameters for spiking neurons such as firing rates and thresholds. We present a set of optimization techniques to minimize performance loss in the conversion process for ConvNets and fully connected deep networks. These techniques yield networks that outperform all previous SNNs on the MNIST database to date, and many networks here are close to maximum performance after only 20 ms of simulated time. The techniques include using rectified linear units (ReLUs) with zero bias during training, and using a new weight normalization method to help regulate firing rates. Our method for converting an ANN into an SNN enables low-latency classification with high accuracies already after the first output spike, and compared with previous SNN approaches it yields improved performance without increased training time. The presented analysis and optimization techniques boost the value of spiking deep networks as an attractive framework for neuromorphic computing platforms aimed at fast and efficient pattern recognition.</p>
<p>卷积网络 (ConvNets) 和深度信念网络 (DBN) 等深度神经网络代表了许多机器学习和计算机视觉分类问题的最新技术。为了克服深度网络的巨大计算成本，鉴于现在可用于尖峰神经网络 (SNN) 的专用硬件，最近提出了尖峰深度网络。然而，这是以性能损失为代价的，因为从没有时间概念的模拟神经网络 (ANN) 转换为稀疏触发、事件驱动的 SNN。在这里，我们分析了将深度 ANN 转换为 SNN 对尖峰神经元参数选择（例如放电率和阈值）的影响。我们提出了一组优化技术，以最大限度地减少 ConvNets 和完全连接的深度网络在转换过程中的性能损失。迄今为止，这些技术产生的网络在 MNIST 数据库上的性能优于所有以前的 SNN，并且这里的许多网络仅在 20 毫秒的模拟时间后就接近最高性能。这些技术包括在训练期间使用零偏差的修正线性单元 (ReLU)，以及使用新的权重归一化方法来帮助调节发射率。我们将 ANN 转换为 SNN 的方法可以在第一个输出尖峰之后实现高精度的低延迟分类，并且与之前的 SNN 方法相比，它在不增加训练时间的情况下提高了性能。所提出的分析和优化技术提高了脉冲深度网络作为神经形态计算平台的有吸引力的框架的价值，该框架旨在快速有效的模式识别。</p>
<h2 id="Keywords-Index-Terms-4"><a href="#Keywords-Index-Terms-4" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><ul>
<li>IEEE Keywords<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Neuromorphics&amp;newsearch=true">Neuromorphics</a>,神经形态学</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Handheld computers&amp;newsearch=true">Handheld computers</a>,掌上电脑</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Neurons&amp;newsearch=true">Neurons</a>,神经元</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Robustness&amp;newsearch=true">Robustness</a>,鲁棒性</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Accuracy&amp;newsearch=true">Accuracy</a>准确性</li>
</ul>
</li>
<li>INSPEC: Controlled Indexing 受控索引<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:neural nets&amp;newsearch=true">neural nets</a>,神经网络</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:pattern classification&amp;newsearch=true">pattern classification</a>模式分类</li>
</ul>
</li>
<li>INSPEC: Non-Controlled Indexing 不受控索引<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:threshold balancing&amp;newsearch=true">threshold balancing</a>,阈值平衡</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:weight balancing&amp;newsearch=true">weight balancing</a>,权重平衡</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:spiking neural networks&amp;newsearch=true">spiking neural networks</a>,尖峰神经网络</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:deep ANN&amp;newsearch=true">deep ANN</a>,深度ANN</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:spiking neurons&amp;newsearch=true">spiking neurons</a>,尖峰神经元</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:optimization techniques&amp;newsearch=true">optimization techniques</a>,优化技术</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:ConvNets&amp;newsearch=true">ConvNets</a>,卷积网络</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:fully connected deep networks&amp;newsearch=true">fully connected deep networks</a>,全连接深度网络</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:MNIST database&amp;newsearch=true">MNIST database</a>,MNIST数据集</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:rectified linear units&amp;newsearch=true">rectified linear units</a>,整流线性单元</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:ReLU&amp;newsearch=true">ReLU</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:weight normalization method&amp;newsearch=true">weight normalization method</a>,权重归一化方法</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:firing rates&amp;newsearch=true">firing rates</a>,发射率</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:SNN&amp;newsearch=true">SNN</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:low-latency classification&amp;newsearch=true">low-latency classification</a>,低延迟分类</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:spiking deep networks&amp;newsearch=true">spiking deep networks</a>,尖峰深度网络</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:convolutional neural networks&amp;newsearch=true">convolutional neural networks</a>卷积神经网络（CNN）</li>
</ul>
</li>
</ul>
<h2 id="Comments-20"><a href="#Comments-20" class="headerlink" title="Comments"></a>Comments</h2><p>参见<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34970603/article/details/107140315">SNN论文系列-ANN to SNN 学习算法</a>，未来几天主要看该博客中提到的论文。</p>
<h2 id="References-8"><a href="#References-8" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper66-0101"><a href="#Paper66-0101" class="headerlink" title="Paper66-0101"></a>Paper66-0101</h1><h2 id="Published-in-12"><a href="#Published-in-12" class="headerlink" title="Published in"></a>Published in</h2><h2 id="Title-12"><a href="#Title-12" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://link.springer.com/content/pdf/10.1007/s11263-014-0788-3.pdf">Spiking Deep Convolutional Neural Networks for Energy-Efficient Object Recognition</a></p>
<h2 id="Authors-10"><a href="#Authors-10" class="headerlink" title="Authors"></a>Authors</h2><p>Ignore</p>
<h2 id="Abstract-19"><a href="#Abstract-19" class="headerlink" title="Abstract"></a>Abstract</h2><p>Deep-learning neural networks such as convolutional neural network (CNN) have shown great potential as a solution for difficult vision problems, such as object recognition. Spiking neural networks (SNN)-based architectures have shown great potential as a solution for realizing ultra-low power consumption using spike-based neuromorphic hardware. This work describes a novel approach for converting a deep CNN into a SNN that enables mapping CNN to spike-based hardware architectures. Our approach first tailors the CNN architecture to fit the requirements of SNN, then trains the tailored CNN in the same way as one would with CNN, and finally applies the learned network weights to an SNN architecture derived from the tailored CNN. We evaluate the resulting SNN on publicly available Defense Advanced Research Projects Agency (DARPA) Neovision2 Tower and CIFAR-10 datasets and show similar object recognition accuracy as the original CNN. Our SNN implementation is amenable to direct mapping to spike-based neuromorphic hardware, such as the ones being developed under the DARPA SyNAPSE program. Our hardware mapping analysis suggests that SNN implementation on such spikebased hardware is two orders of magnitude more energyefficient than the original CNN implementation on off-theshelf FPGA-based hardware</p>
<p>卷积神经网络 (CNN) 等深度学习神经网络已显示出作为解决困难视觉问题（例如对象识别）的巨大潜力。基于尖峰神经网络 (SNN) 的架构已显示出作为使用基于尖峰的神经形态硬件实现超低功耗的解决方案的巨大潜力。这项工作描述了一种将深度 CNN 转换为 SNN 的新方法，该方法能够将 CNN 映射到基于尖峰的硬件架构。我们的方法首先定制 CNN 架构以适应 SNN 的要求，然后以与使用 CNN 相同的方式训练定制的 CNN，最后将学习到的网络权重应用于从定制的 CNN 派生的 SNN 架构。我们在公开可用的国防高级研究计划局 (DARPA) Neovision2 Tower 和 CIFAR-10 数据集上评估生成的 SNN，并显示出与原始 CNN 相似的对象识别准确性。我们的 SNN 实现适合直接映射到基于尖峰的神经形态硬件，例如在 DARPA SyNAPSE 计划下开发的硬件。我们的硬件映射分析表明，在基于脉冲的硬件上的 SNN 实现比基于 FPGA 的现成硬件上的原始 CNN 实现高两个数量级的能效。</p>
<h2 id="Keywords-Index-Terms-5"><a href="#Keywords-Index-Terms-5" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>Deep learning · Machine learning · Convolutional neural networks · Spiking neural networks · Neuromorphic circuits · Object recognition</p>
<p>深度学习 · 机器学习 · 卷积神经网络 · 尖峰神经网络 · 神经形态电路 · 对象识别</p>
<h2 id="Comments-21"><a href="#Comments-21" class="headerlink" title="Comments"></a>Comments</h2><p>SNN入门。</p>
<h2 id="References-9"><a href="#References-9" class="headerlink" title="References"></a>References</h2><p>参见<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34970603/article/details/107140315">SNN论文系列-ANN to SNN 学习算法</a>，未来几天主要看该博客中提到的论文。</p>
<hr>
<h1 id="Paper65-1231"><a href="#Paper65-1231" class="headerlink" title="Paper65-1231"></a>Paper65-1231</h1><p><strong>2021年最后一篇！</strong></p>
<h2 id="Published-in-13"><a href="#Published-in-13" class="headerlink" title="Published in"></a>Published in</h2><p>Upper D. The unsuccessful self-treatment of a case of “writer’s block”. Journal of applied behavior analysis. 1974;7(3):497.</p>
<h2 id="Title-13"><a href="#Title-13" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1311997/">The unsuccessful self-treatment of a case of “writer’s block”</a></p>
<h2 id="Authors-11"><a href="#Authors-11" class="headerlink" title="Authors"></a>Authors</h2><p>DENNIS UPPER</p>
<p>VETERANS ADMINISTRATION HOSPITAL, BROCKTON, MASSACHUSETTS</p>
<p>马萨诸塞州布罗克顿退伍军人管理局医院</p>
<h2 id="Abstract-20"><a href="#Abstract-20" class="headerlink" title="Abstract"></a>Abstract</h2><p>[Here is some Lemon Juice]</p>
<h2 id="Keywords-Index-Terms-6"><a href="#Keywords-Index-Terms-6" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>[X-ray Analyzer has been used]</p>
<h2 id="Comments-22"><a href="#Comments-22" class="headerlink" title="Comments"></a>Comments</h2><p><strong>Received 25 October 1973. (Published without revision.)</strong></p>
<blockquote>
<p>I have studied this manuscript very carefully with lemon juice and X-rays and have not detected a single flaw in either design or writing style. I suggest it be published without revision. Clearly it is the most concise manuscript I have ever seen-yet it contains sufficient detail to allow other investigators to replicate Dr. Upper’s failure. In comparison with the other manuscripts I get from you containing all that complicated detail, this one was a pleasure to examine. Surely we can find a place for this paper in the Journal-perhaps on the edge of a blank page.</p>
</blockquote>
<p>来源：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/37799514/answer/73570502">你读过哪些有趣的论文？ - 知乎</a></p>
<p>这篇论文截至2021年12月31日，已经斩获了103个引用。</p>
<h2 id="References-10"><a href="#References-10" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper64-1230"><a href="#Paper64-1230" class="headerlink" title="Paper64-1230"></a>Paper64-1230</h1><h2 id="Published-in-14"><a href="#Published-in-14" class="headerlink" title="Published in"></a>Published in</h2><p>J. Am. Chem. Soc. 1988</p>
<p>Merkel A, Havlas Z, Zahradnik R. Evaluation of the rate constant for the SN2 reaction fluoromethane+ hydride. fwdarw. methane+ fluoride in the gas phase. <a target="_blank" rel="noopener" href="https://pubs.acs.org/journal/jacsat">Journal of the American Chemical Society</a>. 1988 Dec;110(25):8355-9.</p>
<h2 id="Title-14"><a href="#Title-14" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://pubs.acs.org/doi/pdf/10.1021/ja00233a012">Evaluation of the Rate Constant for the S<sub>N</sub>2 Reaction CH<sub>3</sub>F + H<sup>-</sup> → CH<sub>4</sub> + F<sup>-</sup> in the Gas Phase</a></p>
<p>评估 SN2 反应氟甲烷 + 氢化物→气相中的甲烷+氟化物的速率常数。 </p>
<h2 id="Authors-12"><a href="#Authors-12" class="headerlink" title="Authors"></a>Authors</h2><p>Angela Merkel</p>
<p>Central Institute of Physical Chemistry of Academy of Sciences of GDR</p>
<p>东德科学院中央物理化学研究所</p>
<p>Zdenék Havlas</p>
<p>institute of Organic Chemistry and Biochemistry, Czechoslovak Academy of Sciences.</p>
<p>捷克斯洛伐克科学院有机化学与生物化学研究所。</p>
<p>Rudolf Zahradnik</p>
<p>J. Heyrorsky Institute of Physical Chemistry and Electrochemistry, Czechoslovak Academy of Sciences.</p>
<p>J. Heyrorsky 物理化学和电化学研究所，捷克斯洛伐克科学院。</p>
<h2 id="Abstract-21"><a href="#Abstract-21" class="headerlink" title="Abstract"></a>Abstract</h2><p>Rate constants for the title reaction are calculated within the framework of statistical theories. The input data are taken mainly from recently published ab initio quantum chemical calculations. Comparison with the experimental estimate from flowing afterglow measurements by Tanaka et al. of k%j% = 1.5 X 1CT11 cm3 s”1 shows that, for three of four input data sets, the experimental value can be reproduced with reasonable accuracy. Using two kinetic models, rate constants ranging from 2.1 X 10~12 to 4.5 X 10~u cm3 s”1 are obtained for these three data sets. Variation of the kinetic model changes the rate constants by up to one order of magnitude. The remaining uncertainties should stimulate further experiments as well as higher level quantum chemical calculations and theoretical treatment of the reaction dynamics.</p>
<p>标题反应的速率常数是在统计理论框架内计算的。 输入数据主要来自最近发表的 ab initio 量子化学计算。 与 Tanaka 等人的流动余辉测量的实验估计值进行比较。 of k%j% = 1.5 X 1CT11 cm3 s”1 表明，对于四个输入数据集中的三个，实验值可以以合理的精度重现。使用两个动力学模型，速率常数范围为 2.1 X 10~12 到 4.5 这三个数据集得到了 X 10~u cm3 s”1。 动力学模型的变化将速率常数改变多达一个数量级。 剩余的不确定性应该会刺激进一步的实验以及更高水平的量子化学计算和反应动力学的理论处理。</p>
<h2 id="Comments-23"><a href="#Comments-23" class="headerlink" title="Comments"></a>Comments</h2><p>JACS是化学领域顶刊，不过这并不是重点，重点在于它的第一作者是德国前总理<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%AE%89%E6%A0%BC%E6%8B%89%C2%B7%E9%BB%98%E5%85%8B%E5%B0%94/3335469">默克尔</a>…</p>
<h2 id="References-11"><a href="#References-11" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper63-1229"><a href="#Paper63-1229" class="headerlink" title="Paper63-1229"></a>Paper63-1229</h1><h2 id="Published-in-15"><a href="#Published-in-15" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://pubs.acs.org/joc">The Journal of Organic Chemistry</a></p>
<h2 id="Title-15"><a href="#Title-15" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://pubs.acs.org/doi/abs/10.1021/jo0349227">Synthesis of Anthropomorphic Molecules:  The NanoPutians</a></p>
<p>拟人化分子的合成： The NanoPutians</p>
<h2 id="Authors-13"><a href="#Authors-13" class="headerlink" title="Authors"></a>Authors</h2><p>Stephanie H. Chanteau and James M. Tour</p>
<p>Department of Chemistry and Center for Nanoscale Science and Technology, MS 222, Rice University, Houston, Texas 77005</p>
<p>化学系和纳米科学与技术中心，MS 222，莱斯大学，休斯顿，德克萨斯 77005</p>
<h2 id="Abstract-22"><a href="#Abstract-22" class="headerlink" title="Abstract"></a>Abstract</h2><p>Described here are the synthetic details en route to an array of 2-nm-tall anthropomorphic molecules in monomeric, dimeric, and polymeric form. These anthropomorphic figures are called, as a class, NanoPutians. Using tools of chemical synthesis, the ultimate in designed miniaturization can be attained while preparing the most widely recognized structures:  those that resemble humans.</p>
<p>这里描述的是合成细节，用于合成单体、二聚体和聚合物形式的 2 纳米高拟人分子阵列。 这些拟人化的人物被称为一类，NanoPutians。 使用化学合成工具，可以在制备最广为人知的结构的同时实现最终的微型化设计： 那些类似于人类的结构。</p>
<p><img src="/2021/10/29/papers/NanoPutian.png" alt="NanoPutian"></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In conclusion, a series of monomeric, dimeric, and polymeric anthropomorphic molecules have been synthesized. These anthropomorphic entities are dubbed NanoPutians, as a class. Furthermore, they are assigned common names based on their individual anthropomorphic designs. These represent the ultimate in anthropomorphic design miniaturization.</p>
<p>总之，已经合成了一系列单体、二聚体和聚合的拟人分子。 这些拟人实体被称为 NanoPutians，作为一个类。 此外，根据他们个人的拟人化设计，他们被分配了通用名称。 这些代表了拟人化设计小型化的极致。</p>
<h2 id="Comments-24"><a href="#Comments-24" class="headerlink" title="Comments"></a>Comments</h2><p>曾经在高中化学题里面见识到的有机小人！</p>
<p>这项搞笑工作还得到了nature的报道：<a target="_blank" rel="noopener" href="https://www.nature.com/articles/news031013-3">NanoKids made in lab</a>，摘录一段：</p>
<blockquote>
<p>Eight Houston schools are using the curriculum-linked disc as a teaching aid for 11-13-year-olds. If the feedback is good and funding is forthcoming, Tour hopes that students across the United States will get to play with his NanoKids.</p>
</blockquote>
<p>“休斯顿的八所学校正在使用与课程相关的光盘作为 11-13 岁儿童的教具。 如果反馈良好并且资金能够到位，Tour希望美国各地的学生都能和他的 NanoKids 一起玩。”</p>
<hr>
<h1 id="Paper62-1227"><a href="#Paper62-1227" class="headerlink" title="Paper62-1227"></a>Paper62-1227</h1><h2 id="Published-in-16"><a href="#Published-in-16" class="headerlink" title="Published in"></a>Published in</h2><p>Wu Y, Deng L, Li G, Zhu J, Xie Y, Shi L. Direct training for spiking neural networks: Faster, larger, better. InProceedings of the AAAI Conference on Artificial Intelligence 2019 Jul 17 (Vol. 33, No. 01, pp. 1311-1318).</p>
<h2 id="Title-16"><a href="#Title-16" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/3929">Direct training for spiking neural networks: Faster, larger, better</a></p>
<h2 id="Authors-14"><a href="#Authors-14" class="headerlink" title="Authors"></a>Authors</h2><p>Tsinghua University</p>
<p>清华大学</p>
<p>University of California, Santa Barbara</p>
<p>加州大学圣巴巴拉分校</p>
<h2 id="Abstract-23"><a href="#Abstract-23" class="headerlink" title="Abstract"></a>Abstract</h2><p>Spiking neural networks (SNNs) that enables energy efficient implementation on emerging neuromorphic hardware are gaining more attention. Yet now, SNNs have not shown competitive performance compared with artificial neural networks (ANNs), due to the lack of effective learning algorithms and efficient programming frameworks. We address this issue from two aspects: (1) We propose a neuron normalization technique to adjust the neural selectivity and develop a direct learning algorithm for deep SNNs. (2) Via narrowing the rate coding window and converting the leaky integrate-and-fire (LIF) model into an explicitly iterative version, we present a Pytorch-based implementation method towards the training of large-scale SNNs. In this way, we are able to train deep SNNs with tens of times speedup. As a result, we achieve significantly better accuracy than the reported works on neuromorphic datasets (N-MNIST and DVSCIFAR10), and comparable accuracy as existing ANNs and pre-trained SNNs on non-spiking datasets (CIFAR10). To our best knowledge, this is the first work that demonstrates direct training of deep SNNs with high performance on CIFAR10, and the efficient implementation provides a new way to explore the potential of SNNs.</p>
<p>能够在新兴的神经形态硬件上实现高能效实施的尖峰神经网络 (SNN) 正受到越来越多的关注。然而现在，由于缺乏有效的学习算法和高效的编程框架，SNN 与人工神经网络 (ANN) 相比还没有表现出具有竞争力的性能。我们从两个方面解决这个问题：（1）我们提出了一种神经元归一化技术来调整神经选择性并开发一种用于深度 SNN 的直接学习算法。 (2) 通过缩小速率编码窗口并将泄漏集成和发射 (LIF) 模型转换为显式迭代版本，我们提出了一种基于 Pytorch 的大规模 SNN 训练的实现方法。通过这种方式，我们能够以数十倍的速度训练深度 SNN。因此，我们在神经形态数据集（N-MNIST 和 DVSCIFAR10）上实现了明显更好的准确性，并且与现有 ANN 和非尖峰数据集（CIFAR10）上的预训练 SNN 相当的准确性。据我们所知，这是第一个展示在 CIFAR10 上直接训练具有高性能的深度 SNN 的工作，并且有效的实施为探索 SNN 的潜力提供了一种新方法。</p>
<h2 id="References-12"><a href="#References-12" class="headerlink" title="References"></a>References</h2><p>利用snn实现手写数字识别，训练算法为STBP(作者提出)，准确度提升为99.4%。<br>GitHub地址：<br><a target="_blank" rel="noopener" href="https://github.com/yjwu17/BP-for-SpikingNN">https://github.com/yjwu17/BP-for-SpikingNN</a><br>配套论文：<br><strong>Wu, Yujie, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. “Direct Training for Spiking Neural Networks: Faster, Larger, Better.” arXiv preprint arXiv:1809.05793 (2018).</strong><br>Wu, Yujie, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. “Spatio-temporal backpropagation for training high-performance spiking neural networks.” Frontiers in neuroscience 12 (2018).<br>————————————————<br>版权声明：本文为CSDN博主「Kyrie   开一」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/Kyrie6c/article/details/115289056">https://blog.csdn.net/Kyrie6c/article/details/115289056</a></p>
<hr>
<h1 id="Paper61-1219"><a href="#Paper61-1219" class="headerlink" title="Paper61-1219"></a>Paper61-1219</h1><h2 id="Published-in-17"><a href="#Published-in-17" class="headerlink" title="Published in"></a>Published in</h2><p>Wu Y, Deng L, Li G, Zhu J, Shi L. Spatio-temporal backpropagation for training high-performance spiking neural networks. Frontiers in neuroscience. 2018 May 23;12:331.</p>
<h2 id="Title-17"><a href="#Title-17" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://internal-journal.frontiersin.org/articles/10.3389/fnins.2018.00331/full">Spatio-temporal backpropagation for training high-performance spiking neural networks</a></p>
<h2 id="Authors-15"><a href="#Authors-15" class="headerlink" title="Authors"></a>Authors</h2><p>Department of Precision Instrument, Center for Brain-Inspired Computing Research, Beijing Innovation Center for Future Chip, Tsinghua University, Beijing, China</p>
<p>精密仪器系，类脑计算研究中心，北京未来芯片创新中心，清华大学，北京，中国</p>
<p>Department of Electrical and Computer Engineering, University of California, Santa Barbara, Santa Barbara, CA, United States</p>
<p>美国加利福尼亚大学圣巴巴拉分校电气与计算机工程系</p>
<p>State Key Lab of Intelligence Technology and System, Tsinghua National Lab for Information Science and Technology, Tsinghua University, Beijing, China</p>
<p>智能技术与系统国家重点实验室，清华信息科学与技术国家实验室，清华大学，北京，中国</p>
<h2 id="Abstract-24"><a href="#Abstract-24" class="headerlink" title="Abstract"></a>Abstract</h2><p>Spiking neural networks (SNNs) are promising in ascertaining brain-like behaviors since spikes are capable of encoding spatio-temporal information. Recent schemes, e.g., pre-training from artificial neural networks (ANNs) or direct training based on backpropagation (BP), make the high-performance supervised training of SNNs possible. However, these methods primarily fasten more attention on its spatial domain information, and the dynamics in temporal domain are attached less significance. Consequently, this might lead to the performance bottleneck, and scores of training techniques shall be additionally required. Another underlying problem is that the spike activity is naturally non-differentiable, raising more difficulties in supervised training of SNNs. In this paper, we propose a spatio-temporal backpropagation (STBP) algorithm for training high-performance SNNs. In order to solve the non-differentiable problem of SNNs, an approximated derivative for spike activity is proposed, being appropriate for gradient descent training. The STBP algorithm combines the layer-by-layer spatial domain (SD) and the timing-dependent temporal domain (TD), and does not require any additional complicated skill. We evaluate this method through adopting both the fully connected and convolutional architecture on the static MNIST dataset, a custom object detection dataset, and the dynamic N-MNIST dataset. Results bespeak that our approach achieves the best accuracy compared with existing state-of-the-art algorithms on spiking networks. This work provides a new perspective to investigate the high-performance SNNs for future brain-like computing paradigm with rich spatio-temporal dynamics.</p>
<p>尖峰神经网络 (SNN) 在确定类脑行为方面很有前景，因为尖峰能够编码时空信息。最近的方案，例如人工神经网络 (ANN) 的预训练或基于反向传播 (BP) 的直接训练，使 SNN 的高性能监督训练成为可能。然而，这些方法主要更多地关注其空间域信息，而对时域动态的重视程度较低。因此，这可能会导致性能瓶颈，并且需要额外的训练技术分数。另一个潜在问题是尖峰活动自然是不可微的，这给 SNN 的监督训练带来了更多困难。在本文中，我们提出了一种用于训练高性能 SNN 的时空反向传播 (STBP) 算法。为了解决 SNN 的不可微分问题，提出了尖峰活动的近似导数，适用于梯度下降训练。 STBP算法结合了逐层空间域（SD）和时间相关时域（TD），不需要任何额外的复杂技巧。我们通过在静态 MNIST 数据集、自定义对象检测数据集和动态 N-MNIST 数据集上采用全连接和卷积架构来评估此方法。结果表明，与尖峰网络上现有的最先进算法相比，我们的方法实现了最佳精度。这项工作为研究具有丰富时空动态的未来类脑计算范式的高性能 SNN 提供了新的视角。</p>
<h2 id="References-13"><a href="#References-13" class="headerlink" title="References"></a>References</h2><p>利用snn实现手写数字识别，训练算法为STBP(作者提出)，准确度提升为99.4%。<br>GitHub地址：<br><a target="_blank" rel="noopener" href="https://github.com/yjwu17/BP-for-SpikingNN">https://github.com/yjwu17/BP-for-SpikingNN</a><br>配套论文：<br>Wu, Yujie, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. “Direct Training for Spiking Neural Networks: Faster, Larger, Better.” arXiv preprint arXiv:1809.05793 (2018).<br><strong>Wu, Yujie, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. “Spatio-temporal backpropagation for training high-performance spiking neural networks.” Frontiers in neuroscience 12 (2018).</strong><br>————————————————<br>版权声明：本文为CSDN博主「Kyrie   开一」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/Kyrie6c/article/details/115289056">https://blog.csdn.net/Kyrie6c/article/details/115289056</a></p>
<hr>
<h1 id="Paper60-1226"><a href="#Paper60-1226" class="headerlink" title="Paper60-1226"></a>Paper60-1226</h1><h2 id="Published-in-18"><a href="#Published-in-18" class="headerlink" title="Published in"></a>Published in</h2><h2 id="Title-18"><a href="#Title-18" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.10085">Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks</a></p>
<h2 id="Authors-16"><a href="#Authors-16" class="headerlink" title="Authors"></a>Authors</h2><p>University of California, Santa Barbara</p>
<p>加利福尼亚大学圣巴巴拉分校</p>
<h2 id="Abstract-25"><a href="#Abstract-25" class="headerlink" title="Abstract"></a>Abstract</h2><p>Spiking neural networks (SNNs) are well suited for spatio-temporal learning and implementations on energy-efficient event-driven neuromorphic processors. However, existing SNN error backpropagation (BP) methods lack proper handling of spiking discontinuities and suffer from low performance compared with the BP methods for traditional artificial neural networks. In addition, a large number of time steps are typically required to achieve decent performance, leading to high latency and rendering spike-based computation unscalable to deep architectures. We present a novel Temporal Spike Sequence Learning Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down error backpropagation across two types of inter-neuron and intra-neuron dependencies and leads to improved temporal learning precision. It captures inter-neuron dependencies through presynaptic firing times by considering the all-or-none characteristics of firing activities and captures intra-neuron dependencies by handling the internal evolution of each neuronal state in time. TSSL-BP efficiently trains deep SNNs within a much shortened temporal window of a few steps while improving the accuracy for various image classification datasets including CIFAR10.</p>
<p>尖峰神经网络 (SNN) 非常适合在高能效事件驱动的神经形态处理器上进行时空学习和实现。然而，与传统人工神经网络的 BP 方法相比，现有的 SNN 误差反向传播 (BP) 方法缺乏对尖峰不连续性的适当处理，并且性能低下。此外，通常需要大量时间步才能实现良好的性能，这会导致高延迟并使基于尖峰的计算无法扩展到深度架构。我们提出了一种新的时间尖峰序列学习反向传播 (TSSL-BP) 方法来训练深度 SNN，它打破了跨两种类型的神经元间和神经元内依赖关系的误差反向传播，并提高了时间学习精度。它通过考虑触发活动的全有或全无特征，通过突触前触发时间捕获神经元间依赖关系，并通过及时处理每个神经元状态的内部演化来捕获神经元内依赖关系。 TSSL-BP 在大大缩短的几个步骤的时间窗口内有效地训练深度 SNN，同时提高包括 CIFAR10 在内的各种图像分类数据集的准确性。</p>
<h2 id="Comments-25"><a href="#Comments-25" class="headerlink" title="Comments"></a>Comments</h2><p>PDF: Portable Document Format</p>
<h2 id="References-14"><a href="#References-14" class="headerlink" title="References"></a>References</h2><p>利用snn实现对于多个数据集的识别，训练算法TSSL-BP（作者提出），2020年提出的。<br>GitHub地址：<br><a target="_blank" rel="noopener" href="https://github.com/stonezwr/TSSL-BP">https://github.com/stonezwr/TSSL-BP</a><br>论文地址：<br><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2020/hash/8bdb5058376143fa358981954e7626b8-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/8bdb5058376143fa358981954e7626b8-Abstract.html</a><br>目前就这么多，应该还会更新。也有可能不会更新(坑太深的话可能就溜了)。<br>————————————————<br>版权声明：本文为CSDN博主「Kyrie   开一」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/Kyrie6c/article/details/115289056">https://blog.csdn.net/Kyrie6c/article/details/115289056</a></p>
<hr>
<h1 id="Paper57-1223"><a href="#Paper57-1223" class="headerlink" title="Paper57-1223"></a>Paper57-1223</h1><h2 id="Published-in-19"><a href="#Published-in-19" class="headerlink" title="Published in"></a>Published in</h2><p>IEEE Transactions On Computer-Aided Design Of Integrated Circuits And System</p>
<h2 id="Title-19"><a href="#Title-19" class="headerlink" title="Title"></a>Title</h2><h2 id="Authors-17"><a href="#Authors-17" class="headerlink" title="Authors"></a>Authors</h2><p><sup>1,2</sup> <sup>1</sup> <sup>2</sup> <sup>3</sup></p>
<h2 id="Abstract-26"><a href="#Abstract-26" class="headerlink" title="Abstract"></a>Abstract</h2><h2 id="Keywords-Index-Terms-7"><a href="#Keywords-Index-Terms-7" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-26"><a href="#Comments-26" class="headerlink" title="Comments"></a>Comments</h2><h2 id="References-15"><a href="#References-15" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper53-1228"><a href="#Paper53-1228" class="headerlink" title="Paper53-1228"></a>Paper53-1228</h1><h2 id="Title-20"><a href="#Title-20" class="headerlink" title="Title"></a>Title</h2><p>Preparation of Articles for IEEE TRANSACTIONS and JOURNALS (2021)</p>
<h2 id="Comments-27"><a href="#Comments-27" class="headerlink" title="Comments"></a>Comments</h2><p>这是TCAS-I和TVLSI期刊的模板</p>
<hr>
<h1 id="Paper52-0108"><a href="#Paper52-0108" class="headerlink" title="Paper52-0108"></a>Paper52-0108</h1><h2 id="Published-in-20"><a href="#Published-in-20" class="headerlink" title="Published in"></a>Published in</h2><p>Wu I, Lin PH. NCTU6-lite wins Connect6 tournament. ICGA Journal. 2008 Jan 1;31(4):240-2.</p>
<h2 id="Title-21"><a href="#Title-21" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://content.iospress.com/articles/icga-journal/icg31407">NCTU6-LITE WINS CONNECT6 TOURNAMENT</a></p>
<h2 id="Authors-18"><a href="#Authors-18" class="headerlink" title="Authors"></a>Authors</h2><p>Dept. of Computer Science, National Chiao Tung University, Hsinchu, Taiwan</p>
<p>台湾新竹国立交通大学计算机科学系</p>
<h2 id="Comments-28"><a href="#Comments-28" class="headerlink" title="Comments"></a>Comments</h2><p>六子棋！</p>
<blockquote>
<p>规则与五子棋非常相似，除了第一次黑方下一颗子外，之后黑白双方轮流每次各下两子，先连成六子者获胜。 </p>
<p>因为公平性不是问题，棋盘是可以任意地大，甚至是无限大亦可。对一般玩家而言，采用围棋的十九路棋盘即可，对专业棋士而言，可以采用五十九路棋盘。 </p>
<p>连续6颗算赢，无禁手</p>
<p>发明者是吴毅成教授，中国台湾人，现任国立阳明交通大学资讯工程学系教授。</p>
</blockquote>
<p>这个游戏好像比五子棋公平。</p>
<h2 id="References-16"><a href="#References-16" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/284634562/answer/2281933956">为什么五子棋始终没有发展起来？ - 这里有泉水的回答 - 知乎</a></p>
<hr>
<h1 id="Paper51-0109"><a href="#Paper51-0109" class="headerlink" title="Paper51-0109"></a>Paper51-0109</h1><h2 id="Published-in-21"><a href="#Published-in-21" class="headerlink" title="Published in"></a>Published in</h2><p>Diehl PU, Cook M. Unsupervised learning of digit recognition using spike-timing-dependent plasticity. Frontiers in computational neuroscience. 2015 Aug 3;9:99.</p>
<p>Cite by <strong>821</strong></p>
<h2 id="Title-22"><a href="#Title-22" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.frontiersin.org/articles/10.3389/fncom.2015.00099/full#">Unsupervised learning of digit recognition using spike-timing-dependent plasticity</a></p>
<h2 id="Authors-19"><a href="#Authors-19" class="headerlink" title="Authors"></a>Authors</h2><p>Institute of Neuroinformatics, ETH Zurich and University Zurich, Zurich, Switzerland</p>
<p>神经信息学研究所，苏黎世联邦理工学院和苏黎世大学，苏黎世，瑞士</p>
<h2 id="Abstract-27"><a href="#Abstract-27" class="headerlink" title="Abstract"></a>Abstract</h2><p>In order to understand how the mammalian neocortex is performing computations, two things are necessary; we need to have a good understanding of the available neuronal processing units and mechanisms, and we need to gain a better understanding of how those mechanisms are combined to build functioning systems. Therefore, in recent years there is an increasing interest in how spiking neural networks (SNN) can be used to perform complex computations or solve pattern recognition tasks. However, it remains a challenging task to design SNNs which use biologically plausible mechanisms (especially for learning new patterns), since most such SNN architectures rely on training in a rate-based network and subsequent conversion to a SNN. We present a SNN for digit recognition which is based on mechanisms with increased biological plausibility, i.e., conductance-based instead of current-based synapses, spike-timing-dependent plasticity with time-dependent weight change, lateral inhibition, and an adaptive spiking threshold. Unlike most other systems, we do not use a teaching signal and do not present any class labels to the network. Using this unsupervised learning scheme, our architecture achieves 95% accuracy on the MNIST benchmark, which is better than previous SNN implementations without supervision. The fact that we used no domain-specific knowledge points toward the general applicability of our network design. Also, the performance of our network scales well with the number of neurons used and shows similar performance for four different learning rules, indicating robustness of the full combination of mechanisms, which suggests applicability in heterogeneous biological neural networks.</p>
<p>为了理解哺乳动物新皮层是如何进行计算的，有两件事是必要的；我们需要对可用的神经元处理单元和机制有一个很好的理解，我们需要更好地理解这些机制是如何结合起来构建功能系统的。因此，近年来人们越来越关注如何使用脉冲神经网络 (SNN) 执行复杂的计算或解决模式识别任务。然而，设计使用生物学上似是而非的机制（尤其是学习新模式）的 SNN 仍然是一项具有挑战性的任务，因为大多数此类 SNN 架构都依赖于在基于速率的网络中进行训练并随后转换为 SNN。我们提出了一种用于数字识别的 SNN，它基于具有更高生物学合理性的机制，即基于电导而不是基于电流的突触、具有时间依赖性重量变化的尖峰时间依赖性可塑性、侧向抑制和自适应尖峰阈值.与大多数其他系统不同，我们不使用教学信号，也不向网络提供任何类别标签。使用这种无监督学习方案，我们的架构在 MNIST 基准上实现了 95% 的准确率，这比之前没有监督的 SNN 实现要好。我们没有使用特定领域的知识这一事实表明了我们网络设计的普遍适用性。此外，我们的网络的性能与使用的神经元数量有很好的扩展性，并且在四种不同的学习规则下表现出相似的性能，表明机制的完整组合具有鲁棒性，这表明在异构生物神经网络中的适用性。</p>
<h2 id="Keywords-Index-Terms-8"><a href="#Keywords-Index-Terms-8" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>spiking neural network, STDP, unsupervised learning, classification, digit recognition</p>
<p>脉冲神经网络，STDP，无监督学习，分类，数字识别</p>
<h2 id="Comments-29"><a href="#Comments-29" class="headerlink" title="Comments"></a>Comments</h2><p>价值很高。未来几天可以针对这些项目看论文。</p>
<blockquote>
<p>利用snn实现手写数字识别,训练算法为STDP（snn比较经典的项目，比较适合用来入门）准确度大概为91.56%：<br>GitHub地址：<br><a target="_blank" rel="noopener" href="https://github.com/peter-u-diehl/stdp-mnist">https://github.com/peter-u-diehl/stdp-mnist</a><br>配套论文地址：<br><a target="_blank" rel="noopener" href="http://journal.frontiersin.org/article/10.3389/fncom.2015.00099/abstract#">http://journal.frontiersin.org/article/10.3389/fncom.2015.00099/abstract#</a><br>————————————————<br>版权声明：本文为CSDN博主「Kyrie   开一」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/Kyrie6c/article/details/115289056">https://blog.csdn.net/Kyrie6c/article/details/115289056</a></p>
</blockquote>
<h2 id="References-17"><a href="#References-17" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Kyrie6c/article/details/115289056?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link&amp;utm_relevant_index=5">2021值得做的脉冲神经网络(snn)项目加论文GitHub地址</a></p>
<hr>
<h1 id="Paper50-0110"><a href="#Paper50-0110" class="headerlink" title="Paper50-0110"></a>Paper50-0110</h1><h2 id="Published-in-22"><a href="#Published-in-22" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8919">IEEE Transactions on Circuits and Systems I: Regular Papers</a> ( Early Access )</p>
<h2 id="Title-23"><a href="#Title-23" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9497696">MuGRA: A Scalable Multi-Grained Reconfigurable Accelerator Powered by Elastic Neural Network</a></p>
<h2 id="Authors-20"><a href="#Authors-20" class="headerlink" title="Authors"></a>Authors</h2><p>Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma 630-0192, Japan</p>
<p>信息科学研究科，奈良科学技术学院，<strong>生驹</strong>（？） 630-0192，日本</p>
<h2 id="Abstract-28"><a href="#Abstract-28" class="headerlink" title="Abstract"></a>Abstract</h2><p>A massive core computing architecture is developed for accelerating arbitrary calculations in fully parallel with high speed and low cost. The proposed architecture is reconfigurable in fine-grained (arbitrary functions), mid-grained (flexible function feature, accuracy, and number of operands), and coarse-grained (organization of cores). By implementing a large scale of novel bisection neural network (BNN) on hardware, the re-configuration is conducted by partitioning entire BNN into any specific pieces without redundancy. Each piece of BNN retrieves the arbitrary function approximately. By reconfiguring the BNN topology in software, we can easily adjust dimensions of the computing kernel without rewiring, and achieve a wide range of trade-offs between accuracy and efficiency in hardware. In this manner, the multi-grained reconfigurable accelerator (MuGRA) is achieved. Since MuGRA is flexible in all grained levels, various configurations for each validation are demonstrated with rich options of performance-cost matrix. From the FPGA implementation results, compared with other traditional function approximation methods, our method provides fewer parameter storage requirements. The comparison against related works proves that our accelerator effectively reduces the calculation latency with slight accuracy loss.</p>
<p>开发了海量核心计算架构，以高速、低成本地以完全并行的方式加速任意计算。所提出的架构可在细粒度（任意函数）、中粒度（灵活的函数特征、精度和操作数数量）和粗粒度（核心组织）中重新配置。通过在硬件上实现大规模的新型二分神经网络 (BNN)，重新配置是通过将整个 BNN 划分为任何特定的片段而没有冗余。每个 BNN 都近似检索任意函数。通过在软件中重新配置 BNN 拓扑，我们可以在不重新布线的情况下轻松调整计算内核的维度，并在硬件中实现精度和效率之间的广泛权衡。通过这种方式，实现了多粒度可重构加速器（MuGRA）。由于 MuGRA 在所有粒度级别上都很灵活，因此每个验证的各种配置都通过性能成本矩阵的丰富选项进行了演示。从FPGA的实现结果来看，与其他传统的函数逼近方法相比，我们的方法提供了更少的参数存储需求。与相关工作的对比证明，我们的加速器有效降低了计算延迟，但精度损失很小。</p>
<h2 id="Keywords-Index-Terms-9"><a href="#Keywords-Index-Terms-9" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><ul>
<li>IEEE Keywords<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Computer architecture&amp;newsearch=true">Computer architecture</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Artificial neural networks&amp;newsearch=true">Artificial neural networks</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Hardware&amp;newsearch=true">Hardware</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Neurons&amp;newsearch=true">Neurons</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Field programmable gate arrays&amp;newsearch=true">Field programmable gate arrays</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Time division multiplexing&amp;newsearch=true">Time division multiplexing</a>,时分复用</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Parallel processing&amp;newsearch=true">Parallel processing</a></li>
</ul>
</li>
<li>Author Keywords<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Massive core&amp;newsearch=true">Massive core</a>, <strong>巨大的核心</strong>（？）</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:reconfigurable architecture&amp;newsearch=true">reconfigurable architecture</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:neural network&amp;newsearch=true">neural network</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:approximate computing&amp;newsearch=true">approximate computing</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:FPGA implementation&amp;newsearch=true">FPGA implementation</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:function generator.&amp;newsearch=true">function generator.</a></li>
</ul>
</li>
</ul>
<h2 id="Comments-30"><a href="#Comments-30" class="headerlink" title="Comments"></a>Comments</h2><p>引用了cALP</p>
<hr>
<h1 id="Paper49-1218"><a href="#Paper49-1218" class="headerlink" title="Paper49-1218"></a>Paper49-1218</h1><h2 id="Published-in-23"><a href="#Published-in-23" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639">IEEE Access</a> ( Volume: 9)</p>
<h2 id="Title-24"><a href="#Title-24" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9513316">Drone Detection Sensor With Continuous 2.4 GHz ISM Band Coverage Based on Cost-Effective SDR Platform</a></p>
<h2 id="Authors-21"><a href="#Authors-21" class="headerlink" title="Authors"></a>Authors</h2><p><sup>1,2</sup> <sup>1</sup> <sup>2</sup> <sup>3</sup></p>
<h2 id="Abstract-29"><a href="#Abstract-29" class="headerlink" title="Abstract"></a>Abstract</h2><p>The development of Unmanned Aerial Vehicles (UAVs), commonly referred to as drones, has introduced revolutionary changes in many areas over the past few years. However, aside from opening new possibilities, the usage of drones in an irresponsible and dangerous manner leads to many hazardous incidents. This paper presents a drone detection sensor with a continuous 2.400 GHz-2.483 GHz operational frequency range for detection methods based on passive radio frequency imaging techniques. The implementation based on Software Defined Radio (SDR) and Field Programmable Logic Array (FPGA) hardware that overcomes the 40 MHz real-time bandwidth limit of other popular SDRs is presented utilizing low-cost off-the-shelf components. Furthermore, a hardware realization of the signal processing chain for specific detection algorithms is proposed to minimize the throughput between SDR and the companion computer and offload software computations. The device validation is made in a laboratory and real-life scenario and presented in relation to the sensor used in other works. In addition to the increased real-time bandwidth, the measurements show a 9 dB reduction in detection sensitivity compared to the reference receiver, in line with the analog RF front-end specifications. The final analysis demonstrates the proposed device’s relevance as a sensor for obtaining machine learning datasets and as a part of a final anti-drone system.</p>
<p>无人驾驶飞行器 (UAV)（通常称为无人机）的发展在过去几年中在许多领域带来了革命性的变化。然而，除了开辟新的可能性之外，以不负责任和危险的方式使用无人机会导致许多危险事件。本文介绍了一种无人机检测传感器，其工作频率范围为 2.400 GHz-2.483 GHz，适用于基于无源射频成像技术的检测方法。利用低成本的现成组件展示了基于软件定义无线电 (SDR) 和现场可编程逻辑阵列 (FPGA) 硬件的实现，该硬件克服了其他流行 SDR 的 40 MHz 实时带宽限制。此外，还提出了用于特定检测算法的信号处理链的硬件实现，以最小化 SDR 与配套计算机之间的吞吐量并卸载软件计算。设备验证是在实验室和现实场景中进行的，并与其他作品中使用的传感器相关。除了增加的实时带宽外，与参考接收器相比，测量显示检测灵敏度降低了 9 dB，符合模拟 RF 前端规范。最终分析证明了所提出的设备作为获取机器学习数据集的传感器和最终反无人机系统的一部分的相关性。</p>
<h2 id="Keywords-Index-Terms-10"><a href="#Keywords-Index-Terms-10" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-31"><a href="#Comments-31" class="headerlink" title="Comments"></a>Comments</h2><p>引用cALP。领域差距较远。</p>
<h2 id="References-18"><a href="#References-18" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper47-1216"><a href="#Paper47-1216" class="headerlink" title="Paper47-1216"></a>Paper47-1216</h1><h2 id="Published-in-24"><a href="#Published-in-24" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8919">IEEE Transactions on Circuits and Systems I: Regular Papers</a> ( Volume: 68, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=9481308">Issue: 8</a>, Aug. 2021)</p>
<h2 id="Title-25"><a href="#Title-25" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9439900">Low-Complexity High-Precision Method and Architecture for Computing the Logarithm of Complex Numbers</a></p>
<h2 id="Authors-22"><a href="#Authors-22" class="headerlink" title="Authors"></a>Authors</h2><p><sup>1,2</sup> <sup>1</sup> <sup>2</sup> <sup>3</sup></p>
<h2 id="Abstract-30"><a href="#Abstract-30" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper proposes a low-complexity method and architecture to compute the logarithm of complex numbers based on coordinate rotation digital computer (CORDIC). Our method takes advantage of the vector mode of circular CORDIC and hyperbolic CORDIC, which only needs shift-add operations in its hardware implementation. Our architecture has lower design complexity and higher performance compared with conventional architectures. Through software simulation, we show that this method can achieve high precision for logarithm computation, reaching the relative error of 10 -7 . Finally, we design and implement an example circuit under TSMC 28nm CMOS technology. According to the synthesis report, our architecture has smaller area, lower power consumption, higher precision and wider operation range compared with the alternative architectures.</p>
<p>本文提出了一种基于坐标旋转数字计算机（CORDIC）计算复数对数的低复杂度方法和架构。 我们的方法利用了循环 CORDIC 和双曲线 CORDIC 的矢量模式，它们只需要在其硬件实现中进行移位加运算。 与传统架构相比，我们的架构具有更低的设计复杂性和更高的性能。 通过软件仿真，我们表明该方法可以实现对数计算的高精度，相对误差达到10 -7 。 最后，我们在台积电 28nm CMOS 技术下设计并实现了一个示例电路。 根据综合报告，与替代架构相比，我们的架构具有更小的面积、更低的功耗、更高的精度和更宽的操作范围。</p>
<h2 id="Keywords-Index-Terms-11"><a href="#Keywords-Index-Terms-11" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-32"><a href="#Comments-32" class="headerlink" title="Comments"></a>Comments</h2><p>引用cALP。基于CORDIC。</p>
<h2 id="References-19"><a href="#References-19" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper46-1214"><a href="#Paper46-1214" class="headerlink" title="Paper46-1214"></a>Paper46-1214</h1><h2 id="Published-in-25"><a href="#Published-in-25" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8919">IEEE Transactions on Circuits and Systems I: Regular Papers</a> ( Volume: 68, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=9325891">Issue: 2</a>, Feb. 2021)</p>
<h2 id="Title-26"><a href="#Title-26" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9275371">Ultralow-Latency VLSI Architecture Based on a Linear Approximation Method for Computing <em>N</em>th Roots of Floating-Point Numbers</a></p>
<h2 id="Authors-23"><a href="#Authors-23" class="headerlink" title="Authors"></a>Authors</h2><p><sup>1,2</sup> <sup>1</sup> <sup>2</sup> <sup>3</sup></p>
<h2 id="Abstract-31"><a href="#Abstract-31" class="headerlink" title="Abstract"></a>Abstract</h2><p>State-of-the-art approaches that perform root computations based on the COordinate Rotation Digital Computer (CORDIC) algorithm suffer from high latency in performing multiple iterations. Therefore, root computations based on the CORDIC algorithm cannot meet the strict latency requirements of some applications. <strong>In this paper, we propose a methodology for performing Nth root computations on floating-point numbers based on the piecewise linear (PWL) approximation method.</strong> The proposed method divides an Nth root computation into several subtasks approximated by the PWL algorithm. It determines the widest segments of the subtasks and the smallest fractional width needed to satisfy the predefined maximum relative error Max_Err r . Our design is coded in Verilog HDL and synthesized under TSMC 40 nm CMOS technology. The synthesized results show that our design can reach the highest frequency of 2.703 GHz with an area consumption of 2608.84 μ m 2 and a power consumption of 2.4476 mW. Compared with one stateof-the-art architecture, our design saves 91.60%, 89.84%, and 63.33% of the area, power, and latency @1.89GHz frequency, respectively, while reducing Max_Err r by 57.30%. In addition, it saves 94.52%, 92.68%, and 73.17% of the area, power, and delay @1.89GHz frequency, respectively, and reduces Max_Err r by 1.65% when compared with the other state-of-the-art design.</p>
<p>基于坐标旋转数字计算机 (CORDIC) 算法执行根计算的最先进方法在执行多次迭代时存在高延迟。因此，基于 CORDIC 算法的根计算无法满足某些应用程序的严格延迟要求。在本文中，我们提出了一种基于分段线性 (PWL) 近似方法对浮点数执行 N 次根计算的方法。所提出的方法将第 N 个根计算划分为由 PWL 算法近似的几个子任务。它确定子任务的最宽段和满足预定义最大相对误差 Max_Err r 所需的最小分数宽度。我们的设计采用 Verilog HDL 编码，并在 TSMC 40 nm CMOS 技术下合成。综合结果表明，我们的设计可以达到2.703 GHz的最高频率，面积消耗为2608.84 μ m 2 ，功耗为2.4476 mW。与一种最先进的架构相比，我们的设计在 1.89GHz 频率下分别节省了 91.60%、89.84% 和 63.33% 的面积、功耗和延迟，同时将 Max_Err r 降低了 57.30%。此外，与其他最先进的设计相比，它在 1.89GHz 频率下分别节省了 94.52%、92.68% 和 73.17% 的面积、功耗和延迟，并将 Max_Err r 降低了 1.65%。</p>
<h2 id="Keywords-Index-Terms-12"><a href="#Keywords-Index-Terms-12" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-33"><a href="#Comments-33" class="headerlink" title="Comments"></a>Comments</h2><p>引用cALP。计算N次方根用的。</p>
<h2 id="References-20"><a href="#References-20" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Slides01"><a href="#Slides01" class="headerlink" title="Slides01"></a>Slides01</h1><h2 id="Published-in-26"><a href="#Published-in-26" class="headerlink" title="Published in"></a>Published in</h2><p>June 2019</p>
<h2 id="Title-27"><a href="#Title-27" class="headerlink" title="Title"></a>Title</h2><p>Writing a good ISSCC paper</p>
<h2 id="Authors-24"><a href="#Authors-24" class="headerlink" title="Authors"></a>Authors</h2><p>Jan Van der Spiegel</p>
<p>Kenneth C. Smith</p>
<h2 id="Abstract-32"><a href="#Abstract-32" class="headerlink" title="Abstract"></a>Abstract</h2><p>These slides are originally from a presentation given at the A-SSCS in Hangzhou, China - November 2006.</p>
<h2 id="Keywords-Index-Terms-13"><a href="#Keywords-Index-Terms-13" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-34"><a href="#Comments-34" class="headerlink" title="Comments"></a>Comments</h2><h2 id="References-21"><a href="#References-21" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper45-1213"><a href="#Paper45-1213" class="headerlink" title="Paper45-1213"></a>Paper45-1213</h1><h2 id="Published-in-27"><a href="#Published-in-27" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IEEE Transactions on Neural Networks and Learning Systems</a> ( Early Access )</p>
<h2 id="Title-28"><a href="#Title-28" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9354506">StructADMM: Achieving Ultrahigh Efficiency in Structured Pruning for DNNs</a></p>
<h2 id="Authors-25"><a href="#Authors-25" class="headerlink" title="Authors"></a>Authors</h2><p>Tianyun Zhang<sup>1</sup> Shaokai Ye<sup>2</sup> Xiaoyu Feng<sup>2</sup> Xiaolong Ma<sup>3</sup> Kaiqi Zhang<sup>1</sup> Zhengang Li<sup>3</sup> Jian Tang<sup>1</sup> Sijia Liu<sup>4</sup> Xue Lin<sup>3</sup> Yongpan Liu<sup>2</sup></p>
<p>Makan Fardad<sup>1</sup> and Yanzhi Wang<sup>3</sup> (e-mail: yanz.wang@northeastern.edu)</p>
<p><sup>1</sup>Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY 13244 USA.</p>
<p>雪城大学电气工程和计算机科学系，美国纽约州雪城 13244。</p>
<p><sup>2</sup>Department of Electronic Engineering, Tsinghua University, Beijing 100084, China.</p>
<p>清华大学电子工程系，北京 100084。</p>
<p><sup>3</sup>Department of Electrical and Computer Engineering, Northeastern University, Boston, MA 02115 USA.</p>
<p>东北大学电气与计算机工程系，波士顿，马萨诸塞州 02115 美国。</p>
<p><sup>4</sup>IBM Cambridge Research Center, Cambridge, MA 02141 USA.</p>
<p>IBM 剑桥研究中心，剑桥，马萨诸塞州 02141 美国。</p>
<h2 id="Abstract-33"><a href="#Abstract-33" class="headerlink" title="Abstract"></a>Abstract</h2><p>Weight pruning methods of deep neural networks (DNNs) have been demonstrated to achieve a good model pruning rate without loss of accuracy, thereby alleviating the significant computation/storage requirements of large-scale DNNs. Structured weight pruning methods have been proposed to overcome the limitation of irregular network structure and demonstrated actual GPU acceleration. However, in prior work, the pruning rate (degree of sparsity) and GPU acceleration are limited (to less than 50%) when accuracy needs to be maintained. In this work, we overcome these limitations by proposing a unified, systematic framework of structured weight pruning for DNNs. It is a framework that can be used to induce different types of structured sparsity, such as filterwise, channelwise, and shapewise sparsity, as well as nonstructured sparsity. The proposed framework incorporates stochastic gradient descent (SGD; or ADAM) with alternating direction method of multipliers (ADMM) and can be understood as a dynamic regularization method in which the regularization target is analytically updated in each iteration. Leveraging special characteristics of ADMM, we further propose a progressive, multistep weight pruning framework and a network purification and unused path removal procedure, in order to achieve higher pruning rate without accuracy loss. Without loss of accuracy on the AlexNet model, we achieve 2.58x and 3.65x average measured speedup on two GPUs, clearly outperforming the prior work. The average speedups reach 3.15x and 8.52x when allowing a moderate accuracy loss of 2%. In this case, the model compression for convolutional layers is 15.0x, corresponding to 11.93x measured CPU speedup. As another example, for the ResNet-18 model on the CIFAR-10 data set, we achieve an unprecedented 54.2x structured pruning rate on CONV layers. This is 32x higher pruning rate compared with recent work and can further translate into 7.6x inference time speedup on the Adreno 640 mobile GPU compared with the original, unpruned DNN model. We share our codes and models at the link <a target="_blank" rel="noopener" href="http://bit.ly/2M0V7DO">http://bit.ly/2M0V7DO</a>.</p>
<p>深度神经网络 (DNN) 的权重剪枝方法已被证明可以在不损失准确性的情况下实现良好的模型剪枝率，从而减轻大规模 DNN 的显着计算/存储需求。已经提出了结构化权重剪枝方法来克服不规则网络结构的限制，并证明了实际的 GPU 加速。然而，在之前的工作中，当需要保持准确性时，修剪率（稀疏度）和 GPU 加速是有限的（小于 50%）。在这项工作中，我们通过为 DNN 提出一个统一的、系统的结构化权重修剪框架来克服这些限制。它是一个框架，可用于诱导不同类型的结构化稀疏性，例如 filterwise、channelwise 和 shapewise 稀疏性，以及非结构化稀疏性。所提出的框架将随机梯度下降（SGD；或 ADAM）与乘法器交替方向法（ADMM）相结合，可以理解为一种动态正则化方法，其中正则化目标在每次迭代中都被解析更新。利用 ADMM 的特性，我们进一步提出了渐进式、多步权重剪枝框架和网络净化和未使用路径去除程序，以在不损失精度的情况下实现更高的剪枝率。在不损失 AlexNet 模型精度的情况下，我们在两个 GPU 上实现了 2.58 倍和 3.65 倍的平均测量加速，明显优于之前的工作。当允许 2% 的中等精度损失时，平均加速达到 3.15 倍和 8.52 倍。在这种情况下，卷积层的模型压缩为 15.0 倍，对应于 11.93 倍的实测 CPU 加速。再举一个例子，对于 CIFAR-10 数据集上的 ResNet-18 模型，我们在 CONV 层上实现了前所未有的 54.2 倍结构化修剪率。与最近的工作相比，修剪率提高了 32 倍，并且与原始的未修剪 DNN 模型相比，Adreno 640 移动 GPU 上的推理时间加速了 7.6 倍。我们在链接 <a target="_blank" rel="noopener" href="http://bit.ly/2M0V7DO">http://bit.ly/2M0V7DO</a> 上分享我们的代码和模型。</p>
<h2 id="Keywords-Index-Terms-14"><a href="#Keywords-Index-Terms-14" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><ul>
<li>IEEE Keywords<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Graphics processing units&amp;newsearch=true">Graphics processing units</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Acceleration&amp;newsearch=true">Acceleration</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Convex functions&amp;newsearch=true">Convex functions</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Optimization&amp;newsearch=true">Optimization</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Quantization .LB.signal.RB.&amp;newsearch=true">Quantization (signal)</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Degradation&amp;newsearch=true">Degradation</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Periodic structures&amp;newsearch=true">Periodic structures</a></li>
</ul>
</li>
<li>Author Keywords<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Alternating direction method of multipliers .LB.ADMM.RB.&amp;newsearch=true">Alternating direction method of multipliers (ADMM)</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:deep neural networks .LB.DNNs.RB.&amp;newsearch=true">deep neural networks (DNNs)</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:hardware acceleration&amp;newsearch=true">hardware acceleration</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:weight pruning.&amp;newsearch=true">weight pruning.</a></li>
</ul>
</li>
</ul>
<h2 id="Comments-35"><a href="#Comments-35" class="headerlink" title="Comments"></a>Comments</h2><h2 id="References-22"><a href="#References-22" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper44-0107"><a href="#Paper44-0107" class="headerlink" title="Paper44-0107"></a>Paper44-0107</h1><h2 id="Published-in-28"><a href="#Published-in-28" class="headerlink" title="Published in"></a>Published in</h2><p>Wang Y, Ye S, He Z, Ma X, Zhang L, Lin S, Yuan G, Tan SH, Li Z, Fan D, Qian X. Non-structured DNN weight pruning considered harmful. arXiv preprint arXiv:1907.02124. 2019 Jul.</p>
<h2 id="Title-29"><a href="#Title-29" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://deepai.org/publication/non-structured-dnn-weight-pruning-considered-harmful">Non-structured DNN Weight Pruning Considered Harmful</a></p>
<h2 id="Authors-26"><a href="#Authors-26" class="headerlink" title="Authors"></a>Authors</h2><p>Dept. of Electrical &amp; Computer Engineering, Northeastern University, Boston, MA, USA</p>
<p>美国马萨诸塞州波士顿东北大学电气与计算机工程系</p>
<p>Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China</p>
<p>清华大学交叉信息科学研究所，北京，中国</p>
<p>Dept. of Electrical &amp; Computer Engineering, University of Central Florida, Orlando, FL, USA</p>
<p>美国佛罗里达州奥兰多中佛罗里达大学电气与计算机工程系</p>
<p>Dept. of Electrical &amp; Computer Engineering, University of Southern California, Los Angeles, CA, USA</p>
<p>美国加利福尼亚州洛杉矶南加州大学电气与计算机工程系</p>
<h2 id="Abstract-34"><a href="#Abstract-34" class="headerlink" title="Abstract"></a>Abstract</h2><p>Large deep <a target="_blank" rel="noopener" href="https://deepai.org/machine-learning-glossary-and-terms/neural-network">neural network</a> (DNN) models pose the key challenge to energy efficiency due to the significantly higher energy consumption of off-chip DRAM accesses than arithmetic or SRAM operations. It motivates the intensive research on model compression with two main approaches. Weight pruning leverages the redundancy in the number of weights and can be performed in a non-structured, which has higher flexibility and pruning rate but incurs index accesses due to irregular weights, or structured manner, which preserves the full matrix structure with lower pruning rate. Weight quantization leverages the redundancy in the number of bits in weights. Compared to pruning, quantization is much more hardware-friendly, and has become a “must-do” step for FPGA and ASIC implementations. This paper provides a definitive answer to the question for the first time. First, we build ADMM-NN-S by extending and enhancing ADMM-NN, a recently proposed joint weight pruning and quantization framework. Second, we develop a methodology for fair and fundamental comparison of non-structured and structured pruning in terms of both storage and computation efficiency. Our results show that ADMM-NN-S consistently outperforms the prior art: (i) it achieves 348x, 36x, and 8x overall weight pruning on LeNet-5, AlexNet, and ResNet-50, respectively, with (almost) zero accuracy loss; (ii) we demonstrate the first fully <a target="_blank" rel="noopener" href="https://deepai.org/machine-learning-glossary-and-terms/binarization">binarized</a> (for all layers) DNNs can be lossless in accuracy in many cases. These results provide a strong baseline and credibility of our study. Based on the proposed comparison framework, with the same accuracy and quantization, the results show that non-structrued pruning is not competitive in terms of both storage and computation efficiency. Thus, we conclude that non-structured pruning is considered harmful. We urge the community not to continue the DNN inference acceleration for non-structured sparsity.</p>
<p>由于片外 DRAM 访问的能耗明显高于算术或 SRAM 操作，因此大型深度神经网络 (DNN) 模型对能效提出了关键挑战。它通过两种主要方法激发了对模型压缩的深入研究。权重剪枝利用权重数量的冗余，可以在非结构化中进行，具有更高的灵活性和剪枝率，但由于权重不规则会导致索引访问，或者结构化方式，保留完整矩阵结构，剪枝率较低.权重量化利用权重中位数的冗余。与剪枝相比，量化对硬件更加友好，并且已成为 FPGA 和 ASIC 实现的“必须”步骤。这篇论文首次对这个问题给出了明确的答案。首先，我们通过扩展和增强 ADMM-NN（一种最近提出的联合权重修剪和量化框架）来构建 ADMM-NN-S。其次，我们开发了一种在存储和计算效率方面对非结构化和结构化修剪进行公平和基本比较的方法。我们的结果表明 ADMM-NN-S 始终优于现有技术：（i）它分别在 LeNet-5、AlexNet 和 ResNet-50 上实现了 348 倍、36 倍和 8 倍的整体权重剪枝，（几乎）准确度损失为零; (ii) 我们展示了第一个完全二值化的（对于所有层）DNN 在许多情况下在准确性上是无损的。这些结果为我们的研究提供了强大的基线和可信度。基于所提出的比较框架，在相同的精度和量化的情况下，结果表明非结构化剪枝在存储和计算效率方面都没有竞争力。因此，我们得出结论，非结构化修剪被认为是有害的。我们敦促社区不要继续对非结构化稀疏性进行 DNN 推理加速。</p>
<h2 id="Comments-36"><a href="#Comments-36" class="headerlink" title="Comments"></a>Comments</h2><p>不知道为什么选这篇文章</p>
<p>清华叉院参与</p>
<h2 id="References-23"><a href="#References-23" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper43-1211"><a href="#Paper43-1211" class="headerlink" title="Paper43-1211"></a>Paper43-1211</h1><h2 id="Published-in-29"><a href="#Published-in-29" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://www.nature.com/nmicrobiol"><em>Nature Microbiology</em></a></p>
<p>Chen, Y., Zhang, S., Young, E.M. <em>et al.</em> Genetic circuit design automation for yeast. <em>Nat Microbiol</em> <strong>5,</strong> 1349–1360 (2020). <a target="_blank" rel="noopener" href="https://doi.org/10.1038/s41564-020-0757-2">https://doi.org/10.1038/s41564-020-0757-2</a></p>
<h2 id="Title-30"><a href="#Title-30" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41564-020-0757-2">Genetic circuit design automation for yeast</a></p>
<h2 id="Author-information"><a href="#Author-information" class="headerlink" title="Author information"></a>Author information</h2><h3 id="Affiliations"><a href="#Affiliations" class="headerlink" title="Affiliations"></a>Affiliations</h3><ol>
<li><p>Synthetic Biology Center, Department of Biological Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA</p>
<p>美国马萨诸塞州剑桥市麻省理工学院生物工程系合成生物学中心</p>
<p>Ye Chen, Shuyi Zhang, Eric M. Young &amp; Christopher A. Voigt</p>
</li>
<li><p>Department of Electrical and Computer Engineering, Boston University, Boston, MA, USA</p>
<p>波士顿大学电气与计算机工程系，美国马萨诸塞州波士顿</p>
<p>Timothy S. Jones &amp; Douglas Densmore</p>
</li>
</ol>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><p>Y.C. and C.A.V. conceived the study and designed the experiments. S.Z. performed the computational work. E.M.Y. cloned and characterized native yeast parts. T.J. and D.D. designed and developed Cello 2.0. Y.C. performed all of the other experiments and analysed the data. Y.C., E.M.Y. and C.A.V. wrote the manuscript.</p>
<p>Y.C. 和 C.A.V. 构思了这项研究并设计了实验。 S.Z. 完成了计算工作。 E.M.Y. 克隆并表征了天然酵母部分。 T.J. 和 D.D. 设计并开发了 Cello 2.0。 Y.C. 进行了所有其他实验并分析了数据。 Y.C., E.M.Y. 和 C.A.V. 撰写了论文。</p>
<h3 id="Corresponding-author"><a href="#Corresponding-author" class="headerlink" title="Corresponding author"></a>Corresponding author</h3><p>Correspondence to <a href="mailto:cavoigt@gmail.com">Christopher A. Voigt</a>.</p>
<h2 id="Abstract-35"><a href="#Abstract-35" class="headerlink" title="Abstract"></a>Abstract</h2><p>Cells can be programmed to monitor and react to their environment using genetic circuits. Design automation software maps a desired circuit function to a DNA sequence, a process that requires units of gene regulation (gates) that are simple to connect and behave predictably. This poses a challenge for eukaryotes due to their complex mechanisms of transcription and translation. To this end, we have developed gates for yeast (<em>Saccharomyces cerevisiae</em>) that are connected using RNA polymerase flux as the signal carrier and are insulated from each other and host regulation. They are based on minimal constitutive promoters (~120 base pairs), for which rules are developed to insert operators for DNA-binding proteins. Using this approach, we constructed nine NOT/NOR gates with nearly identical response functions and 400-fold dynamic range. In circuits, they are transcriptionally insulated from each other by placing ribozymes downstream of terminators to block nuclear export of messenger RNAs resulting from RNA polymerase readthrough. Based on these gates, Cello 2.0 was used to build circuits with up to 11 regulatory proteins. A simple dynamic model predicts the circuit response over days. Genetic circuit design automation for eukaryotes simplifies the construction of regulatory networks as part of cellular engineering projects, whether it be to stage processes during bioproduction, serve as environmental sentinels or guide living therapeutics.</p>
<p>可以对细胞进行编程，以使用遗传电路监测它们的环境并对其做出反应。设计自动化软件将所需的电路功能映射到 DNA 序列，这一过程需要易于连接且行为可预测的基因调控单元（门）。由于其复杂的转录和翻译机制，这对真核生物提出了挑战。为此，我们开发了酵母（<em>酿酒酵母</em>）的门，这些门使用 RNA 聚合酶通量作为信号载体进行连接，并相互隔离和宿主调节。它们基于最小组成型启动子（~120 碱基对），为此制定了规则来插入 DNA 结合蛋白的操作符。使用这种方法，我们构建了九个具有几乎相同响应函数和 400 倍动态范围的非/或非门。在电路中，它们通过在终止子下游放置核酶来阻止由 RNA 聚合酶通读产生的信使 RNA 的核输出，从而在转录上相互隔离。基于这些门，Cello 2.0 被用来构建具有多达 11 个调节蛋白的电路。一个简单的动态模型可以预测几天内的电路响应。真核生物的基因电路设计自动化简化了作为细胞工程项目一部分的监管网络的构建，无论是在生物生产过程中进行阶段处理，还是作为环境哨兵或指导活体治疗。</p>
<h2 id="Keywords-Index-Terms-15"><a href="#Keywords-Index-Terms-15" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>not available</p>
<h2 id="Comments-37"><a href="#Comments-37" class="headerlink" title="Comments"></a>Comments</h2><p>“Access to this article via <strong>Nanjing University Library</strong> is not available.”</p>
<h2 id="References-24"><a href="#References-24" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper42-1212"><a href="#Paper42-1212" class="headerlink" title="Paper42-1212"></a>Paper42-1212</h1><h2 id="Published-in-30"><a href="#Published-in-30" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.00329">arXiv</a></p>
<p>Liu YR, Hu YQ, Qian H, Yu Y, Qian C. Zoopt: Toolbox for derivative-free optimization. arXiv preprint arXiv:1801.00329. 2017 Dec 31.</p>
<p><strong>arXivLabs: experimental projects with community collaborators</strong></p>
<p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
<p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
<p>Have an idea for a project that will add value for arXiv’s community? <a target="_blank" rel="noopener" href="https://labs.arxiv.org/"><strong>Learn more about arXivLabs</strong></a> and <a target="_blank" rel="noopener" href="https://arxiv.org/about/people/developers"><strong>how to get involved</strong></a>.</p>
<h2 id="Title-31"><a href="#Title-31" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.00329">ZOOpt: Toolbox for Derivative-Free Optimization</a></p>
<h2 id="Authors-27"><a href="#Authors-27" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://www.lamda.nju.edu.cn/MainPage.ashx">LAMDA</a>组的同学</p>
<p>National Key Laboratory for Novel Software Technology<br>Nanjing University, Nanjing 210023, China</p>
<p>新型软件技术国家重点实验室<br>南京大学 南京 210023</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yu-Ren Liu</a> <a target="_blank" rel="noopener" href="http://www.lamda.nju.edu.cn/liuyr/">刘驭壬</a></p>
<p>LIUYR@LAMDA.NJU.EDU.CN</p>
<p> <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu%2C+Y">Yi-Qi Hu</a></p>
<p>HUYQ@LAMDA.NJU.EDU.CN</p>
<p> <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qian%2C+H">Hong Qian</a></p>
<p>QIANH@LAMDA.NJU.EDU.CN</p>
<p> <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu%2C+Y">Yang Yu</a></p>
<p>YUY@NJU.EDU.CN</p>
<p>Anhui Province Key Laboratory of Big Data Analysis and Application School of Computer Science and Technology<br>University of Science and Technology of China, Hefei 230027, China</p>
<p>计算机科学与技术学院大数据分析与应用安徽省重点实验室<br>中国科学技术大学 合肥 230027</p>
<p> <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qian%2C+C">Chao Qian</a></p>
<p>CHAOQIAN@USTC.EDU.CN</p>
<h2 id="Abstract-36"><a href="#Abstract-36" class="headerlink" title="Abstract"></a>Abstract</h2><p>Recent advances of derivative-free optimization allow efficient approximating the <strong>global optimal solutions</strong> of sophisticated functions, such as functions with many local optima, non-differentiable and non-continuous functions. This article describes the ZOOpt toolbox that provides efficient derivative-free solvers and are designed easy to use. ZOOpt provides a Python package for single-thread optimization, and a light-weighted distributed version with the help of the Julia language for Python described functions. ZOOpt toolbox particularly focuses on optimization problems in machine learning, addressing high-dimensional, noisy, and large-scale problems. The toolbox is being maintained toward ready-to-use tool in real-world machine learning tasks.</p>
<p>无导数优化的最新进展允许有效逼近复杂函数的<strong>全局最优解</strong>，例如具有许多局部最优、不可微分和非连续函数的函数。 本文介绍了 ZOOpt工具箱，该工具箱提供高效的无导数求解器并且易于使用。 ZOOpt 提供了用于单线程优化的 Python 包，以及借助 Julia 语言为 Python 描述的函数提供的轻量级分布式版本。 ZOOpt 工具箱特别关注机器学习中的优化问题，解决高维、嘈杂和大规模问题。 该工具箱正朝着现实世界机器学习任务中的现成工具的方向发展。</p>
<h2 id="Keywords-Index-Terms-16"><a href="#Keywords-Index-Terms-16" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>Software, Derivative-free optimization, Hyper-parameter optimization, Non-convex optimization, Subset selection, Distributed optimization</p>
<p>软件、无导数优化、超参数优化、非凸优化、子集选择、分布式优化</p>
<h2 id="Comments-38"><a href="#Comments-38" class="headerlink" title="Comments"></a>Comments</h2><p>ZOOpt这个玩意可以用来求一个100元函数的全局最优解。例如可以用于神经网络每一层的参数。</p>
<p>LAMDA的同学搞的。</p>
<h2 id="References-25"><a href="#References-25" class="headerlink" title="References"></a>References</h2><p>参见：</p>
<p><a target="_blank" rel="noopener" href="http://nora.nerc.ac.uk/id/eprint/12620/">User’s manual for the particle tracking model ZOOPT</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/eyounx/ZOOpt">github/eyounx/Zoopt</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/polixir/ZOOpt">github/polixir/ZOOpt</a></p>
<hr>
<h1 id="Paper41-0111"><a href="#Paper41-0111" class="headerlink" title="Paper41-0111"></a>Paper41-0111</h1><h2 id="Published-in-31"><a href="#Published-in-31" class="headerlink" title="Published in"></a>Published in</h2><p><strong><em>Alexander Andreopoulos, Hirak J. Kashyap, Tapan K. Nayak, Arnon Amir, Myron D. Flickner</em></strong>; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 7532-7542</p>
<h2 id="Title-32"><a href="#Title-32" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Andreopoulos_A_Low_Power_CVPR_2018_paper.html">A Low Power, High Throughput, Fully Event-Based Stereo System</a></p>
<h2 id="Abstract-37"><a href="#Abstract-37" class="headerlink" title="Abstract"></a>Abstract</h2><p>We introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-Neumann computation model, where no frames, arrays, or any other such data-structures are used. This is the first time that an end-to-end stereo pipeline from image acquisition and rectification, multi-scale spatio-temporal stereo correspondence, winner-take-all, to disparity regularization is implemented fully on event-based hardware. Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects. System evaluation on event-based sequences demonstrates a ~200X improvement in terms of power per pixel per disparity map compared to the closest state-of-the-art, and maximum latencies of up to 11ms from spike injection to disparity map ejection.</p>
<p>我们介绍了一个完全在基于事件的数字硬件上实现的立体对应系统，使用完全基于图形的非冯诺依曼计算模型，其中不使用帧、数组或任何其他此类数据结构。这是第一次在基于事件的硬件上完全实现从图像采集和校正、多尺度时空立体对应、赢家通吃到视差正则化的端到端立体管道。使用一组 TrueNorth 神经突触处理器，我们展示了它们处理由动态视觉传感器 (DVS) 实时传输的基于双边事件的输入的能力，每秒高达 2,000 个视差图，从而产生高保真视差，进而用于重建，在低功耗下，快速变化的场景产生的事件深度。真实世界序列的实验证明了系统能够充分利用 DVS 传感器的异步和稀疏特性来进行低功率深度重建，在传统的基于帧的相机连接到同步处理器的环境中对于快速移动的物体效率低下.对基于事件的序列的系统评估表明，与最接近的最先进技术相比，每个视差图的每像素功率提高了约 200 倍，从尖峰注入到视差图弹出的最大延迟高达 11 毫秒。</p>
<h2 id="Comments-39"><a href="#Comments-39" class="headerlink" title="Comments"></a>Comments</h2><p>非冯诺依曼架构？LJN看过的。</p>
<p>很离谱，pdf下不下来，先搁置</p>
<hr>
<h1 id="Paper39-1209"><a href="#Paper39-1209" class="headerlink" title="Paper39-1209"></a>Paper39-1209</h1><h2 id="Published-in-32"><a href="#Published-in-32" class="headerlink" title="Published in"></a>Published in</h2><p><strong><a target="_blank" rel="noopener" href="https://www.science.org/journal/science">SCIENCE正刊</a></strong></p>
<p>Nielsen, Alec AK, et al. “Genetic circuit design automation.” <strong><em>Science</em></strong> 352.6281 (2016).</p>
<p><strong><em>Science</em></strong> is a leading outlet for scientific news, commentary, and cutting-edge research. Through its print and online incarnations, <strong><em>Science</em></strong> reaches an estimated worldwide readership of more than one million. <strong><em>Science</em></strong>’s authorship is global too, and its articles consistently rank among the world’s most cited research.</p>
<p><strong><em>Science</em></strong> 是科学新闻、评论和前沿研究的领先媒体。 通过其印刷版和在线版，《<strong><em>Science</em></strong>》估计全球读者人数超过 100 万。 <strong><em>Science</em></strong> 的作者也是全球性的，其文章一直位居世界上被引用次数最多的研究之列。</p>
<h2 id="Title-33"><a href="#Title-33" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aac7341">Genetic circuit design automation</a></p>
<p>基因电路设计自动化</p>
<h2 id="Authors-28"><a href="#Authors-28" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aac7341#pill-con1">ALEC A. K. NIELSEN</a><sup>1</sup></p>
<p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aac7341#pill-con2">BRYAN S. DER</a><sup>1,2</sup></p>
<p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aac7341#pill-con3">JONGHYEON SHIN</a><sup>1</sup></p>
<p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aac7341#pill-con4">PRASHANT VAIDYANATHAN</a><sup>2</sup></p>
<p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aac7341#pill-con5">VANYA PARALANOV</a><sup>3</sup></p>
<p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aac7341#pill-con6">ELIZABETH A. STRYCHALSKI</a><sup>3</sup></p>
<p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aac7341#pill-con7">DAVID ROSS</a><sup>3</sup></p>
<p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aac7341#pill-con8">DOUGLAS DENSMORE</a><sup>2</sup></p>
<p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aac7341#pill-con9">CHRISTOPHER A. VOIGT</a><sup>1</sup></p>
<p>Corresponding author. E-mail: cavoigt@gmail.com</p>
<p><sup>1</sup>Synthetic Biology Center, Department of Biological Engineering, <strong>Massachusetts Institute of Technology</strong>, Cambridge, MA 02139, USA.</p>
<p>麻省理工学院生物工程系合成生物学中心，剑桥，马萨诸塞州 02139，美国。</p>
<p><sup>2</sup>Biological Design Center, Department of Biomedical Engineering, Department of Electrical and Computer Engineering, Boston University, Boston, MA 02215, USA.</p>
<p>生物设计中心，生物医学工程系，电子与计算机工程系，波士顿大学，波士顿，马萨诸塞州 02215，美国。</p>
<p><sup>3</sup>Biosystems and Biomaterials Division, Material Measurement Laboratory, National Institute of Standards and Technology, Gaithersburg, MD 20817, USA.</p>
<p>生物系统和生物材料部，材料测量实验室，美国国家标准与技术研究所，盖瑟斯堡，马里兰州 20817，美国。</p>
<h2 id="Abstract-38"><a href="#Abstract-38" class="headerlink" title="Abstract"></a>Abstract</h2><h3 id="Programming-circuitry-for-synthetic-biology"><a href="#Programming-circuitry-for-synthetic-biology" class="headerlink" title="Programming circuitry for synthetic biology"></a>Programming circuitry for synthetic biology</h3><p>合成生物学的编程电路</p>
<p>As synthetic biology techniques become more powerful, researchers are anticipating a future in which the design of biological circuits will be similar to the design of integrated circuits in electronics. Nielsen <em>et al.</em> describe what is essentially a programming language to design computational circuits in living cells. The circuits generated on plasmids expressed in <em>Escherichia coli</em> required careful insulation from their genetic context, but primarily functioned as specified. The circuits could, for example, regulate cellular functions in response to multiple environmental signals. Such a strategy can facilitate the development of more complex circuits by genetic engineering.</p>
<p>随着合成生物学技术变得越来越强大，研究人员预计未来生物电路的设计将类似于电子学中的集成电路设计。 Nielsen <em>等人</em> 描述了在活细胞中设计计算电路的本质上是一种编程语言。 在<em>大肠杆菌</em> 中表达的质粒上产生的电路需要与它们的遗传环境仔细隔离，但主要按规定发挥作用。 例如，这些电路可以响应多种环境信号来调节细胞功能。 这种策略可以促进通过基因工程开发更复杂的电路。</p>
<h3 id="Structured-Abstract"><a href="#Structured-Abstract" class="headerlink" title="Structured Abstract"></a>Structured Abstract</h3><p>结构化摘要</p>
<h4 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h4><p>介绍</p>
<p>Cells respond to their environment, make decisions, build structures, and coordinate tasks. Underlying these processes are computational operations performed by networks of regulatory proteins that integrate signals and control the timing of gene expression. Harnessing this capability is critical for biotechnology projects that require decision-making, control, sensing, or spatial organization. It has been shown that cells can be programmed using synthetic genetic circuits composed of regulators organized to generate a desired operation. However, the construction of even simple circuits is time-intensive and unreliable.</p>
<p>细胞对它们的环境做出反应、做出决定、构建结构和协调任务。 这些过程的基础是由整合信号和控制基因表达时间的调节蛋白网络执行的计算操作。 利用这种能力对于需要决策、控制、传感或空间组织的生物技术项目至关重要。 已经表明，可以使用由调节器组成的合成遗传电路对细胞进行编程，以产生所需的操作。 然而，即使是简单的电路的构建也是耗时且不可靠的。</p>
<h4 id="RATIONALE"><a href="#RATIONALE" class="headerlink" title="RATIONALE"></a>RATIONALE</h4><p>基本原理</p>
<p>Electronic design automation (EDA) was developed to aid engineers in the design of semiconductor-based electronics. In an effort to accelerate genetic circuit design, we applied principles from EDA to enable increased circuit complexity and to simplify the incorporation of synthetic gene regulation into genetic engineering projects. We used the hardware description language Verilog to enable a user to describe a circuit function. The user also specifies the sensors, actuators, and “user constraints file” (UCF), which defines the organism, gate technology, and valid operating conditions. <a href="[www.cellocad.org](http://www.cellocad.org/">Cello</a>) uses this information to automatically design a DNA sequence encoding the desired circuit. This is done via a set of algorithms that parse the Verilog text, create the circuit diagram, assign gates, balance constraints to build the DNA, and simulate performance.</p>
<p>开发电子设计自动化 (EDA) 是为了帮助工程师设计基于半导体的电子产品。 为了加速基因电路设计，我们应用了 EDA 的原理来增加电路的复杂性并简化将合成基因调控纳入基因工程项目的过程。 我们使用硬件描述语言 Verilog 使用户能够描述电路功能。 用户还指定了传感器、执行器和“用户约束文件”（UCF），它定义了生物体、门技术和有效的操作条件。 <a href="[www.cellocad.org](http://www.cellocad.org/">Cello</a>) 使用此信息自动设计编码所需电路的 DNA 序列。 这是通过一组算法来完成的，这些算法解析 Verilog 文本、创建电路图、分配门、平衡约束以构建 DNA 并模拟性能。</p>
<h4 id="RESULTS"><a href="#RESULTS" class="headerlink" title="RESULTS"></a>RESULTS</h4><p>结果</p>
<p>Cello designs circuits by drawing upon a library of Boolean logic gates. Here, the gate technology consists of NOT/NOR logic based on repressors. Gate connection is simplified by defining the input and output signals as RNA polymerase (RNAP) fluxes. We found that the gates need to be insulated from their genetic context to function reliably in the context of different circuits. Each gate is isolated using strong terminators to block RNAP leakage, and input interchangeability is improved using ribozymes and promoter spacers. These parts are varied for each gate to avoid breakage due to recombination. Measuring the load of each gate and incorporating this into the optimization algorithms further reduces evolutionary pressure.</p>
<p>Cello 通过利用布尔逻辑门库来设计电路。 在这里，门技术由基于抑制器的非/非逻辑组成。 通过将输入和输出信号定义为 RNA 聚合酶 (RNAP) 通量来简化门连接。 我们发现门需要与其遗传环境隔离才能在不同电路的环境中可靠地发挥作用。 每个门都使用强终止子隔离以阻止 RNAP 泄漏，并使用核酶和启动子间隔提高输入互换性。 每个浇口的这些部件都是不同的，以避免由于重新组合而损坏。 测量每个门的负载并将其纳入优化算法进一步降低了进化压力。</p>
<p>Cello was applied to the design of 60 circuits for <em>Escherichia coli</em>, where the circuit function was specified using Verilog code and transformed to a DNA sequence. The DNA sequences were built as specified with no additional tuning, requiring 880,000 base pairs of DNA assembly. Of these, 45 circuits performed correctly in every output state (up to 10 regulators and 55 parts). Across all circuits, 92% of the 412 output states functioned as predicted.</p>
<p>Cello 被应用于<em>大肠杆菌</em> 的 60 个电路设计，其中电路功能使用 Verilog 代码指定并转换为 DNA 序列。 DNA 序列是按规定构建的，没有额外的调整，需要 880,000 个碱基对的 DNA 组装。 其中，45 个电路在每个输出状态下都能正确运行（最多 10 个稳压器和 55 个部件）。 在所有电路中，412 个输出状态中的 92% 都按预期运行。</p>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>结论</p>
<p>Our work constitutes a hardware description language for <strong>programming living cells</strong>. This required the co-development of design algorithms with gates that are sufficiently simple and robust to be connected by automated algorithms. We demonstrate that engineering principles can be applied to identify and suppress errors that complicate the compositions of larger systems. This approach leads to highly repetitive and modular genetics, in stark contrast to the encoding of natural regulatory networks. The use of a hardware-independent language and the creation of additional UCFs will allow a single design to be transformed into DNA for different organisms, genetic endpoints, operating conditions, and gate technologies.</p>
<p>我们的工作构成了一种<strong>用于对活细胞进行编程的硬件描述语言</strong>。 这需要设计算法与门的共同开发，这些门足够简单和稳健，可以通过自动算法连接。 我们证明了工程原理可用于识别和抑制使大型系统的组成复杂化的错误。 这种方法导致高度重复和模块化的遗传学，与自然调节网络的编码形成鲜明对比。 使用独立于硬件的语言和创建额外的 UCF 将允许将单一设计转化为不同生物体、遗传终点、操作条件和门技术的 DNA。</p>
<h3 id="Abstract-39"><a href="#Abstract-39" class="headerlink" title="Abstract"></a>Abstract</h3><p>Computation can be performed in living cells by DNA-encoded circuits that process sensory information and control biological functions. Their construction is time-intensive, requiring manual part assembly and balancing of regulator expression. We describe a design environment, Cello, in which a user writes Verilog code that is automatically transformed into a DNA sequence. Algorithms build a circuit diagram, assign and connect gates, and simulate performance. Reliable circuit design requires the insulation of gates from genetic context, so that they function identically when used in different circuits. We used Cello to design 60 circuits for <em>Escherichia coli</em> (880,000 base pairs of DNA), for which each DNA sequence was built as predicted by the software with no additional tuning. Of these, 45 circuits performed correctly in every output state (up to 10 regulators and 55 parts), and across all circuits 92% of the output states functioned as predicted. Design automation simplifies the incorporation of genetic circuits into biotechnology projects that require decision-making, control, sensing, or spatial organization.</p>
<p>计算可以通过处理感觉信息和控制生物功能的 DNA 编码电路在活细胞中进行。它们的构建需要大量时间，需要手动部件组装和调节器表达的平衡。我们描述了一个设计环境 Cello，在该环境中，<strong>用户编写 Verilog 代码，该代码会自动转换为 DNA 序列</strong>。算法构建电路图，分配和连接门，并模拟性能。可靠的电路设计需要将门与遗传环境隔离，以便它们在不同的电路中使用时具有相同的功能。我们使用 Cello 为<em>大肠杆菌</em>（880,000 个 DNA 碱基对）设计了 60 个电路，其中每个 DNA 序列都是按照软件的预测构建的，无需额外调整。其中，45 个电路在每个输出状态（最多 10 个稳压器和 55 个部件）下都能正确运行，并且在所有电路中，92% 的输出状态都按预期运行。设计自动化简化了将基因回路整合到需要决策、控制、传感或空间组织的生物技术项目中。</p>
<h2 id="Keywords-Index-Terms-17"><a href="#Keywords-Index-Terms-17" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>no keywords</p>
<h2 id="Comments-40"><a href="#Comments-40" class="headerlink" title="Comments"></a>Comments</h2><p>在Google Scholar上搜索circuit design automation搜到的</p>
<p>一群麻省理工的疯子试图用硬件描述语言对活体细胞进行编程？？还得到了七百多个引用？？</p>
<p>不明觉厉不明觉厉</p>
<h2 id="References-26"><a href="#References-26" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper37-1207"><a href="#Paper37-1207" class="headerlink" title="Paper37-1207"></a>Paper37-1207</h1><h2 id="Published-in-33"><a href="#Published-in-33" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://link.springer.com/">Springer</a></p>
<p>It’s a book, not a paper.</p>
<h2 id="Title-34"><a href="#Title-34" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://link.springer.com/book/10.1007/978-3-030-35743-6">Using Artificial Neural Networks for Analog Integrated Circuit Design Automation</a></p>
<h2 id="Authors-29"><a href="#Authors-29" class="headerlink" title="Authors"></a>Authors</h2><p>João P. S. Rosa</p>
<p>Daniel J. D. Guerra</p>
<p>Nuno C. G. Horta</p>
<p>Ricardo M. F. Martins</p>
<p>Nuno C. C. Lourenço</p>
<p>Instituto de Telecomunicações, Instituto Superior Técnico, University of Lisbon, Lisbon, Portugal（葡萄牙语）</p>
<p>电信研究所，Instituto Superior Técnico，里斯本大学，葡萄牙里斯本</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This book addresses the automatic sizing and layout of analog integrated circuits (ICs) using deep learning (DL) and artificial neural networks (ANN). It explores an innovative approach to automatic circuit sizing where ANNs learn patterns from previously optimized design solutions. In opposition to classical optimization-based sizing strategies, where computational intelligence techniques are used to iterate over the map from devices’ sizes to circuits’ performances provided by design equations or circuit simulations, ANNs are shown to be capable of solving analog IC sizing as a direct map from specifications to the devices’ sizes. Two separate ANN architectures are proposed: a Regression-only model and a Classification and Regression model. The goal of the Regression-only model is to learn design patterns from the studied circuits, using circuit’s performances as input features and devices’ sizes as target outputs. This model can size a circuit given its specifications for a single topology. The Classification and Regression model has the same capabilities of the previous model, but it can also select the most appropriate circuit topology and its respective sizing given the target specification. The proposed methodology was implemented and tested on two analog circuit topologies. </p>
<p>本书介绍了使用深度学习 (DL) 和人工神经网络 (ANN) 自动调整模拟集成电路 (IC) 的尺寸和布局。它探索了一种自动调整电路大小的创新方法，其中 ANN 从以前优化的设计解决方案中学习模式。与经典的基于优化的尺寸策略相反，在这些策略中，计算智能技术被用来迭代从设备尺寸到由设计方程或电路模拟提供的电路性能的映射，人工神经网络被证明能够解决模拟 IC 尺寸问题从规格到设备尺寸的直接映射。提出了两种独立的 ANN 架构：仅回归模型和分类回归模型。 Regression-only 模型的目标是从研究的电路中学习设计模式，使用电路的性能作为输入特征和设备的大小作为目标输出。该模型可以根据单个拓扑的规格来确定电路的大小。分类和回归模型具有与先前模型相同的功能，但它还可以根据目标规格选择最合适的电路拓扑及其各自的尺寸。在两种模拟电路拓扑结构上实施并测试了所提出的方法。</p>
<h2 id="Keywords-Index-Terms-18"><a href="#Keywords-Index-Terms-18" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>Analog IC sizing</p>
<p>Artificial Neural Networks</p>
<p>Analog IC Placement</p>
<p>Electronic Design Automation</p>
<p>Applied Deep Learning</p>
<p>Analog IC Design Automation</p>
<h2 id="Comments-41"><a href="#Comments-41" class="headerlink" title="Comments"></a>Comments</h2><p>可惜是Analog的</p>
<h2 id="References-27"><a href="#References-27" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper36-1206"><a href="#Paper36-1206" class="headerlink" title="Paper36-1206"></a>Paper36-1206</h1><h2 id="Published-in-34"><a href="#Published-in-34" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/conhome/8704855/proceeding">2019 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</a></p>
<p>M. Fattori, I. A. Fijn, L. Hu, E. Cantatore, F. Torricelli and M. Charbonneau, “Circuit Design and Design Automation for Printed Electronics,” 2019 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), <strong>2019</strong>, pp. 42-47, doi: 10.23919/DATE.2019.8715095.</p>
<h2 id="Title-35"><a href="#Title-35" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8715095">Circuit Design and Design Automation for Printed Electronics</a></p>
<h2 id="Authors-30"><a href="#Authors-30" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086045474">M. Fattori</a></p>
<p>Department of Electrical Engineering, Technical University of Eindhoven, Eindhoven, The Netherlands</p>
<p>荷兰埃因霍温埃因霍温技术大学电气工程系</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086938348">I.A. Fijn</a></p>
<p>Department of Electrical Engineering, Technical University of Eindhoven, Eindhoven, The Netherlands</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086838085">L. Hu</a></p>
<p>Department of Electrical Engineering, Technical University of Eindhoven, Eindhoven, The Netherlands</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37323486600">E. Cantatore</a></p>
<p>Department of Electrical Engineering, Technical University of Eindhoven, Eindhoven, The Netherlands</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37391733600">F. Torricelli</a></p>
<p>Department of Information Engineering, University of Brescia, Brescia, Italy</p>
<p>意大利布雷西亚布雷西亚大学信息工程系</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085536467">M. Charbonneau</a></p>
<p>CEA-LITEN, Grenoble, France</p>
<p>CEA-LITEN, 格勒诺布尔, 法国</p>
<h2 id="Abstract-40"><a href="#Abstract-40" class="headerlink" title="Abstract"></a>Abstract</h2><p>A Process Design Kit (PDK) for gravure-printed Organic Thin-Film Transistor (OTFT) technology is presented in this paper. The transistor model developed in the PDK enables an accurate prediction of static, dynamic and noise performance of complex organic circuits. The developed Electronic Design Automation (EDA) tools exploit an adaptive strategy to improve the versatility of the PDK in relation to the advancements of the manufacturing process. The design and experimental characterization of a Charge Sensitive Amplifier is used to demonstrate the effectiveness of the PDK. The availability of a versatile and accurate Process Design Kit is expected to enable a reliable design process for complex circuits based on an organic printed technology.</p>
<p>本文介绍了一种用于凹版印刷有机薄膜晶体管 (OTFT) 技术的工艺设计套件 (PDK)。 PDK 中开发的晶体管模型能够准确预测复杂有机电路的静态、动态和噪声性能。 开发的电子设计自动化 (EDA) 工具利用自适应策略来提高 PDK 与制造过程进步相关的多功能性。 电荷敏感放大器的设计和实验特性用于证明 PDK 的有效性。 通用且准确的工艺设计套件的可用性有望为基于有机印刷技术的复杂电路提供可靠的设计工艺。</p>
<h2 id="Keywords-Index-Terms-19"><a href="#Keywords-Index-Terms-19" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><ul>
<li>IEEE Keywords<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Automation&amp;newsearch=true">Automation</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Europe&amp;newsearch=true">Europe</a></li>
</ul>
</li>
<li>INSPEC: Controlled Indexing<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:amplifiers&amp;newsearch=true">amplifiers</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:electronic design automation&amp;newsearch=true">electronic design automation</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:integrated circuit design&amp;newsearch=true">integrated circuit design</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:process design&amp;newsearch=true">process design</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:semiconductor device models&amp;newsearch=true">semiconductor device models</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:thin film transistors&amp;newsearch=true">thin film transistors</a></li>
</ul>
</li>
<li>INSPEC: Non-Controlled Indexing<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:electronic design automation tools&amp;newsearch=true">electronic design automation tools</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:process design kit&amp;newsearch=true">process design kit</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:gravure-printed organic thin-film transistor&amp;newsearch=true">gravure-printed organic thin-film transistor</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:charge sensitive amplifier&amp;newsearch=true">charge sensitive amplifier</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:static noise performance&amp;newsearch=true">static noise performance</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:printed electronics&amp;newsearch=true">printed electronics</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:organic printed technology&amp;newsearch=true">organic printed technology</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:manufacturing process&amp;newsearch=true">manufacturing process</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:PDK&amp;newsearch=true">PDK</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:complex organic circuits&amp;newsearch=true">complex organic circuits</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:dynamic noise performance&amp;newsearch=true">dynamic noise performance</a></li>
</ul>
</li>
<li>Author Keywords<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Process Development Kit .LB.PDK.RB.&amp;newsearch=true">Process Development Kit (PDK)</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:design automation&amp;newsearch=true">design automation</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:printed organic electronics&amp;newsearch=true">printed organic electronics</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:OTFTs&amp;newsearch=true">OTFTs</a></li>
</ul>
</li>
</ul>
<h2 id="Comments-42"><a href="#Comments-42" class="headerlink" title="Comments"></a>Comments</h2><p>Useless</p>
<h2 id="References-28"><a href="#References-28" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper35-1205"><a href="#Paper35-1205" class="headerlink" title="Paper35-1205"></a>Paper35-1205</h1><h2 id="Published-in-35"><a href="#Published-in-35" class="headerlink" title="Published in"></a>Published in</h2><h2 id="Title-36"><a href="#Title-36" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9436651">An Efficient and Flexible Accelerator Design for Sparse Convolutional Neural Networks</a></p>
<h2 id="Authors-31"><a href="#Authors-31" class="headerlink" title="Authors"></a>Authors</h2><h2 id="Abstract-41"><a href="#Abstract-41" class="headerlink" title="Abstract"></a>Abstract</h2><h2 id="Keywords-Index-Terms-20"><a href="#Keywords-Index-Terms-20" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-43"><a href="#Comments-43" class="headerlink" title="Comments"></a>Comments</h2><h2 id="References-29"><a href="#References-29" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper34-1204"><a href="#Paper34-1204" class="headerlink" title="Paper34-1204"></a>Paper34-1204</h1><h2 id="Published-in-36"><a href="#Published-in-36" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8919">IEEE Transactions on Circuits and Systems I: Regular Papers</a> ( Volume: 68, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=9629436">Issue: 12</a>, Dec. 2021)</p>
<p>新鲜出炉</p>
<h2 id="Title-37"><a href="#Title-37" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9516591">The Challenges and Emerging Technologies for Low-Power Artificial Intelligence IoT Systems</a></p>
<h2 id="Authors-32"><a href="#Authors-32" class="headerlink" title="Authors"></a>Authors</h2><p>作者阵容非常豪华。北大黄如院士和叶乐教授团队.</p>
<p>IEEE Member: <strong>Student Member</strong>; <strong>Graduate Student Member</strong>; <strong>Associate Member</strong>; <strong>Member</strong>; <strong>Senior Member</strong>; <strong>Life Senior Member</strong>; <strong>Life Member</strong>; <strong>Life Fellow</strong>; <strong>Honorary</strong>; <strong>Fellow</strong></p>
<p>Le Ye<sup>1</sup>, Member, IEEE, Corresponding author</p>
<p>Zhixuan Wang<sup>1</sup>, Graduate Student Member, IEEE</p>
<p>Ying Liu<sup>1</sup>, Graduate Student Member, IEEE</p>
<p>Peiyu Chen<sup>1</sup>, Graduate Student Member, IEEE</p>
<p>Heyi Li<sup>1</sup>, Graduate Student Member, IEEE</p>
<p>Hao Zhang<sup>1</sup>, Graduate Student Member, IEEE</p>
<p>Meng Wu</p>
<p>Wei He</p>
<p>Linxiao Shen<sup>1</sup>, Member, IEEE</p>
<p>Yihan Zhang<sup>1</sup>, Member, IEEE</p>
<p>Zhichao Tan<sup>2</sup>, Senior Member, IEEE</p>
<p>Yangyuan Wang<sup>1</sup>, Life Fellow, IEEE</p>
<p>Ru Huang<sup>1</sup>, Fellow, IEEE, Corresponding author</p>
<p><sup>1</sup>Key Laboratory of Microelectronics Devices and Circuits (MOE), Institute of Microelectronics, Peking University, Beijing, China</p>
<p>北京大学微电子研究所微电子器件与电路重点实验室（MOE），北京，中国</p>
<p><sup>2</sup>College of Information Science &amp; Electronic Engineering, Zhejiang University, Hangzhou, China</p>
<p>浙江大学信息科学与电子工程学院，杭州，中国</p>
<h2 id="Abstract-42"><a href="#Abstract-42" class="headerlink" title="Abstract"></a>Abstract</h2><p>The Internet of Things (IoT) is an interface with the physical world that usually operates in random-sparse-event (RSE) scenarios. This article discusses main challenges of IoT chips: power consumption, power supply, artificial intelligence (AI), small-signal acquisition, and evaluation criteria. To overcome these challenges, many works recently aimed at IoT system design have emerged. This work reviews the architecture and circuit innovations that have contributed to IoT developments. This paper does not cover security of IoT. Event-driven architectures and nonuniform sampling ADCs significantly reduce the long-term average power. Besides, embedding AI engines in IoT nodes (AIoT) is one critical trend. The computing-in-memory technique improves the energy efficiency of the AI engine. Asynchronous spike neural networks (ASNNs) AI engines show low power potential. In addition to data processing, small-signal acquisition is also critical. The charge-domain analog-front-end (AFE) techniques such as floating inverter-based amplifiers improve energy efficiency. In addition to the above low power and high energy efficiency technologies, energy harvesting can also enhance the lifetime of AIoT devices. This article discusses recent ambient RF and natural energy harvesting approaches and high-efficiency DC-DC with a wide load range. Finally, novel evaluation criteria are introduced to establish benchmark standards for AIoT chips.</p>
<p>物联网 (IoT) 是与物理世界的接口，通常在随机稀疏事件 (RSE) 场景中运行。本文讨论了物联网芯片的主要挑战：功耗、电源、人工智能（AI）、小信号采集和评估标准。为了克服这些挑战，最近出现了许多针对物联网系统设计的工作。这项工作回顾了为物联网发展做出贡献的架构和电路创新。本文不涉及物联网的安全性。事件驱动架构和非均匀采样 ADC 显着降低了长期平均功耗。此外，在物联网节点（AIoT）中嵌入人工智能引擎是一个关键趋势。内存计算技术提高了人工智能引擎的能源效率。异步尖峰神经网络 (ASNN) AI 引擎显示出低功耗潜力。除了数据处理，小信号采集也很关键。电荷域模拟前端 (AFE) 技术，例如基于浮动反相器的放大器，可提高能效。除了上述低功耗和高能效技术外，能量收集还可以提高 AIoT 设备的寿命。本文讨论了最近的环境射频和自然能量收集方法以及具有宽负载范围的高效 DC-DC。最后，引入新的评估标准来建立 AIoT 芯片的基准标准。</p>
<h2 id="Keywords-Index-Terms-21"><a href="#Keywords-Index-Terms-21" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><ul>
<li>IEEE Keywords<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Batteries&amp;newsearch=true">Batteries</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Artificial intelligence&amp;newsearch=true">Artificial intelligence</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Power demand&amp;newsearch=true">Power demand</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Internet of Things&amp;newsearch=true">Internet of Things</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Energy harvesting&amp;newsearch=true">Energy harvesting</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Sensors&amp;newsearch=true">Sensors</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Servers&amp;newsearch=true">Servers</a></li>
</ul>
</li>
<li>Author Keywords<ul>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Internet of Things&amp;newsearch=true">Internet of Things</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:random sparse events&amp;newsearch=true">random sparse events</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:light-duty artificial intelligence&amp;newsearch=true">light-duty artificial intelligence</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:event-driven&amp;newsearch=true">event-driven</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:level-crossing ADC&amp;newsearch=true">level-crossing ADC</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:asynchronous&amp;newsearch=true">asynchronous</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:computing in memory&amp;newsearch=true">computing in memory</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:spike neural network&amp;newsearch=true">spike neural network</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:charge-domain&amp;newsearch=true">charge-domain</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:power management&amp;newsearch=true">power management</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:energy harvesting&amp;newsearch=true">energy harvesting</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:evaluation criteria&amp;newsearch=true">evaluation criteria</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:ultralow power&amp;newsearch=true">ultralow power</a>,</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:energy-efficient&amp;newsearch=true">energy-efficient</a></li>
</ul>
</li>
</ul>
<h2 id="Comments-44"><a href="#Comments-44" class="headerlink" title="Comments"></a>Comments</h2><p>这是北大黄如院士和叶乐教授团队12月TCAS-I上的highlight文章，写的很不错，可以作为我们多个课题方向的背景支撑材料。</p>
<p>建议精读。</p>
<p>系列报告：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/wDZqMxg8ecAFfp8BgfeoMg">【云上博论】全国微电子研究生学术论坛暨第655期清华大学博士生学术论坛</a></p>
<h2 id="References-30"><a href="#References-30" class="headerlink" title="References"></a>References</h2><p>论文一共引用了124篇文献，最后一篇是拉扎维的。</p>
<p>参考了哪些文献？</p>
<p>大量的ISSCC</p>
<hr>
<h1 id="Paper11-1202"><a href="#Paper11-1202" class="headerlink" title="Paper11-1202"></a>Paper11-1202</h1><h2 id="Published-in-37"><a href="#Published-in-37" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Tao Yang, Yadong Wei, Zhijun Tu, Haolun Zeng, Michel A. Kinsy, Nanning Zheng, Pengju Ren, “Design Space Exploration of Neural Network Activation Function Circuits”, <em>Computer-Aided Design of Integrated Circuits and Systems IEEE Transactions on</em>, vol. 38, no. 10, pp. 1974-1978, 2019.</p>
<h2 id="Title-38"><a href="#Title-38" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8467987/">Design Space Exploration of Neural Network Activation Function Circuits</a></p>
<h2 id="Authors-33"><a href="#Authors-33" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37087012027">Tao Yang</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086610958">Yadong Wei</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37087010777">Zhijun Tu</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37087010928">Haolun Zeng</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37318978700">Michel A. Kinsy</a></p>
<p>Department of Electrical and Computer Engineering, Boston University, Boston, MA, USA</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37271536700">Nanning Zheng</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37069213800">Pengju Ren</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<h2 id="Abstract-43"><a href="#Abstract-43" class="headerlink" title="Abstract"></a>Abstract</h2><p>The widespread application of artificial neural networks has prompted researchers to experiment with field-programmable gate array and customized ASIC designs to speed up their computation. These implementation efforts have generally focused on weight multiplication and signal summation operations, and less on activation functions used in these applications. Yet, efficient hardware implementations of nonlinear activation functions like exponential linear units (ELU), scaled ELU (SELU), and hyperbolic tangent (tanh), are central to designing effective neural network accelerators, since these functions require lots of resources. In this paper, we explore efficient hardware implementations of activation functions using purely combinational circuits, with a focus on two widely used nonlinear activation functions, i.e., SELU and tanh. Our experiments demonstrate that neural networks are generally insensitive to the precision of the activation function. The results also prove that the proposed combinational circuit-based approach is very efficient in terms of speed and area, with negligible accuracy loss on the MNIST, CIFAR-10, and IMAGE NET benchmarks. Synopsys design compiler synthesis results show that circuit designs for tanh and SELU can save between ×3.13∼×7.69 and ×4.45∼×8.45 area compared to the look-up table/memory-based implementations, and can operate at 5.14 GHz and 4.52 GHz using the 28-nm SVT library, respectively. The implementation is available at: <a target="_blank" rel="noopener" href="https://github.com/ThomasMrY/ActivationFunctionDemo">https://github.com/ThomasMrY/ActivationFunctionDemo</a>.</p>
<p>人工神经网络的广泛应用促使研究人员尝试现场可编程门阵列和定制 ASIC 设计，以加快计算速度。这些实现工作通常集中在权重乘法和信号求和操作上，较少关注这些应用程序中使用的激活函数。然而，非线性激活函数的高效硬件实现，如指数线性单元 (ELU)、缩放 ELU (SELU) 和双曲正切 (tanh)，对于设计有效的神经网络加速器至关重要，因为这些函数需要大量资源。在本文中，我们探索使用纯组合电路的激活函数的有效硬件实现，重点是两个广泛使用的非线性激活函数，即 SELU 和 tanh。我们的实验表明，神经网络通常对激活函数的精度不敏感。结果还证明，所提出的基于组合电路的方法在速度和面积方面非常有效，在 MNIST、CIFAR-10 和 IMAGE NET 基准上的精度损失可以忽略不计。 Synopsys 设计编译器综合结果表明，与基于查找表/内存的实现相比，tanh 和 SELU 的电路设计可以节省 ×3.13∼×7.69 和 ×4.45∼×8.45 之间的面积，并且可以在 5.14 GHz 和 4.52 GHz 下运行分别使用 28-nm SVT 库。该实现可从以下网址获得：<a target="_blank" rel="noopener" href="https://github.com/ThomasMrY/ActivationFunctionDemo。">https://github.com/ThomasMrY/ActivationFunctionDemo。</a></p>
<h2 id="Keywords-Index-Terms-22"><a href="#Keywords-Index-Terms-22" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-45"><a href="#Comments-45" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-31"><a href="#References-31" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper10-1201"><a href="#Paper10-1201" class="headerlink" title="Paper10-1201"></a>Paper10-1201</h1><h2 id="Published-in-38"><a href="#Published-in-38" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Leandro D. Medus, Taras Iakymchuk, Jose Vicente Frances-Villora, Manuel Bataller-Mompeán, Alfredo Rosado-Muñoz, “A Novel Systolic Parallel Hardware Architecture for the FPGA Acceleration of Feedforward Neural Networks”, <em>Access IEEE</em>, vol. 7, pp. 76084-76103, 2019.</p>
<h2 id="Title-39"><a href="#Title-39" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8731886">A Novel Systolic Parallel Hardware Architecture for the FPGA Acceleration of Feedforward Neural Networks</a></p>
<h2 id="Authors-34"><a href="#Authors-34" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086863367">Leandro D. Medus</a></p>
<p>Group for Processing and Digital Design, Universitat de Valencia, Burjassot, Spain</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/38228373600">Taras Iakymchuk</a></p>
<p>Group for Processing and Digital Design, Universitat de Valencia, Burjassot, Spain</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085356350">Jose Vicente Frances-Villora</a></p>
<p>Group for Processing and Digital Design, Universitat de Valencia, Burjassot, Spain</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/38256528200">Manuel Bataller-Mompeán</a></p>
<p>Group for Processing and Digital Design, Universitat de Valencia, Burjassot, Spain</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085364609">Alfredo Rosado-Muñoz</a></p>
<p>Group for Processing and Digital Design, Universitat de Valencia, Burjassot, Spain</p>
<h2 id="Abstract-44"><a href="#Abstract-44" class="headerlink" title="Abstract"></a>Abstract</h2><p>New chips for machine learning applications appear, they are tuned for a specific topology, being efficient by using highly parallel designs at the cost of high power or large complex devices. However, the computational demands of deep neural networks require flexible and efficient hardware architectures able to fit different applications, neural network types, number of inputs, outputs, layers, and units in each layer, making the migration from software to hardware easy. This paper describes novel hardware implementing any feedforward neural network (FFNN): multilayer perceptron, autoencoder, and logistic regression. The architecture admits an arbitrary input and output number, units in layers, and a number of layers. The hardware combines matrix algebra concepts with serial-parallel computation. It is based on a systolic ring of neural processing elements (NPE), only requiring as many NPEs as neuron units in the largest layer, no matter the number of layers. The use of resources grows linearly with the number of NPEs. This versatile architecture serves as an accelerator in real-time applications and its size does not affect the system clock frequency. Unlike most approaches, a single activation function block (AFB) for the whole FFNN is required. Performance, resource usage, and accuracy for several network topologies and activation functions are evaluated. The architecture reaches 550 MHz clock speed in a Virtex7 FPGA. The proposed implementation uses 18-bit fixed point achieving similar classification performance to a floating point approach. A reduced weight bit size does not affect the accuracy, allowing more weights in the same memory. Different FFNN for Iris and MNIST datasets were evaluated and, for a real-time application of abnormal cardiac detection, a ${\hspace{-0.112em}\times \hspace{-0.112em}}256$ acceleration was achieved. The proposed architecture can perform up to 1980 Giga operations per second (GOPS), implementing the multilayer FFNN of up to 3600 neurons per layer in a single chip. The architecture can be extended to bigger capacity devices or multi-chip by the simple NPE ring extension.</p>
<p>出现了用于机器学习应用的新芯片，它们针对特定拓扑进行了调整，通过以高功率或大型复杂设备为代价使用高度并行的设计来提高效率。然而，深度神经网络的计算需求需要灵活高效的硬件架构，能够适应不同的应用、神经网络类型、输入、输出、层数和每层单元的数量，从而使从软件到硬件的迁移变得容易。本文描述了实现任何前馈神经网络 (FFNN) 的新型硬件：多层感知器、自动编码器和逻辑回归。该体系结构允许任意输入和输出数量、层中的单元和层数。硬件将矩阵代数概念与串行-并行计算相结合。它基于神经处理元件 (NPE) 的收缩环，只需要与最大层中的神经元单元一样多的 NPE，无论层数如何。资源的使用随着 NPE 的数量线性增长。这种通用架构可用作实时应用程序中的加速器，其大小不会影响系统时钟频率。与大多数方法不同，整个 FFNN 需要单个激活功能块 (AFB)。评估了几种网络拓扑和激活函数的性能、资源使用和准确性。该架构在 Virtex7 FPGA 中达到 550 MHz 时钟速度。建议的实现使用 18 位定点实现与浮点方法相似的分类性能。减小的权重位大小不会影响准确性，从而在同一内存中允许更多的权重。评估了 Iris 和 MNIST 数据集的不同 FFNN，对于异常心脏检测的实时应用，实现了 ${\hspace{-0.112em}\times \hspace{-0.112em}}256$ 的加速度。所提出的架构可以执行高达每秒 1980 次 Giga 操作 (GOPS)，在单个芯片中实现每层高达 3600 个神经元的多层 FFNN。通过简单的 NPE 环扩展，该架构可以扩展到更大容量的设备或多芯片。</p>
<h2 id="Keywords-Index-Terms-23"><a href="#Keywords-Index-Terms-23" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-46"><a href="#Comments-46" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-32"><a href="#References-32" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper9-1130"><a href="#Paper9-1130" class="headerlink" title="Paper9-1130"></a>Paper9-1130</h1><h2 id="Published-in-39"><a href="#Published-in-39" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Renato J. Cintra, Stefan Duffner, Christophe Garcia, André Leite, “Low-Complexity Approximate Convolutional Neural Networks”, <em>Neural Networks and Learning Systems IEEE Transactions on</em>, vol. 29, no. 12, pp. 5981-5992, 2018.</p>
<h2 id="Title-40"><a href="#Title-40" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8334697">Low-Complexity Approximate Convolutional Neural Networks</a></p>
<h2 id="Authors-35"><a href="#Authors-35" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37297800100">Renato J. Cintra</a></p>
<p>Signal Processing Group, Universidade Federal de Pernambuco, Recife, Brazil</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37565697300">Stefan Duffner</a></p>
<p>Université de Lyon, CNRS, INSA-Lyon, LIRIS, UMR5205, Villeurbanne, France</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37269224300">Christophe Garcia</a></p>
<p>Université de Lyon, CNRS, INSA-Lyon, LIRIS, UMR5205, Villeurbanne, France</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086523490">André Leite</a></p>
<p>Signal Processing Group, Universidade Federal de Pernambuco, Recife, Brazil</p>
<h2 id="Abstract-45"><a href="#Abstract-45" class="headerlink" title="Abstract"></a>Abstract</h2><p>In this paper, we present an approach for minimizing the computational complexity of the trained convolutional neural networks (ConvNets). The idea is to approximate all elements of a given ConvNet and replace the original convolutional filters and parameters (pooling and bias coefficients; and activation function) with an efficient approximations capable of extreme reductions in computational complexity. Low-complexity convolution filters are obtained through a binary (zero and one) linear programming scheme based on the Frobenius norm over sets of dyadic rationals. The resulting matrices allow for multiplication-free computations requiring only addition and bit-shifting operations. Such low-complexity structures pave the way for low power, efficient hardware designs. We applied our approach on three use cases of different complexities: 1) a “light” but efficient ConvNet for face detection (with around 1000 parameters); 2) another one for hand-written digit classification (with more than 180000 parameters); and 3) a significantly larger ConvNet: AlexNet with ≈1.2 million matrices. We evaluated the overall performance on the respective tasks for different levels of approximations. In all considered applications, very low-complexity approximations have been derived maintaining an almost equal classification performance.</p>
<p>在本文中，我们提出了一种最小化训练卷积神经网络 (ConvNets) 计算复杂度的方法。这个想法是近似给定 ConvNet 的所有元素，并用能够极大降低计算复杂性的有效近似值替换原始卷积滤波器和参数（池化和偏置系数；以及激活函数）。低复杂度卷积滤波器是通过基于二元有理数集的 Frobenius 范数的二元（零和一）线性规划方案获得的。结果矩阵允许只需要加法和位移操作的无乘法计算。这种低复杂度的结构为低功耗、高效的硬件设计铺平了道路。我们将我们的方法应用于三个不同复杂性的用例：1）用于人脸检测的“轻量级”但高效的 ConvNet（具有大约 1000 个参数）； 2）另一种手写数字分类（180000多个参数）；和 3) 一个明显更大的 ConvNet：AlexNet，具有约 120 万个矩阵。我们针对不同的近似水平评估了各个任务的整体性能。在所有考虑的应用程序中，已经导出了非常低复杂度的近似值，并保持了几乎相同的分类性能。</p>
<h2 id="Keywords-Index-Terms-24"><a href="#Keywords-Index-Terms-24" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-47"><a href="#Comments-47" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-33"><a href="#References-33" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper15-1128"><a href="#Paper15-1128" class="headerlink" title="Paper15-1128"></a>Paper15-1128</h1><h2 id="Published-in-40"><a href="#Published-in-40" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Wen-Chang Yang, Shu-Yun Lin, Tsung-Chu Huang, “Range-Lookup Approximate Computing Acceleration for Any Activation Functions in Low-Power Neural Network”, <em>Consumer Electronics - Taiwan (ICCE-Taiwan) 2020 IEEE International Conference on</em>, pp. 1-2, 2020.</p>
<h2 id="Title-41"><a href="#Title-41" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9258057">Range-Lookup Approximate Computing Acceleration for Any Activation Functions in Low-Power Neural Network</a></p>
<h2 id="Authors-36"><a href="#Authors-36" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088560962">Wen-Chang Yang</a></p>
<p>National Changhua University of Education, Changhua, Taiwan</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088564479">Shu-Yun Lin</a></p>
<p>National Changhua University of Education, Changhua, Taiwan</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/38183099400">Tsung-Chu Huang</a></p>
<p>National Changhua University of Education, Changhua, Taiwan</p>
<h2 id="Abstract-46"><a href="#Abstract-46" class="headerlink" title="Abstract"></a>Abstract</h2><p>Consumer electronics have become versatile for processing a lot of signals in any distribution. This results in that high-speed activating of neural network should fit for any distribution of error functions. In this paper, we propose a set of transistor-level magnitude-comparators. Then we apply them to develop range-addressable memory to design a lightweight-slope lookup table. We then develop an efficient algorithm for constructing a lightweight-slope piecewise line. The proposed techniques are suitable to design any sigmoidal activation functions in neural network. From experiments and comparisons, the proposed LUT can be more efficient effective than previous looking tables. Especially applied in the proposed range-addressable memory, the power-delay product can be reduced by more than 30 folds.</p>
<p>消费电子产品已成为处理任何分布中的大量信号的通用工具。 这导致神经网络的高速激活应该适合误差函数的任何分布。 在本文中，我们提出了一组晶体管级幅度比较器。 然后我们应用它们来开发范围可寻址内存来设计一个轻量级斜率查找表。 然后，我们开发了一种有效的算法来构建轻量级斜率分段线。 所提出的技术适用于设计神经网络中的任何 sigmoidal 激活函数。 从实验和比较来看，提议的 LUT 可以比以前的查找表更有效。 特别是应用在提出的范围可寻址存储器中，功率延迟积可以减少30倍以上。</p>
<h2 id="Keywords-Index-Terms-25"><a href="#Keywords-Index-Terms-25" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-48"><a href="#Comments-48" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-34"><a href="#References-34" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper14-1127"><a href="#Paper14-1127" class="headerlink" title="Paper14-1127"></a>Paper14-1127</h1><h2 id="Published-in-41"><a href="#Published-in-41" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Hui Chen, Lin Jiang, Yuanyong Luo, Zhonghai Lu, Yuxiang Fu, Li Li, Zongguang Yu, “A CORDIC-Based Architecture with Adjustable Precision and Flexible Scalability to Implement Sigmoid and Tanh Functions”, <em>Circuits and Systems (ISCAS) 2020 IEEE International Symposium on</em>, pp. 1-5, 2020.</p>
<h2 id="Title-42"><a href="#Title-42" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9180864">A CORDIC-Based Architecture with Adjustable Precision and Flexible Scalability to Implement Sigmoid and Tanh Functions</a></p>
<h2 id="Authors-37"><a href="#Authors-37" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088540313">Hui Chen</a></p>
<p>Nanjing University, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088931162">Lin Jiang</a></p>
<p>Nanjing University, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086493094">Yuanyong Luo</a></p>
<p>Nanjing University, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088687446">Zhonghai Lu</a></p>
<p>KTH Royal Institute of Technology, Stockholm, Sweden</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085833583">Yuxiang Fu</a></p>
<p>Nanjing University, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085682466">Li Li</a></p>
<p>Nanjing University, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088687271">Zongguang Yu</a></p>
<p>Nanjing University, Nanjing, China</p>
<h2 id="Abstract-47"><a href="#Abstract-47" class="headerlink" title="Abstract"></a>Abstract</h2><p>In the artificial neural networks, tanh (hyperbolic tangent) and sigmoid functions are widely used as activation functions. Past methods to compute them may have shortcomings such as low precision or inflexible architecture that is difficult to expand, so we propose a CORDIC-based architecture to implement sigmoid and tanh functions, which has adjustable precision and flexible scalability. It just needs shift-add-or-subtract operations to compute high-accuracy results and is easy to expand the input range through scaling the negative iterations of CORDIC without changing the original architecture. We adopt the control variable method to explore the accuracy distribution through software simulation. A specific case (ARCH. (1, 15, 18), RMSE: 10 -6 ) is designed and synthesized under the TSMC 40nm CMOS technology, the report shows that it has the area of 36512.78μm 2 and power of 12.35mW at the frequency of 1GHz. The maximum work frequency can reach 1.5GHz, which is better than the state-of-the-art methods.</p>
<p>在人工神经网络中，tanh（双曲正切）和 sigmoid 函数被广泛用作激活函数。过去的计算方法可能存在精度低或架构不灵活、难以扩展等缺点，因此我们提出了一种基于 CORDIC 的架构来实现 sigmoid 和 tanh 函数，该架构具有可调节的精度和灵活的可扩展性。它只需要移位加减运算即可计算出高精度结果，并且在不改变原始架构的情况下，可以通过缩放 CORDIC 的负迭代轻松扩展输入范围。我们采用控制变量法，通过软件仿真探索精度分布。一个具体案例（ARCH. (1, 15, 18), RMSE: 10 -6 ）是在台积电40nm CMOS工艺下设计合成的，报告显示其面积为36512.78μm 2 ，功率为12.35mW频率为 1GHz。最高工作频率可达1.5GHz，优于最先进的方法。</p>
<h2 id="Keywords-Index-Terms-26"><a href="#Keywords-Index-Terms-26" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-49"><a href="#Comments-49" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-35"><a href="#References-35" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper13-1126"><a href="#Paper13-1126" class="headerlink" title="Paper13-1126"></a>Paper13-1126</h1><h2 id="Published-in-42"><a href="#Published-in-42" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Yue Gao, Weiqiang Liu, Fabrizio Lombardi, “Design and Implementation of an Approximate Softmax Layer for Deep Neural Networks”, <em>Circuits and Systems (ISCAS) 2020 IEEE International Symposium on</em>, pp. 1-5, 2020.</p>
<h2 id="Title-43"><a href="#Title-43" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9180870">Design and Implementation of an Approximate Softmax Layer for Deep Neural Networks</a></p>
<h2 id="Authors-38"><a href="#Authors-38" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088932960">Yue Gao</a></p>
<p>Nanjing University of Aeronautics and Astronautics, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085338613">Weiqiang Liu</a></p>
<p>Nanjing University of Aeronautics and Astronautics, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37279999000">Fabrizio Lombardi</a></p>
<p>Northeastern University, Boston, MA, USA</p>
<h2 id="Abstract-48"><a href="#Abstract-48" class="headerlink" title="Abstract"></a>Abstract</h2><p>Deep neural networks (DNNs) have been widely used in classification due to their high accuracy. The softmax function is one of the important non-linear functions in DNNs. Therefore, high performance and efficient hardware design are sought. However, the improvement of the softmax function is difficult because the exponent and the division units are complex. In this paper, we propose new approximate hardware architectures for both the exponent and the division units. Compared with the state-of-the-art designs, the proposed approximate softmax design consumes significantly less resources and also achieves high performance while maintaining a very high accuracy.</p>
<p>深度神经网络（DNN）由于其高精度而被广泛用于分类。 softmax 函数是 DNN 中重要的非线性函数之一。 因此，寻求高性能和高效的硬件设计。 但是softmax函数的改进难度很大，因为指数和除法单位比较复杂。 在本文中，我们为指数和除法单元提出了新的近似硬件架构。 与最先进的设计相比，所提出的近似 softmax 设计消耗的资源明显更少，并且在保持非常高的精度的同时还实现了高性能。</p>
<h2 id="Keywords-Index-Terms-27"><a href="#Keywords-Index-Terms-27" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-50"><a href="#Comments-50" class="headerlink" title="Comments"></a>Comments</h2><p>可以作为对比。同时好几篇文章都在提softmax，有必要考虑一下softmax性能。</p>
<h2 id="References-36"><a href="#References-36" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper30-1124"><a href="#Paper30-1124" class="headerlink" title="Paper30-1124"></a>Paper30-1124</h1><h2 id="Published-in-43"><a href="#Published-in-43" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/conhome/8023017/proceeding">2017 46th International Conference on Parallel Processing (ICPP)</a></p>
<p>中国计算机学会和清华推荐的<strong>B类</strong>会议</p>
<p>Y. Nagasaka, A. Nukada and S. Matsuoka, “High-Performance and Memory-Saving Sparse General Matrix-Matrix Multiplication for NVIDIA Pascal GPU,” 2017 46th International Conference on Parallel Processing (ICPP), 2017, pp. 101-110, doi: 10.1109/ICPP.2017.19.</p>
<h2 id="Title-44"><a href="#Title-44" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8025284">High-Performance and Memory-Saving Sparse General Matrix-Matrix Multiplication for NVIDIA Pascal GPU</a></p>
<h2 id="Authors-39"><a href="#Authors-39" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085596361">Yusuke Nagasaka</a></p>
<p>Tokyo Institute of Technology, Tokyo, Japan</p>
<p>东京工业大学，东京，日本</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37392857200">Akira Nukada</a></p>
<p>Tokyo Institute of Technology, Tokyo, Japan</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085374497">Satoshi Matsuoka</a></p>
<p>Tokyo Institute of Technology, Tokyo, Japan</p>
<h2 id="Abstract-49"><a href="#Abstract-49" class="headerlink" title="Abstract"></a>Abstract</h2><p>Sparse general matrix-matrix multiplication (SpGEMM) is one of the key kernels of preconditioners such as algebraic multigrid method or graph algorithms. However, the performance of SpGEMM is quite low on modern processors due to random memory access to both input and output matrices. As well as the number and the pattern of non-zero elements in the output matrix, important for achieving locality, are unknown before the execution. Moreover, the state-of-the-art GPU implementations of SpGEMM requires large amounts of memory for temporary results, limiting the matrix size computable on fast GPU device memory. We propose a new fast SpGEMM algorithm requiring small amount of memory and achieving high performance. Calculation of the pattern and value in output matrix is optimized by using GPU’s on-chip shared memory and a hash table. Additionally, our algorithm launches multiple kernels running concurrently to improve the utilization of GPU resources. The kernels for the calculation of each row of output matrix are chosen based on the number of non-zero elements. Performance evaluation using matrices from the Sparse Matrix Collection of University Florida on NVIDIA’s Pascal generation GPU shows that our approach achieves speedups of up to x4.3 in single precision and x4.4 in double precision compared to existing SpGEMM libraries. Furthermore, the memory usage is reduced by 14.7% in single precision and 10.9% in double precision on average, allowing larger matrices to be computed.</p>
<p>通用稀疏矩阵矩阵乘法 (Sparse general matrix-matrix multiplication, SpGEMM) 是代数多重网格算法（algebraic multigrid method, AMG算法）或图算法等预处理器(preconditioners)的关键内核之一。然而，由于对输入和输出矩阵的随机内存访问，SpGEMM 在现代处理器上的性能非常低。对于实现局部性很重要的输出矩阵中非零元素的数量和分布(pattern)在执行之前是未知的。此外，SpGEMM 最先进的 GPU 实现需要大量显存？内存？用于临时结果，限制了可在 GPU 设备显存上计算的矩阵大小。我们提出了一种新的快速 SpGEMM 算法，只需要较少的内存即可实现高性能。我们通过使用 GPU 的片上共享内存和哈希表对输出矩阵的pattern和值的计算进行了优化。此外，我们的算法会启动多个并发运行的kernel，以提高 GPU 资源的利用率。计算每一行输出矩阵的内核是根据非零元素的数量选择的。在 NVIDIA 的 Pascal GPU 上使用佛罗里达大学稀疏矩阵集合(Sparse Matrix Collection of University Florida)中的矩阵进行的性能评估表明，与现有的 SpGEMM 库相比，<strong>我们的方法在单精度和双精度下实现了高达 x4.3 的加速</strong>。此外，单精度内存使用量平均减少 14.7%，双精度内存使用量平均减少 10.9%，同时允许计算更大的矩阵。</p>
<h2 id="Keywords-Index-Terms-28"><a href="#Keywords-Index-Terms-28" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><ul>
<li><p>IEEE Keywords</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Sparse matrices&amp;newsearch=true">Sparse matrices</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Graphics processing units&amp;newsearch=true">Graphics processing units</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Memory management&amp;newsearch=true">Memory management</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Acceleration&amp;newsearch=true">Acceleration</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Instruction sets&amp;newsearch=true">Instruction sets</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Kernel&amp;newsearch=true">Kernel</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Parallel processing&amp;newsearch=true">Parallel processing</a></p>
<p>稀疏矩阵，GPU，内存管理，加速，指令集，内核，并行处理</p>
</li>
<li><p>INSPEC: Controlled Indexing</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:graphics processing units&amp;newsearch=true">graphics processing units</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:matrix multiplication&amp;newsearch=true">matrix multiplication</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:parallel processing&amp;newsearch=true">parallel processing</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:shared memory systems&amp;newsearch=true">shared memory systems</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:software performance evaluation&amp;newsearch=true">software performance evaluation</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:storage management&amp;newsearch=true">storage management</a></p>
<p>GPU，矩阵乘法，并行处理，shared memory system，软件性能评估，存储管理</p>
</li>
<li><p>INSPEC: Non-Controlled Indexing</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:sparse general matrix-matrix multiplication&amp;newsearch=true">sparse general matrix-matrix multiplication</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:SpGEMM&amp;newsearch=true">SpGEMM</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:NVIDIA Pascal GPU&amp;newsearch=true">NVIDIA Pascal GPU</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:graphics processing unit&amp;newsearch=true">graphics processing unit</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:high performance computing&amp;newsearch=true">high performance computing</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:on-chip shared memory&amp;newsearch=true">on-chip shared memory</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:hash table&amp;newsearch=true">hash table</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:performance evaluation&amp;newsearch=true">performance evaluation</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:memory usage reduction&amp;newsearch=true">memory usage reduction</a></p>
<p>SpGEMM, NV Pascal GPU, GPU，高性能计算，片上共享内存，哈希表，性能评估，减少内存用量</p>
</li>
<li><p>Author Keywords</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Sparse matrix&amp;newsearch=true">Sparse matrix</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:SpGEMM&amp;newsearch=true">SpGEMM</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:GPU&amp;newsearch=true">GPU</a></p>
<p>稀疏矩阵，SpGEMM, GPU</p>
</li>
</ul>
<h2 id="Comments-51"><a href="#Comments-51" class="headerlink" title="Comments"></a>Comments</h2><p>本工作是Performance optimization, modeling and analysis of sparse matrix-matrix products on multi-core and many-core processors的前置。paper30的一作是paper5的一作。这篇文章是基于GPU的，paper5是基于CPU的。</p>
<p>提到的佛罗里达大学稀疏矩阵集合(Sparse Matrix Collection of University Florida)可以下载下来看看。</p>
<h2 id="References-37"><a href="#References-37" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://www.nvidia.cn/data-center/pascal-gpu-architecture/">NVIDIA Pascal GPU Architecture</a></p>
<p>产品包括：</p>
<p><strong>GeForce系列游戏显卡</strong></p>
<p><strong>GTX1050、1050Ti、1060(3G, 5G, 6G)、1070、1070Ti、1080、1080Ti</strong>等</p>
<p><strong>QUADRO系列专业显卡</strong></p>
<p>GP100、P6000、P5000、P4000、P2000、P1000、P600、P400等 [13] </p>
<p><strong>Tesla系列加速计算卡</strong></p>
<p><strong>P100</strong>、P4、P40</p>
<p><strong>NVS系列多显示器商用显卡</strong></p>
<p>暂无Pascal产品</p>
<p><strong>TITAN显卡</strong></p>
<p>TITAN Xp</p>
<hr>
<h1 id="Paper29-1123"><a href="#Paper29-1123" class="headerlink" title="Paper29-1123"></a>Paper29-1123</h1><h2 id="Published-in-44"><a href="#Published-in-44" class="headerlink" title="Published in"></a>Published in</h2><p>SIAM Journal on Scientific Computing</p>
<p>期刊一般。不要考虑。</p>
<h2 id="Title-45"><a href="#Title-45" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://epubs.siam.org/doi/10.1137/15M104253X">Exploiting Multiple Levels of Parallelism in Sparse Matrix-Matrix Multiplication</a></p>
<h2 id="Authors-40"><a href="#Authors-40" class="headerlink" title="Authors"></a>Authors</h2><p><strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Azad%2C+Ariful">Ariful Azad</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Ballard%2C+Grey">Grey Ballard</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Buluç%2C+Aydin">Aydin Buluç</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Demmel%2C+James">James Demmel</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Grigori%2C+Laura">Laura Grigori</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Schwartz%2C+Oded">Oded Schwartz</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Toledo%2C+Sivan">Sivan Toledo</a></strong>, and <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Williams%2C+Samuel">Samuel Williams</a></strong></p>
<p>ARIFUL AZAD†, GREY BALLARD‡, AYDIN BULUC¸†, JAMES DEMMEL§, LAURA GRIGORI¶, ODED SCHWARTZk, SIVAN TOLEDO#, AND SAMUEL WILLIAMS†</p>
<p>† CRD, Lawrence Berkeley National Laboratory, Berkeley, CA 94720 (azad@lbl.gov, abuluc@lbl.gov, swwilliams@lbl.gov).劳伦斯伯克利国家实验室，伯克利</p>
<p>‡ Computer    Science    Department,    Wake    Forest    University,    Winston    Salem,    NC    94551 (ballard@wfu.edu).计算机科学系，维克森林大学，温斯顿塞勒姆，北卡罗来纳州</p>
<p>§ EECS, University of California, Berkeley, Berkeley, CA 94720 (demmel@eecs.berkeley.edu).EECS，UCB加州大学伯克利分校，伯克利</p>
<p>¶ INRIA Paris-Rocquencourt, Alpines, Paris, 75005, France (laura.grigori@inria.fr). kThe Hebrew University, Rothberg A405, Jerusalem, Israel (odedsc@cs.huji.ac.il).希伯来大学，以色列耶路撒冷</p>
<p># Blavatnik School of Computer Science, Tel Aviv University, Ramot Aviv, Tel-Aviv 69978, Israel (stoledo@tau.ac.il).Blavatnik 计算机科学学院，特拉维夫大学，Ramot Aviv，特拉维夫 69978，以色列 (stoledo@tau.ac.il)。</p>
<p>† ‡ § ¶ # </p>
<h2 id="Abstract-50"><a href="#Abstract-50" class="headerlink" title="Abstract"></a>Abstract</h2><p>Sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many high-performance graph algorithms as well as for some linear solvers, such as algebraic multigrid. The <strong>scaling</strong> of existing parallel implementations of SpGEMM is heavily bound by communication. Even though 3D (or 2.5D) algorithms have been proposed and theoretically analyzed in the flat MPI model on Erdös—Rényi matrices, those algorithms had not been implemented in practice and their complexities had not been analyzed for the general case. In this work, <strong>we present the first implementation of the 3D SpGEMM formulation</strong> that exploits multiple (intranode and internode) levels of parallelism, achieving significant speedups over the state-of-the-art publicly available codes at <strong>all levels of concurrencies</strong>. We extensively evaluate our implementation and identify bottlenecks that should be subject to further research.</p>
<p>稀疏矩阵乘法（或 SpGEMM）是许多高性能图算法以及一些linear solvers（例如代数多重网格算法（algebraic multigrid method, AMG算法））的关键原语（key primitive）。 SpGEMM 现有并行实现的scaling(扩展？）在很大程度上受到通信的约束。 尽管已经在 <strong>Erdös—Rényi 矩阵的平面 MPI 模型</strong>中提出了 3D（或 2.5D）算法并对其进行了理论分析，但这些算法尚未在实践中实现，并且尚未针对一般情况分析其复杂性。 在这项工作中，<strong>我们展示了 3D SpGEMM 公式的第一个实现</strong>，它利用了多个（节点内和节点间）并行性，在所有并发级别上实现了对最先进的公开可用代码的显着加速。 我们广泛评估了我们的实施并确定了下一步研究的瓶颈。</p>
<h2 id="Keywords-Index-Terms-29"><a href="#Keywords-Index-Terms-29" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>parallel computing, numerical linear algebra, sparse matrix-matrix multiplication, 2.5D algorithms, 3D algorithms, multithreading, SpGEMM, 2D decomposition, graph algorithms</p>
<p>并行计算、数值线性代数、SpGEMM、2.5D 算法、3D 算法、多线程、SpGEMM、2D 分解、图算法</p>
<h2 id="Comments-52"><a href="#Comments-52" class="headerlink" title="Comments"></a>Comments</h2><p>本工作是Performance optimization, modeling and analysis of sparse matrix-matrix products on multi-core and many-core processors的前置。paper29的一作是paper5的三作。文章期刊似乎不是很有名，<strong>IEEE上也查不到</strong>。</p>
<h2 id="References-38"><a href="#References-38" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper28-1113"><a href="#Paper28-1113" class="headerlink" title="Paper28-1113"></a>Paper28-1113</h1><h2 id="Published-in-45"><a href="#Published-in-45" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/conhome/9567713/proceeding">ESSCIRC 2021 - IEEE 47th European Solid State Circuits Conference (ESSCIRC)</a></p>
<p>T. Iizuka, H. Xu and A. A. Abidi, “A Tutorial on Systematic Design of CMOS A/D Converters: Illustrated by a 10 b, 500 MS/s SAR ADC with 2 GHz RBW,” ESSCIRC 2021 - IEEE 47th European Solid State Circuits Conference (ESSCIRC), 2021, pp. 381-386, doi: 10.1109/ESSCIRC53450.2021.9567842.</p>
<h2 id="Title-46"><a href="#Title-46" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9567842/">A Tutorial on Systematic Design of CMOS A/D Converters: Illustrated by a 10 b, 500 MS/s SAR ADC with 2 GHz RBW</a></p>
<h2 id="Authors-41"><a href="#Authors-41" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088570233">Tetsuya Iizuka</a></p>
<p>Systems Design Lab., School of Engineering, The University of Tokyo, Tokyo, Japan</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085438096">Hao Xu</a></p>
<p>University of California, Los Angeles, CA, USA</p>
<p><strong>Apple Inc.,</strong> CA, USA</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37275349700">Asad A. Abidi</a></p>
<p>University of California, Los Angeles, CA, USA</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088570233">饭冢哲也</a></p>
<p>日本东京大学工程学院系统设计实验室</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085438096">徐浩</a></p>
<p>美国加州大学洛杉矶分校</p>
<p>美国加利福尼亚州苹果公司</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37275349700">Asad A. Abidi</a></p>
<p>美国加州大学洛杉矶分校</p>
<h2 id="Abstract-51"><a href="#Abstract-51" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper presents a systematic design framework for ADC optimization. Our emphasis is on a robust design that is highly repeatable, which is driven by a deep understanding of the behavior of circuit building blocks. A 10 b 500 MS/s single-channel SAR ADC designed in this framework displays uniform performance for inputs up to 2 GHz at state-of-the-art FoM, which demonstrates the power of design based on analytical expressions.</p>
<p>本文提出了一个用于 ADC 优化的系统设计框架。 我们的重点是高度可重复的稳健设计，这是由对电路构建块行为的深入理解驱动的。 在此框架中设计的 10 b 500 MS/s 单通道 SAR ADC 在最先进的 FoM 下对高达 2 GHz 的输入显示出一致的性能，这展示了基于分析表达式的设计的力量。</p>
<p>类似Artificial Intelligence Designs Digital Circuits【AIDDC】</p>
<h2 id="Keywords-Index-Terms-30"><a href="#Keywords-Index-Terms-30" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><ul>
<li>IEEE Keywords <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Systematics&amp;newsearch=true">Systematics</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Conferences&amp;newsearch=true">Conferences</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Europe&amp;newsearch=true">Europe</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Tutorials&amp;newsearch=true">Tutorials</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Solid state circuit design&amp;newsearch=true">Solid state circuit design</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Tools&amp;newsearch=true">Tools</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Frequency conversion&amp;newsearch=true">Frequency conversion</a></li>
<li>Author Keywords <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Systematic design&amp;newsearch=true">Systematic design</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:analog-to-digital converter&amp;newsearch=true">analog-to-digital converter</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:regenerative comparator&amp;newsearch=true">regenerative comparator</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:sample and hold&amp;newsearch=true">sample and hold</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:harmonic distortion&amp;newsearch=true">harmonic distortion</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:jitter&amp;newsearch=true">jitter</a></li>
</ul>
<h2 id="Comments-53"><a href="#Comments-53" class="headerlink" title="Comments"></a>Comments</h2><p>老师让打印的</p>
<hr>
<h1 id="Paper27-1111"><a href="#Paper27-1111" class="headerlink" title="Paper27-1111"></a>Paper27-1111</h1><h2 id="Published-in-46"><a href="#Published-in-46" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Madhubabu Anumukonda, Prasadraju Lakkamraju, Shubhajit Roy Chowdhury, “FPGA-Based High-Performance Phonocardiography System for Extraction of Cardiac Sound Components Using Inverse Delayed Neuron Model”, <em>Frontiers in Medical Technology</em>, vol. 3, 2021.</p>
<p>IEEE搜不到</p>
<h2 id="Title-47"><a href="#Title-47" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.frontiersin.org/articles/10.3389/fmedt.2021.666650/full">FPGA-Based High-Performance Phonocardiography System for Extraction of Cardiac Sound Components Using Inverse Delayed Neuron Model</a></p>
<p>使用逆延迟神经元模型提取心脏声音成分的基于 FPGA 的高性能心音图系统（？）</p>
<h2 id="Authors-42"><a href="#Authors-42" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://www.frontiersin.org/people/u/1228973"><img src="https://loop.frontiersin.org/images/profile/1228973/24" alt="img">Madhubabu Anumukonda</a><sup>1*</sup>, <a target="_blank" rel="noopener" href="https://www.frontiersin.org/people/u/1229999"><img src="https://loop.frontiersin.org/images/profile/1229999/24" alt="img">Prasadraju Lakkamraju</a><sup>1</sup> and <img src="https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg" alt="img">Shubhajit Roy Chowdhury<sup>2</sup></p>
<p>玛达胡巴布 阿努木空达； 朴冉撒达瑞居 啦咔咔马冉居；蜀布哈吉特 罗伊 朱迪胡瑞</p>
<p>1 Center for Very Large Scale Integration and Embedded Systems Technology, International Institute of Information Technology Hyderabad, Hyderabad, India</p>
<p>1 海得拉巴国际信息技术研究所超大规模集成和嵌入式系统技术中心，印度海得拉巴</p>
<p>2 School for Computing and Electrical Engineering, Indian Institute of Technology Mandi, Suran, India</p>
<p>2 印度理工学院曼迪计算与电气工程学院，印度苏兰</p>
<h2 id="Abstract-52"><a href="#Abstract-52" class="headerlink" title="Abstract"></a>Abstract</h2><p>The study focuses on the extraction of cardiac sound components using a multi-channel micro-electromechanical system (MEMS) microphone-based phonocardiography system. The proposed multi-channel phonocardiography system classifies the cardiac sound components using artificial neural networks (ANNs) and synaptic weights that are calculated using the inverse delayed (ID) function model of the neuron. The proposed ANN model was simulated in MATLABR and implemented in a field-programmable gate array (FPGA). The proposed system examined both abnormal and normal samples collected from 30 patients. Experimental results revealed a good sensitivity of 99.1% and an accuracy of 0.9.</p>
<p>该研究的重点是使用基于多通道微机电系统 (micro-electromechanical system, MEMS) 麦克风的心音图(phonocardiography)系统提取心脏声音成分。 所提出的多通道心音图系统使用人工神经网络 (artificial neural networks, ANN) 和使用神经元的逆延迟 (inverse delayed, ID) 函数模型计算的突触权重？对心脏声音分量进行分类。 提出的 ANN 模型在 MATLABR 中进行了模拟，并在现场可编程门阵列 (field-programmable gate array, FPGA) 中实现。 所提出的系统检查了从 30 名患者收集的异常和正常样本。 实验结果显示良好的灵敏度为 99.1%，准确度为 0.9。</p>
<h2 id="Keywords-Index-Terms-31"><a href="#Keywords-Index-Terms-31" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>phonocardiography, cardiac sounds, inverse delayed function model of neuron, artificial neural networks, field programmable gate array</p>
<p>心音图、心音、神经元逆延迟函数模型、人工神经网络、现场可编程门阵列</p>
<h2 id="Comments-54"><a href="#Comments-54" class="headerlink" title="Comments"></a>Comments</h2><p>没有看的必要。</p>
<h2 id="References-39"><a href="#References-39" class="headerlink" title="References"></a>References</h2><p>MATLABR应该就是MATLAB</p>
<hr>
<h1 id="Paper24-1110"><a href="#Paper24-1110" class="headerlink" title="Paper24-1110"></a>Paper24-1110</h1><h2 id="Published-in-47"><a href="#Published-in-47" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Hui Chen, Lin Jiang, Heping Yang, Zhonghai Lu, Yuxiang Fu, Li Li, Zongguang Yu, “An Efficient Hardware Architecture with Adjustable Precision and Extensible Range to Implement Sigmoid and Tanh Functions”, <em>Electronics</em>, vol. 9, pp. 1739, 2020.</p>
<h2 id="Title-48"><a href="#Title-48" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.mdpi.com/2079-9292/9/10/1739">An Efficient Hardware Architecture with Adjustable Precision and Extensible Range to Implement Sigmoid and Tanh Functions</a></p>
<h2 id="Authors-43"><a href="#Authors-43" class="headerlink" title="Authors"></a>Authors</h2><p>Hui Chen, Lin Jiang, Heping Yang, Zhonghai Lu, Yuxiang Fu, Li Li, Zongguang Yu</p>
<h2 id="Abstract-53"><a href="#Abstract-53" class="headerlink" title="Abstract"></a>Abstract</h2><p>The efficient and precise hardware implementations of tanh and sigmoid functions play an important role in various neural network algorithms. Different applications have different requirements for accuracy. However, it is difficult for traditional methods to achieve adjustable precision. Therefore, we propose an efficient-hardware, adjustable-precision and high-speed architecture to implement them for the first time. Firstly, we present two methods to implement sigmoid and tanh functions. One is based on the rotation mode of hyperbolic CORDIC and the vector mode of linear CORDIC (called RHC-VLC), another is based on the carry-save method and the vector mode of linear CORDIC (called CSM-VLC). We validate the two methods by MATLAB and RTL implementations. Synthesized under the TSMC 40 nm CMOS technology, we find that a special case , based on RHC-VLC method, has the area of 4290.98 m and the power of 1.69 mW at the frequency of 1.5 GHz. However, under the same frequency, (a special case based on CSM-VLC method) costs 3196.36 m area and 1.38 mW power. They are both superior to existing methods for implementing such an architecture with adjustable precision.</p>
<p>tanh 和 sigmoid 函数的高效和精确的硬件实现在各种神经网络算法中发挥着重要作用。不同的应用对精度有不同的要求。然而，传统方法难以达到可调节的精度。因此，我们首次提出了一种高效硬件、可调精度和高速架构来实现它们。首先，我们提出了两种实现 sigmoid 和 tanh 函数的方法。一种是基于双曲线CORDIC的旋转模式和线性CORDIC的向量模式（称为RHC-VLC），另一种是基于进位保存方法和线性CORDIC的向量模式（称为CSM-VLC）。我们通过 MATLAB 和 RTL 实现验证了这两种方法。在台积电 40 nm CMOS 工艺下合成，我们发现一个特例，基于 RHC-VLC 方法，在 1.5 GHz 频率下的面积为 4290.98 m，功率为 1.69 mW。然而，在相同频率下，（基于CSM-VLC方法的特例）成本为3196.36 m面积和1.38 mW功率。它们都优于以可调精度实现这种架构的现有方法。</p>
<h2 id="Keywords-Index-Terms-32"><a href="#Keywords-Index-Terms-32" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p><a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=sigmoid and tanh">sigmoid and tanh</a>; <a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=hyperbolic and linear CORDIC">hyperbolic and linear CORDIC</a>; <a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=carry-save method">carry-save method</a>; <a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=efficient hardware">efficient hardware</a>; <a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=adjustable precision">adjustable precision</a>; <a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=high speed">high speed</a></p>
<h2 id="Comments-55"><a href="#Comments-55" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-40"><a href="#References-40" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper23-1115"><a href="#Paper23-1115" class="headerlink" title="Paper23-1115"></a>Paper23-1115</h1><h2 id="Published-in-48"><a href="#Published-in-48" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Yunqi Gao, Feng Luan, Jiaqi Pan, Xu Li, Yaodong He, “FPGA-Based Implementation of Stochastic Configuration Networks for Regression Prediction”, <em>Sensors</em>, vol. 20, pp. 4191, 2020.</p>
<h2 id="Title-49"><a href="#Title-49" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.mdpi.com/1424-8220/20/15/4191">FPGA-Based Implementation of Stochastic Configuration Networks for Regression Prediction</a></p>
<h2 id="Authors-44"><a href="#Authors-44" class="headerlink" title="Authors"></a>Authors</h2><p>Yunqi Gao</p>
<p>Feng Luan </p>
<p>Jiaqi Pan</p>
<p>Xu Li</p>
<p>Yaodong He </p>
<h2 id="Abstract-54"><a href="#Abstract-54" class="headerlink" title="Abstract"></a>Abstract</h2><p>The implementation of neural network regression prediction based on digital circuits is one of the challenging problems in the field of machine learning and cognitive recognition, and it is also an effective way to relieve the pressure of the Internet in the era of intelligence. As a nonlinear network, the stochastic configuration network (SCN) is considered to be an effective method for regression prediction due to its good performance in learning and generalization. Therefore, in this paper, we adapt the SCN to regression analysis, and design and verify the field programmable gate array (FPGA) framework to implement SCN model for the first time. In addition, in order to improve the performance of the SCN model based on the FPGA, the implementation of the nonlinear activation function on the FPGA is optimized, which effectively improves the prediction accuracy while considering the utilization rate of hardware resources. Experimental results based on the simulation data set and the real data set prove that the proposed FPGA framework successfully implements the SCN regression prediction model, and the improved SCN model has higher accuracy and a more stable performance. Compared with the extreme learning machine (ELM), the prediction performance of the proposed SCN implementation model based on the FPGA for the simulation data set and the real data set is improved by 56.37% and 17.35%, respectively.</p>
<p>基于数字电路的神经网络回归预测的实现是机器学习和认知识别领域的挑战性问题之一，也是智能时代缓解互联网压力的有效途径。作为一种非线性网络，随机配置网络（SCN）因其在学习和泛化方面的良好性能而被认为是一种有效的回归预测方法。因此，在本文中，我们将 SCN 应用于回归分析，并首次设计并验证了现场可编程门阵列 (FPGA) 框架来实现 SCN 模型。此外，为了提高基于FPGA的SCN模型的性能，对非线性激活函数在FPGA上的实现进行了优化，在考虑硬件资源利用率的同时有效提高了预测精度。基于仿真数据集和真实数据集的实验结果证明，所提出的FPGA框架成功实现了SCN回归预测模型，改进后的SCN模型具有更高的精度和更稳定的性能。与极限学习机（ELM）相比，所提出的基于FPGA的SCN实现模型对仿真数据集和真实数据集的预测性能分别提高了56.37%和17.35%。</p>
<h2 id="Keywords-Index-Terms-33"><a href="#Keywords-Index-Terms-33" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-56"><a href="#Comments-56" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-41"><a href="#References-41" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper21-1118"><a href="#Paper21-1118" class="headerlink" title="Paper21-1118"></a>Paper21-1118</h1><h2 id="Published-in-49"><a href="#Published-in-49" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Linyu Wei, Jueping Cai, Vantruong Nguyen, Jie Chu, Kailin Wen, “P-SFA: Probability based Sigmoid Function Approximation for Low-complexity Hardware Implementation”, <em>Microprocessors and Microsystems</em>, pp. 103105, 2020.</p>
<h2 id="Title-50"><a href="#Title-50" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726">P-SFA: Probability based Sigmoid Function Approximation for Low-complexity Hardware Implementation</a></p>
<h2 id="Authors-45"><a href="#Authors-45" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726#!">Linyu Wei</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726#!">Jueping Cai</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726#!">Vantruong Nguyen</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726#!">Jie Chu</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726#!">Kailin Wen</a></p>
<h2 id="Abstract-55"><a href="#Abstract-55" class="headerlink" title="Abstract"></a>Abstract</h2><p>A probability-based sigmoid function approximation (P-SFA), which is based on piecewise linear function and neuron’s values statistical probability distribution in each layer, is proposed to lower the complexity of neural network hardware implementation with only addition circuit. The sigmoid function is divided into three fixed regions, and the number of sub-regions with different sizes in each fixed region is adapted to neuron’s values distribution in each layer to reduce the error between the sigmoid function and P-SFA function. The experimental results on FPGA show that the P-SFA function is efficient in terms of power and speed, and the recognition accuracies in DNN and CNN for MNIST with P-SFA are the highest among the state-of-the-art methods, up to 97.46% and 99.02% respectively.</p>
<p>为了降低神经网络硬件实现的复杂度，仅用加法电路，提出了一种基于分段线性函数和神经元值统计概率分布的基于概率的sigmoid函数逼近（P-SFA）。 sigmoid 函数被划分为三个固定区域，每个固定区域中不同大小的子区域的数量适应每层神经元的值分布，以减少 sigmoid 函数和 P-SFA 函数之间的误差。 在 FPGA 上的实验结果表明，P-SFA 函数在功率和速度方面是高效的，并且在 DNN 和 CNN 中对带有 P-SFA 的 MNIST 的识别准确率是最先进的方法中最高的，高达 分别为 97.46% 和 99.02%。</p>
<h2 id="Keywords-Index-Terms-34"><a href="#Keywords-Index-Terms-34" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>Sigmoid function, Neural networks, Piecewise linear function</p>
<h2 id="Comments-57"><a href="#Comments-57" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-42"><a href="#References-42" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper20-1119"><a href="#Paper20-1119" class="headerlink" title="Paper20-1119"></a>Paper20-1119</h1><h2 id="Published-in-50"><a href="#Published-in-50" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Xuhui Yang, Qingguo Zhou, Jinqiang Wang, Lihong Han, Fang Feng, Rui Zhou, Kuan-Ching Li, “FPGA-based approximate calculation system of General Vector Machine”, <em>Microelectronics Journal</em>, 2019.</p>
<h2 id="Title-51"><a href="#Title-51" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918">FPGA-based approximate calculation system of General Vector Machine</a></p>
<h2 id="Authors-46"><a href="#Authors-46" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Xuhui Yang</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Qingguo Zhou</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Jinqiang Wang</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Lihong Han</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Fang Feng</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Rui Zhou</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Kuan-Ching Li</a></p>
<h2 id="Abstract-56"><a href="#Abstract-56" class="headerlink" title="Abstract"></a>Abstract</h2><p>In this paper, a General Vector Machine (GVM) approximate calculation system based on FPGA is introduced. The author designed a parallel computing architecture of GVM on FPGA, discussed a matrix parallel computing method, developed approximation calculation methods of sigmoid function and softmax layer on FPGA. As an example, the paper implemented GVM on FPGA to identify MNIST data sets and tested the recognition rate with 14 different data accuracy of parameters, then gave some suggestions for data accuracy selection for parameters. Finally, the approximate calculation system is implemented and tested on XCKU3P, XC7Z020, XC7VX690 and XCUV190 FPGA chips. The results demonstrate that the computation speed is 112 times higher than CPU (Intel Core(TM) i9-7900X), and the performance can be equivalent to the GPU (NVIDIA GTX 1080 Ti). The approximate calculation system can effectively accelerate the calculation of GVM. It has the characteristics of good acceleration, and is suitable for embedded intelligent devices.</p>
<p>本文介绍了一种基于FPGA的通用向量机(GVM)近似计算系统。作者在FPGA上设计了GVM并行计算架构，讨论了矩阵并行计算方法，开发了FPGA上sigmoid函数和softmax层的近似计算方法。例如，论文在FPGA上实现GVM对MNIST数据集进行识别，并测试了14种不同数据精度参数的识别率，然后给出了参数数据精度选择的一些建议。最后，在XCKU3P、XC7Z020、XC7VX690和XCUV190 FPGA芯片上实现并测试了近似计算系统。结果表明，计算速度比CPU（Intel Core(TM) i9-7900X）高112倍，性能可与GPU（NVIDIA GTX 1080 Ti）相当。近似计算系统可以有效加速GVM的计算。具有加速性好等特点，适用于嵌入式智能设备。</p>
<h2 id="Keywords-Index-Terms-35"><a href="#Keywords-Index-Terms-35" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-58"><a href="#Comments-58" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-43"><a href="#References-43" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper19-1120"><a href="#Paper19-1120" class="headerlink" title="Paper19-1120"></a>Paper19-1120</h1><h2 id="Published-in-51"><a href="#Published-in-51" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Samba Raju Chiluveru, Gyanendra, Snehit Chunarkar, Manoj Tripathy, Brajesh Kumar Kaushik, “Efficient Hardware Implementation of DNN-Based Speech Enhancement Algorithm With Precise Sigmoid Activation Function”, <em>Circuits and Systems II: Express Briefs IEEE Transactions on</em>, vol. 68, no. 11, pp. 3461-3465, 2021.</p>
<h2 id="Title-52"><a href="#Title-52" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9439489">Efficient Hardware Implementation of DNN-Based Speech Enhancement Algorithm With Precise Sigmoid Activation Function</a></p>
<h2 id="Authors-47"><a href="#Authors-47" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088523310">Samba Raju Chiluveru</a></p>
<p>Instrumentation and Signal Processing Group, Indian Institute of Technology Roorkee, Roorkee, India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088563152">Gyanendra</a></p>
<p>Microelectronics and VLSI Group, Indian Institute of Technology Roorkee, Roorkee, India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37089005346">Snehit Chunarkar</a></p>
<p>Instrumentation and Signal Processing Group, Indian Institute of Technology Roorkee, Roorkee, India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37301125900">Manoj Tripathy</a></p>
<p>Instrumentation and Signal Processing Group, Indian Institute of Technology Roorkee, Roorkee, India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37394845100">Brajesh Kumar Kaushik</a></p>
<p>Microelectronics and VLSI Group, Indian Institute of Technology Roorkee, Roorkee, India</p>
<h2 id="Abstract-57"><a href="#Abstract-57" class="headerlink" title="Abstract"></a>Abstract</h2><p>This brief presents the hardware implementation of deep neural network-based speech enhancement algorithm (DNN-SEA) with a precise sigmoid activation function. Further, an adaptive step-size-based slope and intercept method (AS-SIM) has been developed to approximate the sigmoid function that uses the maximum allowable error (ϵ) as an input parameter. The performance of the DNN-SEA is measured in terms of speech quality and intelligibility using a perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI) parameters, respectively. Hardware implementation is performed by Synopsys Design Compiler with a TSMC 90-nm library. The performance comparison has been conducted in terms of area, gate count, delay, and power consumption. Results confirm that the proposed AS-SIM approximation improves performance in terms of PESQ and STOI value and significantly reduces the area requirement and power consumption.</p>
<p>本简介介绍了具有精确 sigmoid 激活函数的基于深度神经网络的语音增强算法 (DNN-SEA) 的硬件实现。 此外，已经开发了一种基于自适应步长的斜率和截距方法 (AS-SIM) 来近似使用最大允许误差 (ϵ) 作为输入参数的 sigmoid 函数。 DNN-SEA 的性能分别使用语音质量感知评估 (PESQ) 和短时客观可懂度 (STOI) 参数在语音质量和可懂度方面进行测量。 硬件实现由 Synopsys 设计编译器使用 TSMC 90-nm 库执行。 在面积、门数、延迟和功耗方面进行了性能比较。 结果证实，所提出的 AS-SIM 近似提高了 PESQ 和 STOI 值方面的性能，并显着降低了面积要求和功耗。</p>
<h2 id="Keywords-Index-Terms-36"><a href="#Keywords-Index-Terms-36" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-59"><a href="#Comments-59" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-44"><a href="#References-44" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper18-1121"><a href="#Paper18-1121" class="headerlink" title="Paper18-1121"></a>Paper18-1121</h1><h2 id="Published-in-52"><a href="#Published-in-52" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Fatemeh Mohammadi Shakiba, MengChu Zhou, “Novel Analog Implementation of a Hyperbolic Tangent Neuron in Artificial Neural Networks”, <em>Industrial Electronics IEEE Transactions on</em>, vol. 68, no. 11, pp. 10856-10867, 2021.</p>
<h2 id="Title-53"><a href="#Title-53" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9248592">Novel Analog Implementation of a Hyperbolic Tangent Neuron in Artificial Neural Networks</a></p>
<h2 id="Authors-48"><a href="#Authors-48" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088922980">Fatemeh Mohammadi Shakiba</a></p>
<p>Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA</p>
<p>Fatemeh Mohammadi Shakiba (Graduate Student Member, IEEE) was born in Tehran, Iran. She received the M.S. degree in computer engineering from Southern Illinois University in Carbondale, Carbondale, IL, USA, in 2018. She is currently working toward the Ph.D. degree in computer engineering with New Jersey Institute of Technology, Newark, NJ, USA.</p>
<p>Her research interests include artificial neural networks, machine learning, deep learning, data classification, and data analysis.</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37273591600">MengChu Zhou</a></p>
<p>Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA</p>
<p>Institute of Systems Engineering and Collaborative Laboratory for Intelligent Science and Systems, Macau University of Science and Technology, Macau, China</p>
<p>Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China</p>
<p>MengChu Zhou (Fellow, IEEE) received the B.S. degree in control engineering from the Nanjing University of Science and Technology, Nanjing, China, in 1983, the M.S. degree in automatic control from the Beijing Institute of Technology, Beijing, China, in 1986, and the Ph.D. degree in computer and systems engineering from Rensselaer Polytechnic Institute, Troy, NY, USA, in 1990.</p>
<p>He is a Distinguished Professor of New Jersey Institute of Technology. He has authored or coauthored 900 publications including 12 books, 600+ journal papers (500+ in IEEE transactions), 27 patents and 29 book-chapters. His interests interests include Petri nets, intelligent automation, and big data.</p>
<p>Prof. Zhou is the founding Editor of IEEE Press Book Series on Systems Science and Engineering and Editor-in-Chief of IEEE/CAA Journal of Automatica Sinica. He is Fellow of IFAC, AAAS, CAA, and NAI.</p>
<h2 id="Abstract-58"><a href="#Abstract-58" class="headerlink" title="Abstract"></a>Abstract</h2><p>Recently, enormous datasets have made power dissipation and area usage lie at the heart of designs for artificial neural networks (ANNs). Considering the significant role of activation functions in neurons and the growth of hardware-based neural networks like memristive neural networks, this work proposes a novel design for a hyperbolic tangent activation function (Tanh) to be used in memristive-based neuromorphic architectures. The purpose of implementing a CMOS-based design for Tanh is to decrease power dissipation and area usage. This design also increases the overall speed of computation in ANNs, while keeping the accuracy in an acceptable range. The proposed design is one of the first analog designs for the hyperbolic tangent and its performance is analyzed by using two well-known datsets, including the Modified National Institute of Standards and Technology (MNIST) and Fashion-MNIST. The direct implementation of the proposed design for Tanh is proposed and investigated via software and hardware modeling.</p>
<p>最近，大量数据集使功耗和面积使用成为人工神经网络 (ANN) 设计的核心。考虑到激活函数在神经元中的重要作用以及基于硬件的神经网络（如忆阻神经网络）的增长，这项工作提出了一种用于基于忆阻的神经形态架构的双曲正切激活函数 (Tanh) 的新设计。为 Tanh 实施基于 CMOS 的设计的目的是降低功耗和面积使用。这种设计还提高了人工神经网络的整体计算速度，同时将精度保持在可接受的范围内。所提议的设计是最早的双曲正切模拟设计之一，其性能通过使用两个众所周知的数据集进行分析，包括修改后的国家标准与技术研究所 (MNIST) 和 Fashion-MNIST。通过软件和硬件建模提出并研究了 Tanh 所提议设计的直接实现。</p>
<h2 id="Keywords-Index-Terms-37"><a href="#Keywords-Index-Terms-37" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-60"><a href="#Comments-60" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-45"><a href="#References-45" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper17-1122"><a href="#Paper17-1122" class="headerlink" title="Paper17-1122"></a>Paper17-1122</h1><h2 id="Published-in-53"><a href="#Published-in-53" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Muhammad Awais Hussain, Tsung-Han Tsai, “An Efficient and Fast Softmax Hardware Architecture (EFSHA) for Deep Neural Networks”, <em>Artificial Intelligence Circuits and Systems (AICAS) 2021 IEEE 3rd International Conference on</em>, pp. 1-4, 2021.</p>
<h2 id="Title-54"><a href="#Title-54" class="headerlink" title="Title"></a>Title</h2><p>[An Efficient and Fast Softmax Hardware Architecture (EFSHA) for Deep Neural Networks]9<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9458541">https://ieeexplore.ieee.org/abstract/document/9458541</a></p>
<h2 id="Authors-49"><a href="#Authors-49" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086446904">Muhammad Awais Hussain</a></p>
<p>National Central University, Zhongli, Taiwan</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085650268">Tsung-Han Tsai</a></p>
<p>National Central University, Zhongli, Taiwan</p>
<h2 id="Abstract-59"><a href="#Abstract-59" class="headerlink" title="Abstract"></a>Abstract</h2><p>Deep neural networks are widely used in computer vision applications due to their high performance. However, DNNs involve a large number of computations in the training and inference phase. Among the different layers of a DNN, the softmax layer has one of the most complex computations as it involves exponent and division operations. So, a hardware-efficient implementation is required to reduce the on-chip resources. In this paper, we propose a new hardware-efficient and fast implementation of the softmax activation function. The proposed hardware implementation consumes fewer hardware resources and works at high speed as compared to the state-of-the-art techniques.</p>
<p>深度神经网络由于其高性能而广泛用于计算机视觉应用。 然而，DNN 在训练和推理阶段涉及大量计算。 在 DNN 的不同层中，softmax 层具有最复杂的计算之一，因为它涉及指数和除法运算。 因此，需要一种硬件高效的实现来减少片上资源。 在本文中，我们提出了一种新的硬件高效且快速的 softmax 激活函数实现。 与最先进的技术相比，所提出的硬件实现消耗更少的硬件资源并高速工作。</p>
<h2 id="Keywords-Index-Terms-38"><a href="#Keywords-Index-Terms-38" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-61"><a href="#Comments-61" class="headerlink" title="Comments"></a>Comments</h2><p>这篇文章的结果可能可以作为对比。相对友好。</p>
<h2 id="References-46"><a href="#References-46" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper16-1125"><a href="#Paper16-1125" class="headerlink" title="Paper16-1125"></a>Paper16-1125</h1><h2 id="Published-in-54"><a href="#Published-in-54" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Mahmoud Masadeh, Osman Hasan, Sofiène Tahar, “Machine-Learning-Based Self-Tunable Design of Approximate Computing”, <em>Very Large Scale Integration (VLSI) Systems IEEE Transactions on</em>, vol. 29, no. 4, pp. 800-813, 2021.</p>
<h2 id="Title-55"><a href="#Title-55" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9360869">Machine-Learning-Based Self-Tunable Design of Approximate Computing</a></p>
<h2 id="Authors-50"><a href="#Authors-50" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085711380">Mahmoud Masadeh</a></p>
<p>Concordia University, Montreal, Canada</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37601575700">Osman Hasan</a></p>
<p>Concordia University, Montreal, Canada</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37277153600">Sofiène Tahar</a></p>
<p>Concordia University, Montreal, Canada</p>
<h2 id="Abstract-60"><a href="#Abstract-60" class="headerlink" title="Abstract"></a>Abstract</h2><p>Approximate computing (AC) is an emerging computing paradigm suitable for intrinsic error-tolerant applications to reduce energy consumption and execution time. Different approximate techniques and designs, at both hardware and software levels, have been proposed and demonstrated the effectiveness of relaxing the average output quality constraint. However, the output quality of AC is highly input-dependent, i.e., for some input data, the output errors may reach unacceptable levels. Therefore, there is a dire need for an input-dependent tunable approximate design. With this motivation, in this article, we propose a lightweight and efficient machine-learning-based approach to build an input-aware design selector, i.e., quality controller, to adapt the approximate design in order to meet the target output quality (TOQ). For illustration purposes, we use a library of 8-bit and 16-bit energy-efficient approximate array multipliers with 20 different settings, which are commonly used in image and audio processing applications. The simulation results, based on two sets of images, including an 8 Scene Categories Dataset, which is a benchmark of images data set, demonstrate the effectiveness of the lightweight selector where the proposed tunable design achieves a significant reduction in quality loss with relatively low overhead.</p>
<p>近似计算 (AC) 是一种新兴的计算范式，适用于内在容错应用程序，以减少能耗和执行时间。已经在硬件和软件级别提出了不同的近似技术和设计，并证明了放宽平均输出质量约束的有效性。然而，AC 的输出质量高度依赖于输入，即对于某些输入数据，输出误差可能达到不可接受的水平。因此，迫切需要一种依赖于输入的可调近似设计。出于这个动机，在本文中，我们提出了一种轻量级且高效的基于机器学习的方法来构建输入感知设计选择器，即质量控制器，以适应近似设计以满足目标输出质量 (TOQ) .出于说明目的，我们使用了 8 位和 16 位节能近似阵列乘法器库，它们具有 20 种不同的设置，这些乘法器通常用于图像和音频处理应用程序。模拟结果基于两组图像，包括 8 个场景类别数据集，这是图像数据集的基准，证明了轻量级选择器的有效性，其中所提出的可调设计以相对较低的开销实现了质量损失的显着降低.</p>
<h2 id="Keywords-Index-Terms-39"><a href="#Keywords-Index-Terms-39" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-62"><a href="#Comments-62" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大。</p>
<h2 id="References-47"><a href="#References-47" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper12-1109"><a href="#Paper12-1109" class="headerlink" title="Paper12-1109"></a>Paper12-1109</h1><h2 id="Published-in-55"><a href="#Published-in-55" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>T.K.R Arvind, Marcel Brand, Christian Heidorn, Srinivas Boppu, Frank Hannig, Jürgen Teich, “Hardware Implementation of Hyperbolic Tangent Activation Function for Floating Point Formats”, <em>VLSI Design and Test (VDAT) 2020 24th International Symposium on</em>, pp. 1-6, 2020.</p>
<h2 id="Title-56"><a href="#Title-56" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9190305">Hardware Implementation of Hyperbolic Tangent Activation Function for Floating Point Formats</a></p>
<h2 id="Authors-51"><a href="#Authors-51" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088503670">T.K.R Arvind</a></p>
<p>School of Electrical Sciences, Indian Institute of Technology Bhubaneswar (IITBBS), India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086153198">Marcel Brand</a></p>
<p>Hardware/Software Co-Design, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088368833">Christian Heidorn</a></p>
<p>Hardware/Software Co-Design, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/38296883500">Srinivas Boppu</a></p>
<p>School of Electrical Sciences, Indian Institute of Technology Bhubaneswar (IITBBS), India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37266978900">Frank Hannig</a></p>
<p>Hardware/Software Co-Design, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37276355800">Jürgen Teich</a></p>
<p>Hardware/Software Co-Design, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany</p>
<h2 id="Abstract-61"><a href="#Abstract-61" class="headerlink" title="Abstract"></a>Abstract</h2><p>In this paper, we present the efficient hardware implementation of hyperbolic tangent activation function, which is most widely used in artificial neural networks for accelerating machine learning applications. The proposed design considers the floating point representation of numbers for the first time, the nonlinear nature of the activation function while sampling, and uses a lookup table for implementation. The unique way of dividing the input range into bins which follows the binary pattern reduces the hardware implementation cost. Furthermore, the input data itself is used as the address for lookup table; thus, no extra cost involved in hashing the lookup table and involves only one memory access time resulting in faster and efficient hardware implementation. Our design proves to be 3× faster when compared to similar hardware implementations using CMOS 90 nm process.</p>
<p>在本文中，我们介绍了双曲正切激活函数的高效硬件实现，该函数在人工神经网络中最广泛用于加速机器学习应用。 所提出的设计首次考虑了数字的浮点表示，采样时激活函数的非线性特性，并使用查找表来实现。 将输入范围划分为遵循二进制模式的 bin 的独特方式降低了硬件实现成本。 此外，输入数据本身用作查找表的地址； 因此，散列查找表不涉及额外成本，并且只涉及一次内存访问时间，从而实现更快、更高效的硬件实现。 与使用 CMOS 90 nm 工艺的类似硬件实现相比，我们的设计被证明要快 3 倍。</p>
<h2 id="Keywords-Index-Terms-40"><a href="#Keywords-Index-Terms-40" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-63"><a href="#Comments-63" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-48"><a href="#References-48" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper8-1108"><a href="#Paper8-1108" class="headerlink" title="Paper8-1108"></a>Paper8-1108</h1><h2 id="Published-in-56"><a href="#Published-in-56" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://academic.oup.com/nar">Nucleic Acids Research</a> [Oxford Academic]</p>
<p>Volume 46, Issue 6, 6 April 2018, Page e33,</p>
<h2 id="Title-57"><a href="#Title-57" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://academic.oup.com/nar/article/46/6/e33/4791133?login=true">HipMCL: a high-performance parallel implementation of the Markov clustering algorithm for large-scale networks - FREE</a></p>
<p>HipMCL：大规模网络马尔可夫聚类算法的高性能并行实现 - 免费</p>
<h2 id="Authors-52"><a href="#Authors-52" class="headerlink" title="Authors"></a>Authors</h2><p>Ariful Azad, Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720-8150, USA 劳伦斯伯克利国家实验室计算研究部</p>
<p>Georgios A Pavlopoulos, DOE Joint Genome Institute, Lawrence Berkeley National Laboratory, 2800 Mitchell Drive, Walnut Creek, CA 94598, USA 美国能源部联合基因组研究所，劳伦斯伯克利国家实验室</p>
<p>Christos A Ouzounis, Biological Computation &amp; Process Laboratory, Chemical Process &amp; Energy Resources Institute, Centre for Research &amp; Technology Hellas, Thessalonica 57001, Greece 生物计算与过程实验室，化学过程与能源研究所，希腊研究与技术中心，希腊塞萨洛尼卡</p>
<p>Nikos C Kyrpides, DOE Joint Genome Institute, Lawrence Berkeley National Laboratory, 2800 Mitchell Drive, Walnut Creek, CA 94598, USA 美国能源部联合基因组研究所，劳伦斯伯克利国家实验室</p>
<p>Aydin Buluç, Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720-8150, USA and Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA 94720, USA 劳伦斯伯克利国家实验室计算研究部；加州大学伯克利分校电气工程与计算机科学系</p>
<h2 id="Abstract-62"><a href="#Abstract-62" class="headerlink" title="Abstract"></a>Abstract</h2><p>Biological networks capture structural or functional properties of relevant entities such as molecules, proteins or genes. Characteristic examples are gene expression networks or protein–protein interaction networks, which hold information about functional affinities or structural similarities. Such networks have been expanding in size due to increasing scale and abundance of biological data. While various clustering algorithms have been proposed to find highly connected regions, Markov Clustering (MCL) has been one of the most successful approaches to cluster sequence similarity or expression networks. Despite its popularity, MCL’s scalability to cluster large datasets still remains a bottleneck due to high running times and memory demands. Here, we present High-performance MCL (HipMCL), a parallel implementation of the original MCL algorithm that can run on distributed-memory computers. We show that HipMCL can efficiently utilize 2000 compute nodes and cluster a network of ∼70 million nodes with ∼68 billion edges in ∼2.4 h. By exploiting distributed-memory environments, HipMCL clusters large-scale networks several orders of magnitude faster than MCL and enables clustering of even bigger networks. HipMCL is based on MPI and OpenMP and is freely available under a modified BSD license.</p>
<p>生物网络捕获相关实体（例如分子、蛋白质或基因）的结构或功能特性。典型的例子是基因表达网络或蛋白质-蛋白质相互作用网络，它们保存有关功能亲和力或结构相似性的信息。由于生物数据的规模和丰富性不断增加，此类网络的规模一直在扩大。虽然已经提出了各种聚类算法来寻找高度连接的区域，但马尔可夫聚类 (MCL) 一直是对序列相似性或表达网络进行聚类的最成功的方法之一。尽管它很受欢迎，但由于高运行时间和内存需求，<strong>MCL 对大型数据集进行集群的可扩展性仍然是一个瓶颈</strong>。在这里，我们展示了高性能 MCL (HipMCL)，它是原始 MCL 算法的并行实现，可以在分布式内存计算机上运行。我们表明 HipMCL 可以有效地利用 2000 个计算节点，并在 2.4 小时内将一个由 7000 万个节点和 680 亿条边组成的网络聚集在一起。通过利用分布式内存环境，HipMCL 对大规模网络的集群比 MCL 快几个数量级，并支持对更大网络的集群。 HipMCL 基于 MPI 和 OpenMP，<a target="_blank" rel="noopener" href="https://bitbucket.org/azadcse/hipmcl/">可在修改后的 BSD 许可下免费获得</a>。</p>
<h2 id="Keywords"><a href="#Keywords" class="headerlink" title="Keywords"></a>Keywords</h2><p>No keywords</p>
<h2 id="Comments-64"><a href="#Comments-64" class="headerlink" title="Comments"></a>Comments</h2><p>“By exploiting distributed-memory environments, <a target="_blank" rel="noopener" href="https://bitbucket.org/azadcse/hipmcl/">HipMCL</a> clusters large-scale networks <strong>several orders of magnitude faster</strong> than MCL and enables clustering of even bigger networks. “</p>
<p>“震惊地望着虚空上的残影。<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/萧炎">萧炎</a>半晌无语，他没想到。<a target="_blank" rel="noopener" href="https://bitbucket.org/azadcse/hipmcl/">HipMCL</a>的速度，竟然恐怖如斯。”</p>
<hr>
<h1 id="Paper7-1107"><a href="#Paper7-1107" class="headerlink" title="Paper7-1107"></a>Paper7-1107</h1><h2 id="Published-in-57"><a href="#Published-in-57" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://journals.sagepub.com/home/hpc">The International Journal of High Performance Computing Applications</a></p>
<h2 id="Title-58"><a href="#Title-58" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://journals.sagepub.com/doi/abs/10.1177/1094342011403516">The Combinatorial BLAS: design, implementation, and applications</a></p>
<p><a target="_blank" rel="noopener" href="http://nidhogg.cs.ucsb.edu/research/tech_reports/reports/2010-18.pdf">Download</a></p>
<h2 id="Authors-53"><a href="#Authors-53" class="headerlink" title="Authors"></a>Authors</h2><p>Aydın Buluc: High Performance Computing Research Lawrence Berkeley National Laboratory</p>
<p>高性能计算研究劳伦斯伯克利国家实验室</p>
<p> 1 Cyclotron Road, Berkeley, CA 94720 abuluc@lbl.gov</p>
<p>John R. Gilbert: Department of Computer Science University of California, Santa Barbara </p>
<p>加州大学圣巴巴拉分校计算机科学系</p>
<p>Santa Barbara, CA 93106-5110 gilbert@cs.ucsb.edu</p>
<h2 id="Abstract-63"><a href="#Abstract-63" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper presents a scalable high-performance software library to be used for graph analysis and data mining. Large combinatorial graphs appear in many applications of high-performance computing, including computational biology, informatics, analytics, web search, dynamical systems, and sparse matrix methods. Graph computations are difficult to parallelize using traditional approaches due to their irregular nature and low operational intensity. Many graph computations, however, contain sufficient coarse-grained parallelism for thousands of processors, which can be uncovered by using the right primitives. We describe the parallel Combinatorial BLAS, which consists of a small but powerful set of linear algebra primitives specifically targeting graph and data mining applications. We provide an extensible library interface and some guiding principles for future development. The library is evaluated using two important graph algorithms, in terms of both performance and ease-of-use. The scalability and raw performance of the example applications, using the Combinatorial BLAS, are unprecedented on distributed memory clusters.</p>
<p>本文提出了一个<strong>可扩展的</strong>高性能软件库，用于图形分析和数据挖掘。大型组合图出现在高性能计算的许多应用中，包括计算生物学、信息学、分析学、网络搜索、动力系统和稀疏矩阵方法。由于其不规则性和<strong>低操作强度</strong>，图计算难以使用传统方法并行化。然而，许多图计算包含足够的用于数千个处理器的粗粒度并行性，这可以通过使用正确的primitives来发现。我们描述了并行组合 BLAS，它由一组小而强大的线性代数原语(primitives)组成，专门针对图和数据挖掘应用程序。我们提供了一个可扩展的库接口和一些未来发展的指导原则。在性能和易用性方面，该库使用两种重要的图形算法进行评估。使用组合 BLAS 的示例应用程序的可扩展性和原始性能在分布式内存集群上是前所未有的。</p>
<h2 id="Keywords-1"><a href="#Keywords-1" class="headerlink" title="Keywords"></a>Keywords</h2><p>Mathematical Software, Graph Analysis, Software Framework, Sparse Matrices, Combinatorial Scientific Computing.</p>
<p>数学软件、图形分析、软件框架、稀疏矩阵、组合科学计算。</p>
<p><a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Betweenness+Centrality">Betweenness centrality</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Combinatorial+BLAS">combinatorial BLAS</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Combinatorial+Scientific+Computing">combinatorial scientific computing</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Graph+Analysis">graph analysis</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Markov+Clustering">Markov clustering</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Mathematical+Software">mathematical software</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Parallel+Graph+Library">parallel graph library</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Software+Framework">software framework</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Sparse+Matrices">sparse matrices</a></p>
<p>介数中心性、组合BLAS、组合科学计算、图分析、马尔可夫聚类、数学软件、并行图库、软件框架、稀疏矩阵</p>
<h2 id="Comments-65"><a href="#Comments-65" class="headerlink" title="Comments"></a>Comments</h2><p>看不懂。</p>
<hr>
<h1 id="Paper6-1105"><a href="#Paper6-1105" class="headerlink" title="Paper6-1105"></a>Paper6-1105</h1><h2 id="Published-in-58"><a href="#Published-in-58" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://link.springer.com/conference/para">International Workshop on Applied Parallel Computing</a></p>
<p>PARA 2006: <a target="_blank" rel="noopener" href="https://link.springer.com/book/10.1007/978-3-540-75755-9">Applied Parallel Computing. State of the Art in Scientific Computing</a> pp 260-269| <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-540-75755-9_32#citeas">Cite as</a></p>
<h2 id="Title-59"><a href="#Title-59" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-540-75755-9_32">High-Performance Graph Algorithms from Parallel Sparse Matrices</a> </p>
<p>基于(from?)并行稀疏矩阵的高性能图算法</p>
<h2 id="Authors-54"><a href="#Authors-54" class="headerlink" title="Authors"></a>Authors</h2><p>John R. Gilbert<sup>1</sup>, Steve Reinhardt<sup>2</sup>, and Viral B. Shah<sup>1</sup></p>
<ol>
<li>1.University of California, Dept. of Computer Science, Harold Frank Hall, Santa Barbara, CA 93106USA</li>
<li>2.Silicon Graphics Inc. </li>
</ol>
<h2 id="Abstract-64"><a href="#Abstract-64" class="headerlink" title="Abstract"></a>Abstract</h2><p>Large-scale computation on graphs and other discrete structures is becoming increasingly important in many applications, including computational biology, web search, and knowledge discovery. High-performance combinatorial computing is an infant field, in sharp contrast with numerical scientific computing.</p>
<p>We argue that many of the tools of high-performance numerical computing – in particular, parallel algorithms and data structures for computation with sparse matrices – can form the nucleus of a robust infrastructure for parallel computing on graphs. We demonstrate this with an implementation of a graph analysis benchmark using the sparse matrix infrastructure in Star-P, our parallel dialect of the Matlab programming language.</p>
<p>图和其他离散结构的大规模计算在许多应用中变得越来越重要，包括计算生物学、网络搜索和知识发现。 高性能组合计算是一个新生领域，与数值科学计算形成鲜明对比。</p>
<p>我们认为，许多高性能数值计算工具——特别是用于稀疏矩阵计算的并行算法和数据结构——可以构成图并行计算的强大基础架构的核心。 我们通过使用 Star-P（我们的 Matlab 编程语言的并行语言）中的稀疏矩阵基础结构实现图分析基准来证明这一点。</p>
<h2 id="Keywords-Index-Terms-41"><a href="#Keywords-Index-Terms-41" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>Sparse Matrix, Input Graph, Large Graph, Sparse Matrice, Basic Design Principle</p>
<h2 id="Comments-66"><a href="#Comments-66" class="headerlink" title="Comments"></a>Comments</h2><p>被<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X">Performance optimization, modeling and analysis of sparse matrix-matrix products on multi-core and many-core processors</a>引用。</p>
<hr>
<h1 id="Paper5-1104"><a href="#Paper5-1104" class="headerlink" title="Paper5-1104"></a>Paper5-1104</h1><h2 id="Published-in-59"><a href="#Published-in-59" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/journal/parallel-computing">Parallel Computing</a></p>
<h2 id="Title-60"><a href="#Title-60" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X">Performance optimization, modeling and analysis of sparse matrix-matrix products on multi-core and many-core processors</a></p>
<p>稀疏矩阵乘法在多核和众核处理器上的性能优化、建模与分析</p>
<h2 id="Authors-55"><a href="#Authors-55" class="headerlink" title="Authors"></a>Authors</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X#!">Yusuke Nagasaka</a>: Tokyo Institute of Technology, Tokyo, Japan【长坂雄介？，东京工业大学】</li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X#!">Satoshi Matsuoka</a>: RIKEN Center for Computational Science, Kobe, Japan【松冈聪？，RIKEN 计算科学中心，日本神户】</li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X#!">Ariful Azad</a>: Indiana University, Bloomington, Indiana, USA【阿里夫·阿扎德，印第安纳大学，布卢明顿，印第安纳州，美国】 </li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X#!">Aydın Buluç</a>: Lawrence Berkeley National Laboratory, Berkeley, California, USA【艾登布卢奇，劳伦斯伯克利国家实验室，美国加利福尼亚州伯克利】</li>
</ul>
<h2 id="Abstract-65"><a href="#Abstract-65" class="headerlink" title="Abstract"></a>Abstract</h2><p>Sparse matrix-matrix multiplication (SpGEMM) is a computational primitive that is widely used in areas ranging from traditional <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/engineering/numerical-application">numerical applications</a> to recent <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/big-data-analysis">big data analysis</a> and <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/machine-learning">machine learning</a>. Although many SpGEMM algorithms have been proposed, hardware specific optimizations for multi- and many-core processors are lacking and a detailed analysis of their performance under various use cases and matrices is not available. We firstly identify and mitigate multiple bottlenecks with <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/memory-management">memory management</a> and thread scheduling on <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/intel-xeon-phi">Intel Xeon Phi</a> (Knights Landing or KNL). <strong>Specifically targeting many-core processors</strong>, we develop a hash-table-based algorithm and optimize a heap-based shared-memory SpGEMM algorithm. We examine their performance together with other publicly available codes. Different from the literature, our evaluation also includes use cases that are representative of real <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/graph-algorithms">graph algorithms</a>, such as multi-source breadth-first search or triangle counting. <strong>Our hash-table and heap-based algorithms are showing significant speedups from libraries in the majority of the cases</strong> while different algorithms dominate the other scenarios with different matrix size, <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/sparsity">sparsity</a>, compression factor and operation type. We wrap up in-depth evaluation results and make a recipe to give the best SpGEMM algorithm for target scenario. We build the performance model for hash-table and heap-based algorithms, which supports the recipe. A critical finding is that hash-table-based SpGEMM gets a significant performance boost if the nonzeros are not required to be sorted within each row of the output matrix. Finally, we integrate our implementations into a large-scale protein clustering code named HipMCL, accelerating its SpGEMM kernel by up to 10X and achieving an overall performance boost for the whole HipMCL application by 2.6X.</p>
<p>稀疏矩阵-矩阵乘法 (SpGEMM) 是一种计算原语（computational primitive），广泛应用于从传统<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/engineering/numeric-application">数值应用</a> 到最近的<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/big-data-analysis">大数据分析</a>和<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/machine-learning">机器学习</a>.尽管已经提出了许多 SpGEMM 算法，但缺乏针对多核和众核处理器的硬件特定优化，并且没有对其在各种测试样例下的性能进行详细分析。我们首先通过<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/memory-management">内存管理</a>和<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/intel-xeon-phi">Intel Xeon Phi</a>上的线程调度来识别和缓解多个瓶颈（Knights Landing 或 KNL）。<strong>我们专门针对众核处理器</strong>，开发了基于哈希表的算法并优化了基于堆的共享内存 SpGEMM 算法。我们与其他公开的代码一起检查其性能。与文献不同，我们的评估还包括代表真实<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/graph-algorithms">图算法</a>的用例，例如多源广度优先搜索或三角形计数。在大多数库中，我们的哈希表和基于堆的算法在可以显著加速，而不同的算法主导了具有不同矩阵大小，<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer -science/sparsity">稀疏程度</a>、压缩因子和操作类型的其他场景（？）。我们总结了深入的评估结果并制定了一个方法，为目标场景提供最佳 SpGEMM 算法。我们为支持这个方法的哈希表和基于堆的算法构建了性能模型。一个重要的发现是，如果不需要在输出矩阵的每一行内对非零值进行排序，那么基于哈希表的 SpGEMM 将获得显着的性能提升。最后，我们将我们的实现集成到一个名为 HipMCL 的大规模蛋白质聚类代码中，将其 SpGEMM 内核的速度提高了 10 倍，并将整个 HipMCL 应用程序的整体性能提高了 2.6 倍。</p>
<h2 id="Keywords-2"><a href="#Keywords-2" class="headerlink" title="Keywords"></a>Keywords</h2><p>Sparse matrix</p>
<p>SpGEMM</p>
<p>Intel KNL</p>
<h2 id="Comments-67"><a href="#Comments-67" class="headerlink" title="Comments"></a>Comments</h2><p>需要好好看，因为这篇文章的用词相当高级</p>
<p>简而言之，multi-core是多核，many-core是众核。</p>
<p><img src="/2021/10/29/papers/1.png" alt="cores"></p>
<hr>
<h1 id="Paper4-1106"><a href="#Paper4-1106" class="headerlink" title="Paper4-1106"></a>Paper4-1106</h1><h2 id="Published-in-60"><a href="#Published-in-60" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Ahmed M. Abdelsalam, J. M. Pierre Langlois, F. Cheriet, “A Configurable FPGA Implementation of the Tanh Function Using DCT Interpolation”, <em>Field-Programmable Custom Computing Machines (FCCM) 2017 IEEE 25th Annual International Symposium on</em>, pp. 168-171, 2017.</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/conhome/7964000/proceeding">2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</a></p>
<h2 id="Title-61"><a href="#Title-61" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7966673">A Configurable FPGA Implementation of the Tanh Function Using DCT Interpolation</a></p>
<h2 id="Authors-56"><a href="#Authors-56" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086113448">Ahmed M. Abdelsalam</a></p>
<p>Computer and Software Engineering Department, Polytechnique Montreal, Montreal, Canada</p>
<p>加拿大蒙特利尔理工学院计算机与软件工程系</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37276914800">J. M. Pierre Langlois</a></p>
<p>Computer and Software Engineering Department, Polytechnique Montreal, Montreal, Canada</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37284363700">F. Cheriet</a></p>
<p>Computer and Software Engineering Department, Polytechnique Montreal, Montreal, Canada</p>
<h2 id="Abstract-66"><a href="#Abstract-66" class="headerlink" title="Abstract"></a>Abstract</h2><p>Efficient implementation of non-linear activation functions is essential to the implementation of deep learning models on FPGAs. We introduce such an implementation based on the Discrete Cosine Transform Interpolation Filter (DCTIF). The proposed interpolation architecture combines simple arithmetic operations on the stored samples of the hyperbolic tangent function and on input data. It achieves almost 3× better precision than previous works while using a similar amount computational resources and a small amount of memory. Various combinations of DCTIF parameters can be chosen to trade off the accuracy and the overall circuit complexity of the tanh function. In one case, the proposed architecture approximates the hyperbolic tangent activation function with 0.004 maximum error while requiring only 1.45 kbits BRAM memory and 21 LUTs of a Virtex-7 FPGA.</p>
<p>非线性激活函数的有效实现对于在 FPGA 上实现深度学习模型至关重要。 我们介绍了基于<strong>离散余弦变换插值滤波器 (DCTIF) 的这种实现</strong>。 所提出的插值架构结合了对双曲正切函数的存储样本和输入数据的简单算术运算。 在使用类似数量的计算资源和少量内存的情况下，它的精度比以前的工作高出近 3 倍。 可以选择 DCTIF 参数的各种组合来权衡 tanh 函数的准确性和整体电路复杂性。 在一种情况下，所提出的架构以 0.004 的最大误差逼近双曲正切激活函数，同时仅需要 1.45 kbits BRAM 存储器和 Virtex-7 FPGA 的 21 个 LUT。</p>
<h2 id="Keywords-3"><a href="#Keywords-3" class="headerlink" title="Keywords"></a>Keywords</h2><ul>
<li>IEEE Keywords <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Interpolation&amp;newsearch=true">Interpolation</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Field programmable gate arrays&amp;newsearch=true">Field programmable gate arrays</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Table lookup&amp;newsearch=true">Table lookup</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Discrete cosine transforms&amp;newsearch=true">Discrete cosine transforms</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Decoding&amp;newsearch=true">Decoding</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Hardware&amp;newsearch=true">Hardware</a></li>
<li>INSPEC: Controlled Indexing <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:circuit complexity&amp;newsearch=true">circuit complexity</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:discrete cosine transforms&amp;newsearch=true">discrete cosine transforms</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:field programmable gate arrays&amp;newsearch=true">field programmable gate arrays</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:interpolation&amp;newsearch=true">interpolation</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:learning .LB.artificial intelligence.RB.&amp;newsearch=true">learning (artificial intelligence)</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:transfer functions&amp;newsearch=true">transfer functions</a></li>
<li>INSPEC: Non-Controlled Indexing <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:configurable FPGA implementation&amp;newsearch=true">configurable FPGA implementation</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:tanh function&amp;newsearch=true">tanh function</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:nonlinear activation functions&amp;newsearch=true">nonlinear activation functions</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:deep learning models&amp;newsearch=true">deep learning models</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:discrete cosine transform interpolation filter&amp;newsearch=true">discrete cosine transform interpolation filter</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:DCTIF&amp;newsearch=true">DCTIF</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:circuit complexity&amp;newsearch=true">circuit complexity</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:BRAM memory&amp;newsearch=true">BRAM memory</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:LUT&amp;newsearch=true">LUT</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Virtex-7 FPGA&amp;newsearch=true">Virtex-7 FPGA</a></li>
<li>Author Keywords <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Deep Neural Network .LB.DNN.RB.&amp;newsearch=true">Deep Neural Network (DNN)</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Embedded FPGA&amp;newsearch=true">Embedded FPGA</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Deep learning&amp;newsearch=true">Deep learning</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Activation function&amp;newsearch=true">Activation function</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Hyperbolic tangent&amp;newsearch=true">Hyperbolic tangent</a></li>
</ul>
<h2 id="Comments-68"><a href="#Comments-68" class="headerlink" title="Comments"></a>Comments</h2><p>方法不同。</p>
<p>从Abstract可以看出来作者好像也很难权衡参数到底怎么配。和我挺像的。。。</p>
<hr>
<h1 id="Paper3-1103"><a href="#Paper3-1103" class="headerlink" title="Paper3-1103"></a>Paper3-1103</h1><h2 id="Published-in-61"><a href="#Published-in-61" class="headerlink" title="Published in"></a>Published in</h2><p>TCAS-I</p>
<p>IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 68, NO. 8, AUGUST 2021</p>
<h2 id="Title-62"><a href="#Title-62" class="headerlink" title="Title"></a>Title</h2><p>Low-Complexity High-Precision Method and Architecture for Computing the Logarithm of Complex Numbers</p>
<h2 id="Authors-57"><a href="#Authors-57" class="headerlink" title="Authors"></a>Authors</h2><p>Hui Chen , Graduate Student Member, IEEE, Zongguang Yu , Zhonghai Lu , Senior Member, IEEE, Yuxiang Fu , Member, IEEE, and Li Li , Member, IEEE</p>
<p>Hui Chen, Zongguang Yu, Yonggang Zhang, Yuxiang Fu, and Li Li are with the School of Electronic Science and Engineering, Nanjing University, Nanjing 210093, China (e-mail: yuxiangfu@nju.edu.cn; lili@nju.edu.cn).</p>
<p>Zhonghai Lu is with the Department of Electrical Engineering, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, 16440 Stockholm, Sweden.</p>
<h2 id="Abstract-67"><a href="#Abstract-67" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper proposes a low-complexity method and architecture to compute the logarithm of complex numbers based on coordinate rotation digital computer (CORDIC). Our method takes advantage of the vector mode of circular CORDIC and hyperbolic CORDIC, which only needs shift-add operations in its hardware implementation. Our architecture has lower design complexity and higher performance compared with conventional architectures. Through software simulation, we show that this method can achieve high precision for logarithm computation, reaching the relative error of  $10^{−7}$. Finally, we design and implementation example circuit under TSMC 28nm CMOS technology. According to the synthesis report, our architecture has smaller area, lower power consumption, higher precision and wider operation range compared with the alternative architectures.</p>
<p>本文提出了一种基于CORDIC方法计算复数对数的低复杂度方法和架构。我们的方法利用了循环 CORDIC 和双曲线 CORDIC 的矢量模式的长处，在硬件实现中只需进行移位和加法运算。与传统架构相比，我们的架构具有更低的设计复杂性和更高的性能。通过软件仿真，我们表明该方法可以实现对数计算的高精度，达到$10^{−7}$的<strong>相对误差</strong>。 最后，我们在TSMC 28nm CMOS技术下设计并实现了一个示例电路。根据综合报告，我们的架构与替代架构相比具有更小的面积、更低的功耗、更高的精度和更宽的操作范围。</p>
<h2 id="Comments-69"><a href="#Comments-69" class="headerlink" title="Comments"></a>Comments</h2><p>他们的相对误差非常低，但是最大绝对误差是多少？</p>
<hr>
<h1 id="Paper2-1102"><a href="#Paper2-1102" class="headerlink" title="Paper2-1102"></a>Paper2-1102</h1><h2 id="Published-in-62"><a href="#Published-in-62" class="headerlink" title="Published in"></a>Published in</h2><p>TCAS-I</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8919">IEEE Transactions on Circuits and Systems I: Regular Papers</a> ( Early Access )</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9585319">Paper LINK</a></p>
<h2 id="Title-63"><a href="#Title-63" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9585319">Memory-Efficient CNN Accelerator Based on Interlayer Feature Map Compression</a></p>
<h2 id="Authors-58"><a href="#Authors-58" class="headerlink" title="Authors"></a>Authors</h2><p>Zhuang Shao; Xiaoliang Chen; Li Du; Lei Chen; Yuan Du; Wei Zhuang; Huadong Wei; Chenjia Xie; Zhongfeng Wang</p>
<h2 id="Abstract-68"><a href="#Abstract-68" class="headerlink" title="Abstract"></a>Abstract</h2><p>Existing deep convolutional neural networks (CNNs) generate massive interlayer feature data during network inference. To maintain real-time processing in embedded systems, large on-chip memory is required to buffer the interlayer feature maps. In this paper, we propose an efficient hardware accelerator with an interlayer feature compression technique to significantly reduce the required on-chip memory size and off-chip memory access bandwidth. The accelerator compresses interlayer feature maps through transforming the stored data into frequency domain using hardware-implemented 8×8 discrete cosine transform (DCT). The high-frequency components are removed after the DCT through quantization. Sparse matrix compression is utilized to further compress the interlayer feature maps. The on-chip memory allocation scheme is designed to support dynamic configuration of the feature map buffer size and scratch pad size according to different network-layer requirements. The hardware accelerator combines compression, decompression, and CNN acceleration into one computing stream, achieving minimal compressing and processing delay. A prototype accelerator is implemented on an FPGA platform and also synthesized in TSMC 28-nm COMS technology. It achieves 403GOPS peak throughput and 1.4× 3.3× interlayer feature map reduction by adding light hardware area overhead, making it a promising hardware accelerator for intelligent IoT devices.</p>
<p>现有的深度卷积神经网络 (CNN) 在网络推理过程中会生成大量的层间特征数据。为了在嵌入式系统中保持实时处理，需要大量的片上存储器来缓冲层间特征图。在本文中，我们提出了一种具有层间特征压缩技术的高效硬件加速器，以显着减少所需的片上内存大小和片外内存访问带宽。加速器通过使用硬件实现的 8×8 离散余弦变换 (DCT) 将存储的数据转换到频域来压缩层间特征图。高频分量在 DCT 之后通过量化被去除。稀疏矩阵压缩用于进一步压缩层间特征图。片上内存分配方案旨在支持根据不同网络层要求动态配置特征图buffer大小和scratch pad大小。硬件加速器将压缩、解压和CNN加速整合到一个计算流中，实现最小的压缩和处理延迟。原型加速器在 FPGA 平台上实现，并在 TSMC 28-nm COMS 技术中综合。它通过增加很少的硬件区域开销实现了 403GOPS 峰值吞吐量和 1.4×3.3× 层间特征图减少，使其成为智能物联网设备的有前途的硬件加速器。</p>
<h2 id="Index-Terms"><a href="#Index-Terms" class="headerlink" title="Index Terms"></a>Index Terms</h2><p>Deep convolution neural networks, discrete cosine transform, quantization, interlayer feature maps compression.</p>
<p>深度卷积神经网络、离散余弦变换、量化、层间特征图压缩。</p>
<h2 id="Comments-70"><a href="#Comments-70" class="headerlink" title="Comments"></a>Comments</h2><p>iscl的第一篇TCAS-I。</p>
<hr>
<h1 id="Paper1-1101"><a href="#Paper1-1101" class="headerlink" title="Paper1-1101"></a>Paper1-1101</h1><h2 id="Published-in-63"><a href="#Published-in-63" class="headerlink" title="Published in"></a>Published in</h2><p>TVLSI。</p>
<p>IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS, VOL. 29, NO. 7, JULY 2021</p>
<h2 id="Title-64"><a href="#Title-64" class="headerlink" title="Title"></a>Title</h2><p>PWL-Based Architecture for the Logarithmic Computation of Floating-Point Numbers</p>
<h2 id="Authors-59"><a href="#Authors-59" class="headerlink" title="Authors"></a>Authors</h2><p>Fei Lyu , Zhelong Mao , Jin Zhang, Yu Wang, and Yuanyong Luo</p>
<p>Fei Lyu and Jin Zhang are with the School of Electronics and Information Engineering, Jinling Institute of Technology, Nanjing 211169, China.</p>
<p>Zhelong Mao and Yu Wang are with the School of Electronics Engineering, Nanjing Xiaozhuang University, Nanjing 211171, China.</p>
<p>Yuanyong Luo is with the Linx Laboratory, <strong>Department of Turing Architecture Design, HiSilicon, Huawei Corporation, </strong>Shenzhen 518129, China.</p>
<h2 id="Abstract-69"><a href="#Abstract-69" class="headerlink" title="Abstract"></a>Abstract</h2><p>In this brief, we propose a logarithmic converter for floating-point numbers based on the piecewise linear (PWL) approximation method. The proposed method is applicable to any customized floating-point format with a mantissa length of 16–23 bits and a maximum absolute error (MAE) larger than $10^{−6}$. The logarithmic function is automatically segmented into several maximal subsections by a software-based segmentation scheme with the restriction of a predefined MAE and a fractional word length for the computing units. Then, we make a tradeoff between the piecewise number and the fractional word length. Based on the results of the segmentor, our design is coded in the Verilog hardware description language. The synthesized results show that our design consumes less area, time, and power without compromising accuracy compared to existing techniques based on the COordinate Rotation Digital Computer (CORDIC) and PWL methods.</p>
<p>在这篇brief中，我们提出了一种基于分段线性 (PWL) 拟合方法的浮点对数转换器。 所提出的方法适用于尾数长度为 16-23 位且最大绝对误差 (MAE) 大于 10-6 的任何自定义浮点格式。 目标拟合对数函数通过软件分段器根据预定义的MAE和小数位数限制自动分成多段。 而后我们在分段数和小数字长之间进行权衡。 基于分段器的输出，我们用Verilog完成了硬件实现。 综合结果表明，与基于CORDIC方法和和 PWL 方法的现有技术相比，我们的设计在不影响精度的情况下具有更低的面积、时延和功耗。</p>
<h2 id="Index-Terms-1"><a href="#Index-Terms-1" class="headerlink" title="Index Terms"></a>Index Terms</h2><p>Logarithmic converter, maximum absolute error (MAE), piecewise linear (PWL) approximation.</p>
<p>对数转换器、最大绝对误差 (MAE)、分段线性 (PWL) 拟合。</p>
<h2 id="Comments-71"><a href="#Comments-71" class="headerlink" title="Comments"></a>Comments</h2><p>5页短文，可以用于学习如何写短paper。</p>
<hr>
<h1 id="PaperT-F"><a href="#PaperT-F" class="headerlink" title="PaperT-F"></a>PaperT-F</h1><h2 id="Published-in-64"><a href="#Published-in-64" class="headerlink" title="Published in"></a>Published in</h2><h2 id="Title-65"><a href="#Title-65" class="headerlink" title="Title"></a>Title</h2><h2 id="Authors-60"><a href="#Authors-60" class="headerlink" title="Authors"></a>Authors</h2><p><sup>1,2</sup> <sup>1</sup> <sup>2</sup> <sup>3</sup></p>
<h2 id="Abstract-70"><a href="#Abstract-70" class="headerlink" title="Abstract"></a>Abstract</h2><h2 id="Keywords-Index-Terms-42"><a href="#Keywords-Index-Terms-42" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-72"><a href="#Comments-72" class="headerlink" title="Comments"></a>Comments</h2><h2 id="References-49"><a href="#References-49" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="阅读方法"><a href="#阅读方法" class="headerlink" title="阅读方法"></a>阅读方法</h1><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/23924014">知乎问题：如何有针对地高效地阅读一篇学术论文？</a></p>
<h2 id="回答要点总结"><a href="#回答要点总结" class="headerlink" title="回答要点总结"></a>回答要点总结</h2><p>目标是提交完全看不懂的新领域论文。</p>
<hr>
<p>不要追求完美，没必要逐行读懂，关键是数量要上去，量变引起质变，而不是纠结于一句两句的意思。</p>
<p>带着问题读论文，目的是找到答案为止。先有个大概的、粗略的问题，随着阅读的深入，慢慢提出更细致的问题，进行更深入的阅读。<strong>一定是一整批一起读懂到某个层次，而不是逐篇逐篇地整篇一次读懂。</strong>这篇博客就是保证一批论文全部至少读到abstract看懂的层次。</p>
<p><strong>不要读不会用到的东西，白费的力气必须被极小化！其实，绝大部分论文都只需要了解它的主要观念</strong>（这往往比较容易），<strong>而不需要了解它的详细推导过程</strong>（这反而比较费时）。</p>
<p><strong>整批读略过一次之后，就可以规划出一个你以为比较容易懂的阅读次序。想读懂A论文，不一定非得读A论文，或许阅读引用A论文的B论文，可以看看其他人的理解</strong>。</p>
<blockquote>
<p><strong>我读论文远比学生快，分析远比学生深入，主要的是我敢想象与猜测，而且多年训练下来想象与猜测的准确度很高。所以，许多论文我根本不是「读懂」的，而是「猜对」了！</strong>猜错了就猜错了，继续读论文可以容易的知道对错。</p>
</blockquote>
<hr>
<p>批判性地阅读（下面是我自己的大略翻译，原文见<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/23924014/answer/26470331">如何有针对地高效地阅读一篇学术论文？ - Clei的回答 - 知乎 </a>）</p>
<p>分析和评估，而不是简单的总结。看完论文后，不可以只知道“作者说了什么？“，而是要回答”作者如何实现的？“”作者为什么要这样做？”“效果好不好？”，并不一定非得批评，但是一定要有批判的态度。</p>
<p>【下面这些问题比较烦，其实就是What Why How】</p>
<p>问自己：</p>
<ul>
<li>作者的主要观点是？</li>
<li>作者的意图是？</li>
<li>作者的目标读者是？</li>
<li>作者用了哪些论据去证明自己的观点？</li>
<li>作者用了哪些实验去证明了自己的论据？</li>
<li>作者的基本假设是？</li>
<li>作者有什么偏见？</li>
</ul>
<p>以上问题的回答需要进行记录。</p>
<p>他还给了很多其他问题，但是个人觉得没有必要看。</p>
<hr>
<p>下面是H.B.Zhou老师课堂上的一些观点：</p>
<ul>
<li>问题是什么，写的什么，主题什么；问题的难点在哪里，必须做的点在哪里；怎么去做的，效果如何。为什么要做，创新点在哪里，打算怎么做，怎么去做的，挑战在哪里。</li>
<li>Scenario：把应用场景画出来，画出来就是：为什么要这么做。</li>
<li>Motivation：动机是什么：难，有挑战，别人没做过；我们有创新；我们有新的解决方案。</li>
<li>Solutions：如何去解决。粗略或者详细的solutions。</li>
<li>任何事情都要有<strong>What Why How</strong>的闭环。讨论问题，应该事事有回响，事事有反馈，要有ACK，总结和跟进。</li>
</ul>
<h2 id="References-50"><a href="#References-50" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://wenku.baidu.com/view/bb3dfb7f31b765ce05081437.html">彭明辉，硕士班研究所新生手册</a></li>
<li>Rosen, Leonard J. and Laurence Behrens, eds. The Allyn &amp; Bacon Handbook. 1994.</li>
<li>栾浩 樊凯 项阳，《科研有方— 做科研和写论文的一些经验》</li>
</ol>
<hr>
<p><img src="/2021/10/29/papers/qiufen.jpg" alt="QiuFen"></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Haoran Geng
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://njughr.github.io/2021/10/29/papers/" title="Pipes">https://njughr.github.io/2021/10/29/papers/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ic/" rel="tag"># ic</a>
              <a href="/tags/cs/" rel="tag"># cs</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/10/26/hdl/" rel="prev" title="HDL">
      <i class="fa fa-chevron-left"></i> HDL
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/10/31/english/" rel="next" title="ENGLISH">
      ENGLISH <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#PaperT"><span class="nav-number">1.</span> <span class="nav-text">PaperT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cite"><span class="nav-number">1.1.</span> <span class="nav-text">Cite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms"><span class="nav-number">1.3.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments"><span class="nav-number">1.4.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaperLink"><span class="nav-number">1.5.</span> <span class="nav-text">PaperLink</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper92-0201"><span class="nav-number">2.</span> <span class="nav-text">Paper92-0201</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cite-1"><span class="nav-number">2.1.</span> <span class="nav-text">Cite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-1"><span class="nav-number">2.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-1"><span class="nav-number">2.3.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-1"><span class="nav-number">2.4.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaperLink-1"><span class="nav-number">2.5.</span> <span class="nav-text">PaperLink</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper91-0131"><span class="nav-number">3.</span> <span class="nav-text">Paper91-0131</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cite-2"><span class="nav-number">3.1.</span> <span class="nav-text">Cite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-2"><span class="nav-number">3.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-2"><span class="nav-number">3.3.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-2"><span class="nav-number">3.4.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaperLink-2"><span class="nav-number">3.5.</span> <span class="nav-text">PaperLink</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper84-0119"><span class="nav-number">4.</span> <span class="nav-text">Paper84-0119</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cite-3"><span class="nav-number">4.1.</span> <span class="nav-text">Cite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-3"><span class="nav-number">4.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-3"><span class="nav-number">4.3.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaperLink-3"><span class="nav-number">4.4.</span> <span class="nav-text">PaperLink</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper83-0120"><span class="nav-number">5.</span> <span class="nav-text">Paper83-0120</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cite-4"><span class="nav-number">5.1.</span> <span class="nav-text">Cite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-4"><span class="nav-number">5.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-4"><span class="nav-number">5.3.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaperLink-4"><span class="nav-number">5.4.</span> <span class="nav-text">PaperLink</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper82-0121"><span class="nav-number">6.</span> <span class="nav-text">Paper82-0121</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cite-5"><span class="nav-number">6.1.</span> <span class="nav-text">Cite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-5"><span class="nav-number">6.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Highlights"><span class="nav-number">6.3.</span> <span class="nav-text">Highlights</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-5"><span class="nav-number">6.4.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaperLink-5"><span class="nav-number">6.5.</span> <span class="nav-text">PaperLink</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper81-0122"><span class="nav-number">7.</span> <span class="nav-text">Paper81-0122</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cite-6"><span class="nav-number">7.1.</span> <span class="nav-text">Cite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-6"><span class="nav-number">7.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-6"><span class="nav-number">7.3.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaperLink-6"><span class="nav-number">7.4.</span> <span class="nav-text">PaperLink</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper80-0123"><span class="nav-number">8.</span> <span class="nav-text">Paper80-0123</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cite-7"><span class="nav-number">8.1.</span> <span class="nav-text">Cite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-7"><span class="nav-number">8.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-7"><span class="nav-number">8.3.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaperLink-7"><span class="nav-number">8.4.</span> <span class="nav-text">PaperLink</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper79-0124"><span class="nav-number">9.</span> <span class="nav-text">Paper79-0124</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cite-8"><span class="nav-number">9.1.</span> <span class="nav-text">Cite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-8"><span class="nav-number">9.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-8"><span class="nav-number">9.3.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaperLink-8"><span class="nav-number">9.4.</span> <span class="nav-text">PaperLink</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper78-0118"><span class="nav-number">10.</span> <span class="nav-text">Paper78-0118</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in"><span class="nav-number">10.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title"><span class="nav-number">10.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors"><span class="nav-number">10.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-9"><span class="nav-number">10.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-9"><span class="nav-number">10.5.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">10.6.</span> <span class="nav-text">References</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-1"><span class="nav-number">10.7.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-1"><span class="nav-number">10.8.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-1"><span class="nav-number">10.9.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-10"><span class="nav-number">10.10.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-10"><span class="nav-number">10.11.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-1"><span class="nav-number">10.12.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper76-0116"><span class="nav-number">11.</span> <span class="nav-text">Paper76-0116</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-2"><span class="nav-number">11.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-2"><span class="nav-number">11.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-11"><span class="nav-number">11.3.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper75-0115"><span class="nav-number">12.</span> <span class="nav-text">Paper75-0115</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-3"><span class="nav-number">12.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-3"><span class="nav-number">12.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-12"><span class="nav-number">12.3.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper74-0114"><span class="nav-number">13.</span> <span class="nav-text">Paper74-0114</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-4"><span class="nav-number">13.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-4"><span class="nav-number">13.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-2"><span class="nav-number">13.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-11"><span class="nav-number">13.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-13"><span class="nav-number">13.5.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-2"><span class="nav-number">13.6.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper73-0113"><span class="nav-number">14.</span> <span class="nav-text">Paper73-0113</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-5"><span class="nav-number">14.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-5"><span class="nav-number">14.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-3"><span class="nav-number">14.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-12"><span class="nav-number">14.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-14"><span class="nav-number">14.5.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-3"><span class="nav-number">14.6.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper72-0112"><span class="nav-number">15.</span> <span class="nav-text">Paper72-0112</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-6"><span class="nav-number">15.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-6"><span class="nav-number">15.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-4"><span class="nav-number">15.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-13"><span class="nav-number">15.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-15"><span class="nav-number">15.5.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper71-0106"><span class="nav-number">16.</span> <span class="nav-text">Paper71-0106</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-7"><span class="nav-number">16.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-7"><span class="nav-number">16.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-5"><span class="nav-number">16.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-14"><span class="nav-number">16.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-16"><span class="nav-number">16.5.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-4"><span class="nav-number">16.6.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper70-0105"><span class="nav-number">17.</span> <span class="nav-text">Paper70-0105</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-8"><span class="nav-number">17.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-8"><span class="nav-number">17.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-6"><span class="nav-number">17.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-15"><span class="nav-number">17.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-3"><span class="nav-number">17.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-17"><span class="nav-number">17.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-5"><span class="nav-number">17.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper69-0104"><span class="nav-number">18.</span> <span class="nav-text">Paper69-0104</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-9"><span class="nav-number">18.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-9"><span class="nav-number">18.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-7"><span class="nav-number">18.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-16"><span class="nav-number">18.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-18"><span class="nav-number">18.5.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-6"><span class="nav-number">18.6.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper68-0103"><span class="nav-number">19.</span> <span class="nav-text">Paper68-0103</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-10"><span class="nav-number">19.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-10"><span class="nav-number">19.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-8"><span class="nav-number">19.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-17"><span class="nav-number">19.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-19"><span class="nav-number">19.5.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-7"><span class="nav-number">19.6.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper67-0102"><span class="nav-number">20.</span> <span class="nav-text">Paper67-0102</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-11"><span class="nav-number">20.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-11"><span class="nav-number">20.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-9"><span class="nav-number">20.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-18"><span class="nav-number">20.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-4"><span class="nav-number">20.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-20"><span class="nav-number">20.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-8"><span class="nav-number">20.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper66-0101"><span class="nav-number">21.</span> <span class="nav-text">Paper66-0101</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-12"><span class="nav-number">21.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-12"><span class="nav-number">21.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-10"><span class="nav-number">21.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-19"><span class="nav-number">21.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-5"><span class="nav-number">21.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-21"><span class="nav-number">21.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-9"><span class="nav-number">21.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper65-1231"><span class="nav-number">22.</span> <span class="nav-text">Paper65-1231</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-13"><span class="nav-number">22.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-13"><span class="nav-number">22.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-11"><span class="nav-number">22.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-20"><span class="nav-number">22.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-6"><span class="nav-number">22.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-22"><span class="nav-number">22.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-10"><span class="nav-number">22.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper64-1230"><span class="nav-number">23.</span> <span class="nav-text">Paper64-1230</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-14"><span class="nav-number">23.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-14"><span class="nav-number">23.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-12"><span class="nav-number">23.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-21"><span class="nav-number">23.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-23"><span class="nav-number">23.5.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-11"><span class="nav-number">23.6.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper63-1229"><span class="nav-number">24.</span> <span class="nav-text">Paper63-1229</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-15"><span class="nav-number">24.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-15"><span class="nav-number">24.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-13"><span class="nav-number">24.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-22"><span class="nav-number">24.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">24.5.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-24"><span class="nav-number">24.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper62-1227"><span class="nav-number">25.</span> <span class="nav-text">Paper62-1227</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-16"><span class="nav-number">25.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-16"><span class="nav-number">25.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-14"><span class="nav-number">25.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-23"><span class="nav-number">25.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-12"><span class="nav-number">25.5.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper61-1219"><span class="nav-number">26.</span> <span class="nav-text">Paper61-1219</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-17"><span class="nav-number">26.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-17"><span class="nav-number">26.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-15"><span class="nav-number">26.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-24"><span class="nav-number">26.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-13"><span class="nav-number">26.5.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper60-1226"><span class="nav-number">27.</span> <span class="nav-text">Paper60-1226</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-18"><span class="nav-number">27.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-18"><span class="nav-number">27.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-16"><span class="nav-number">27.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-25"><span class="nav-number">27.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-25"><span class="nav-number">27.5.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-14"><span class="nav-number">27.6.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper57-1223"><span class="nav-number">28.</span> <span class="nav-text">Paper57-1223</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-19"><span class="nav-number">28.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-19"><span class="nav-number">28.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-17"><span class="nav-number">28.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-26"><span class="nav-number">28.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-7"><span class="nav-number">28.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-26"><span class="nav-number">28.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-15"><span class="nav-number">28.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper53-1228"><span class="nav-number">29.</span> <span class="nav-text">Paper53-1228</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-20"><span class="nav-number">29.1.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-27"><span class="nav-number">29.2.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper52-0108"><span class="nav-number">30.</span> <span class="nav-text">Paper52-0108</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-20"><span class="nav-number">30.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-21"><span class="nav-number">30.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-18"><span class="nav-number">30.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-28"><span class="nav-number">30.4.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-16"><span class="nav-number">30.5.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper51-0109"><span class="nav-number">31.</span> <span class="nav-text">Paper51-0109</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-21"><span class="nav-number">31.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-22"><span class="nav-number">31.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-19"><span class="nav-number">31.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-27"><span class="nav-number">31.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-8"><span class="nav-number">31.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-29"><span class="nav-number">31.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-17"><span class="nav-number">31.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper50-0110"><span class="nav-number">32.</span> <span class="nav-text">Paper50-0110</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-22"><span class="nav-number">32.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-23"><span class="nav-number">32.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-20"><span class="nav-number">32.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-28"><span class="nav-number">32.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-9"><span class="nav-number">32.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-30"><span class="nav-number">32.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper49-1218"><span class="nav-number">33.</span> <span class="nav-text">Paper49-1218</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-23"><span class="nav-number">33.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-24"><span class="nav-number">33.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-21"><span class="nav-number">33.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-29"><span class="nav-number">33.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-10"><span class="nav-number">33.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-31"><span class="nav-number">33.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-18"><span class="nav-number">33.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper47-1216"><span class="nav-number">34.</span> <span class="nav-text">Paper47-1216</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-24"><span class="nav-number">34.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-25"><span class="nav-number">34.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-22"><span class="nav-number">34.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-30"><span class="nav-number">34.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-11"><span class="nav-number">34.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-32"><span class="nav-number">34.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-19"><span class="nav-number">34.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper46-1214"><span class="nav-number">35.</span> <span class="nav-text">Paper46-1214</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-25"><span class="nav-number">35.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-26"><span class="nav-number">35.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-23"><span class="nav-number">35.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-31"><span class="nav-number">35.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-12"><span class="nav-number">35.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-33"><span class="nav-number">35.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-20"><span class="nav-number">35.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Slides01"><span class="nav-number">36.</span> <span class="nav-text">Slides01</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-26"><span class="nav-number">36.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-27"><span class="nav-number">36.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-24"><span class="nav-number">36.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-32"><span class="nav-number">36.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-13"><span class="nav-number">36.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-34"><span class="nav-number">36.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-21"><span class="nav-number">36.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper45-1213"><span class="nav-number">37.</span> <span class="nav-text">Paper45-1213</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-27"><span class="nav-number">37.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-28"><span class="nav-number">37.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-25"><span class="nav-number">37.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-33"><span class="nav-number">37.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-14"><span class="nav-number">37.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-35"><span class="nav-number">37.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-22"><span class="nav-number">37.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper44-0107"><span class="nav-number">38.</span> <span class="nav-text">Paper44-0107</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-28"><span class="nav-number">38.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-29"><span class="nav-number">38.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-26"><span class="nav-number">38.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-34"><span class="nav-number">38.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-36"><span class="nav-number">38.5.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-23"><span class="nav-number">38.6.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper43-1211"><span class="nav-number">39.</span> <span class="nav-text">Paper43-1211</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-29"><span class="nav-number">39.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-30"><span class="nav-number">39.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Author-information"><span class="nav-number">39.3.</span> <span class="nav-text">Author information</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Affiliations"><span class="nav-number">39.3.1.</span> <span class="nav-text">Affiliations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Contributions"><span class="nav-number">39.3.2.</span> <span class="nav-text">Contributions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Corresponding-author"><span class="nav-number">39.3.3.</span> <span class="nav-text">Corresponding author</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-35"><span class="nav-number">39.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-15"><span class="nav-number">39.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-37"><span class="nav-number">39.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-24"><span class="nav-number">39.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper42-1212"><span class="nav-number">40.</span> <span class="nav-text">Paper42-1212</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-30"><span class="nav-number">40.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-31"><span class="nav-number">40.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-27"><span class="nav-number">40.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-36"><span class="nav-number">40.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-16"><span class="nav-number">40.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-38"><span class="nav-number">40.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-25"><span class="nav-number">40.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper41-0111"><span class="nav-number">41.</span> <span class="nav-text">Paper41-0111</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-31"><span class="nav-number">41.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-32"><span class="nav-number">41.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-37"><span class="nav-number">41.3.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-39"><span class="nav-number">41.4.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper39-1209"><span class="nav-number">42.</span> <span class="nav-text">Paper39-1209</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-32"><span class="nav-number">42.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-33"><span class="nav-number">42.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-28"><span class="nav-number">42.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-38"><span class="nav-number">42.4.</span> <span class="nav-text">Abstract</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-circuitry-for-synthetic-biology"><span class="nav-number">42.4.1.</span> <span class="nav-text">Programming circuitry for synthetic biology</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Structured-Abstract"><span class="nav-number">42.4.2.</span> <span class="nav-text">Structured Abstract</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#INTRODUCTION"><span class="nav-number">42.4.2.1.</span> <span class="nav-text">INTRODUCTION</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RATIONALE"><span class="nav-number">42.4.2.2.</span> <span class="nav-text">RATIONALE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RESULTS"><span class="nav-number">42.4.2.3.</span> <span class="nav-text">RESULTS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Conclusion"><span class="nav-number">42.4.2.4.</span> <span class="nav-text">Conclusion</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract-39"><span class="nav-number">42.4.3.</span> <span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-17"><span class="nav-number">42.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-40"><span class="nav-number">42.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-26"><span class="nav-number">42.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper37-1207"><span class="nav-number">43.</span> <span class="nav-text">Paper37-1207</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-33"><span class="nav-number">43.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-34"><span class="nav-number">43.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-29"><span class="nav-number">43.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">43.4.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-18"><span class="nav-number">43.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-41"><span class="nav-number">43.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-27"><span class="nav-number">43.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper36-1206"><span class="nav-number">44.</span> <span class="nav-text">Paper36-1206</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-34"><span class="nav-number">44.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-35"><span class="nav-number">44.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-30"><span class="nav-number">44.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-40"><span class="nav-number">44.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-19"><span class="nav-number">44.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-42"><span class="nav-number">44.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-28"><span class="nav-number">44.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper35-1205"><span class="nav-number">45.</span> <span class="nav-text">Paper35-1205</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-35"><span class="nav-number">45.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-36"><span class="nav-number">45.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-31"><span class="nav-number">45.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-41"><span class="nav-number">45.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-20"><span class="nav-number">45.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-43"><span class="nav-number">45.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-29"><span class="nav-number">45.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper34-1204"><span class="nav-number">46.</span> <span class="nav-text">Paper34-1204</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-36"><span class="nav-number">46.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-37"><span class="nav-number">46.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-32"><span class="nav-number">46.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-42"><span class="nav-number">46.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-21"><span class="nav-number">46.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-44"><span class="nav-number">46.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-30"><span class="nav-number">46.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper11-1202"><span class="nav-number">47.</span> <span class="nav-text">Paper11-1202</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-37"><span class="nav-number">47.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-38"><span class="nav-number">47.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-33"><span class="nav-number">47.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-43"><span class="nav-number">47.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-22"><span class="nav-number">47.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-45"><span class="nav-number">47.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-31"><span class="nav-number">47.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper10-1201"><span class="nav-number">48.</span> <span class="nav-text">Paper10-1201</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-38"><span class="nav-number">48.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-39"><span class="nav-number">48.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-34"><span class="nav-number">48.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-44"><span class="nav-number">48.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-23"><span class="nav-number">48.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-46"><span class="nav-number">48.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-32"><span class="nav-number">48.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper9-1130"><span class="nav-number">49.</span> <span class="nav-text">Paper9-1130</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-39"><span class="nav-number">49.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-40"><span class="nav-number">49.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-35"><span class="nav-number">49.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-45"><span class="nav-number">49.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-24"><span class="nav-number">49.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-47"><span class="nav-number">49.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-33"><span class="nav-number">49.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper15-1128"><span class="nav-number">50.</span> <span class="nav-text">Paper15-1128</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-40"><span class="nav-number">50.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-41"><span class="nav-number">50.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-36"><span class="nav-number">50.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-46"><span class="nav-number">50.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-25"><span class="nav-number">50.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-48"><span class="nav-number">50.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-34"><span class="nav-number">50.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper14-1127"><span class="nav-number">51.</span> <span class="nav-text">Paper14-1127</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-41"><span class="nav-number">51.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-42"><span class="nav-number">51.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-37"><span class="nav-number">51.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-47"><span class="nav-number">51.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-26"><span class="nav-number">51.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-49"><span class="nav-number">51.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-35"><span class="nav-number">51.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper13-1126"><span class="nav-number">52.</span> <span class="nav-text">Paper13-1126</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-42"><span class="nav-number">52.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-43"><span class="nav-number">52.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-38"><span class="nav-number">52.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-48"><span class="nav-number">52.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-27"><span class="nav-number">52.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-50"><span class="nav-number">52.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-36"><span class="nav-number">52.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper30-1124"><span class="nav-number">53.</span> <span class="nav-text">Paper30-1124</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-43"><span class="nav-number">53.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-44"><span class="nav-number">53.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-39"><span class="nav-number">53.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-49"><span class="nav-number">53.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-28"><span class="nav-number">53.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-51"><span class="nav-number">53.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-37"><span class="nav-number">53.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper29-1123"><span class="nav-number">54.</span> <span class="nav-text">Paper29-1123</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-44"><span class="nav-number">54.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-45"><span class="nav-number">54.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-40"><span class="nav-number">54.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-50"><span class="nav-number">54.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-29"><span class="nav-number">54.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-52"><span class="nav-number">54.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-38"><span class="nav-number">54.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper28-1113"><span class="nav-number">55.</span> <span class="nav-text">Paper28-1113</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-45"><span class="nav-number">55.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-46"><span class="nav-number">55.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-41"><span class="nav-number">55.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-51"><span class="nav-number">55.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-30"><span class="nav-number">55.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-53"><span class="nav-number">55.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper27-1111"><span class="nav-number">56.</span> <span class="nav-text">Paper27-1111</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-46"><span class="nav-number">56.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-47"><span class="nav-number">56.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-42"><span class="nav-number">56.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-52"><span class="nav-number">56.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-31"><span class="nav-number">56.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-54"><span class="nav-number">56.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-39"><span class="nav-number">56.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper24-1110"><span class="nav-number">57.</span> <span class="nav-text">Paper24-1110</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-47"><span class="nav-number">57.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-48"><span class="nav-number">57.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-43"><span class="nav-number">57.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-53"><span class="nav-number">57.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-32"><span class="nav-number">57.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-55"><span class="nav-number">57.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-40"><span class="nav-number">57.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper23-1115"><span class="nav-number">58.</span> <span class="nav-text">Paper23-1115</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-48"><span class="nav-number">58.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-49"><span class="nav-number">58.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-44"><span class="nav-number">58.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-54"><span class="nav-number">58.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-33"><span class="nav-number">58.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-56"><span class="nav-number">58.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-41"><span class="nav-number">58.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper21-1118"><span class="nav-number">59.</span> <span class="nav-text">Paper21-1118</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-49"><span class="nav-number">59.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-50"><span class="nav-number">59.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-45"><span class="nav-number">59.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-55"><span class="nav-number">59.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-34"><span class="nav-number">59.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-57"><span class="nav-number">59.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-42"><span class="nav-number">59.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper20-1119"><span class="nav-number">60.</span> <span class="nav-text">Paper20-1119</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-50"><span class="nav-number">60.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-51"><span class="nav-number">60.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-46"><span class="nav-number">60.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-56"><span class="nav-number">60.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-35"><span class="nav-number">60.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-58"><span class="nav-number">60.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-43"><span class="nav-number">60.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper19-1120"><span class="nav-number">61.</span> <span class="nav-text">Paper19-1120</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-51"><span class="nav-number">61.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-52"><span class="nav-number">61.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-47"><span class="nav-number">61.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-57"><span class="nav-number">61.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-36"><span class="nav-number">61.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-59"><span class="nav-number">61.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-44"><span class="nav-number">61.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper18-1121"><span class="nav-number">62.</span> <span class="nav-text">Paper18-1121</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-52"><span class="nav-number">62.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-53"><span class="nav-number">62.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-48"><span class="nav-number">62.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-58"><span class="nav-number">62.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-37"><span class="nav-number">62.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-60"><span class="nav-number">62.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-45"><span class="nav-number">62.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper17-1122"><span class="nav-number">63.</span> <span class="nav-text">Paper17-1122</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-53"><span class="nav-number">63.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-54"><span class="nav-number">63.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-49"><span class="nav-number">63.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-59"><span class="nav-number">63.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-38"><span class="nav-number">63.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-61"><span class="nav-number">63.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-46"><span class="nav-number">63.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper16-1125"><span class="nav-number">64.</span> <span class="nav-text">Paper16-1125</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-54"><span class="nav-number">64.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-55"><span class="nav-number">64.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-50"><span class="nav-number">64.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-60"><span class="nav-number">64.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-39"><span class="nav-number">64.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-62"><span class="nav-number">64.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-47"><span class="nav-number">64.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper12-1109"><span class="nav-number">65.</span> <span class="nav-text">Paper12-1109</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-55"><span class="nav-number">65.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-56"><span class="nav-number">65.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-51"><span class="nav-number">65.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-61"><span class="nav-number">65.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-40"><span class="nav-number">65.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-63"><span class="nav-number">65.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-48"><span class="nav-number">65.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper8-1108"><span class="nav-number">66.</span> <span class="nav-text">Paper8-1108</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-56"><span class="nav-number">66.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-57"><span class="nav-number">66.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-52"><span class="nav-number">66.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-62"><span class="nav-number">66.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords"><span class="nav-number">66.5.</span> <span class="nav-text">Keywords</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-64"><span class="nav-number">66.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper7-1107"><span class="nav-number">67.</span> <span class="nav-text">Paper7-1107</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-57"><span class="nav-number">67.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-58"><span class="nav-number">67.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-53"><span class="nav-number">67.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-63"><span class="nav-number">67.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-1"><span class="nav-number">67.5.</span> <span class="nav-text">Keywords</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-65"><span class="nav-number">67.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper6-1105"><span class="nav-number">68.</span> <span class="nav-text">Paper6-1105</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-58"><span class="nav-number">68.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-59"><span class="nav-number">68.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-54"><span class="nav-number">68.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-64"><span class="nav-number">68.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-41"><span class="nav-number">68.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-66"><span class="nav-number">68.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper5-1104"><span class="nav-number">69.</span> <span class="nav-text">Paper5-1104</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-59"><span class="nav-number">69.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-60"><span class="nav-number">69.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-55"><span class="nav-number">69.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-65"><span class="nav-number">69.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-2"><span class="nav-number">69.5.</span> <span class="nav-text">Keywords</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-67"><span class="nav-number">69.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper4-1106"><span class="nav-number">70.</span> <span class="nav-text">Paper4-1106</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-60"><span class="nav-number">70.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-61"><span class="nav-number">70.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-56"><span class="nav-number">70.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-66"><span class="nav-number">70.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-3"><span class="nav-number">70.5.</span> <span class="nav-text">Keywords</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-68"><span class="nav-number">70.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper3-1103"><span class="nav-number">71.</span> <span class="nav-text">Paper3-1103</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-61"><span class="nav-number">71.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-62"><span class="nav-number">71.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-57"><span class="nav-number">71.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-67"><span class="nav-number">71.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-69"><span class="nav-number">71.5.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper2-1102"><span class="nav-number">72.</span> <span class="nav-text">Paper2-1102</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-62"><span class="nav-number">72.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-63"><span class="nav-number">72.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-58"><span class="nav-number">72.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-68"><span class="nav-number">72.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Index-Terms"><span class="nav-number">72.5.</span> <span class="nav-text">Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-70"><span class="nav-number">72.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper1-1101"><span class="nav-number">73.</span> <span class="nav-text">Paper1-1101</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-63"><span class="nav-number">73.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-64"><span class="nav-number">73.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-59"><span class="nav-number">73.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-69"><span class="nav-number">73.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Index-Terms-1"><span class="nav-number">73.5.</span> <span class="nav-text">Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-71"><span class="nav-number">73.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PaperT-F"><span class="nav-number">74.</span> <span class="nav-text">PaperT-F</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-64"><span class="nav-number">74.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-65"><span class="nav-number">74.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-60"><span class="nav-number">74.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-70"><span class="nav-number">74.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-42"><span class="nav-number">74.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-72"><span class="nav-number">74.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-49"><span class="nav-number">74.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95"><span class="nav-number">75.</span> <span class="nav-text">阅读方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E7%AD%94%E8%A6%81%E7%82%B9%E6%80%BB%E7%BB%93"><span class="nav-number">75.1.</span> <span class="nav-text">回答要点总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-50"><span class="nav-number">75.2.</span> <span class="nav-text">References</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Haoran Geng"
      src="/images/paperboat.jpg">
  <p class="site-author-name" itemprop="name">Haoran Geng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/njughr" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;njughr" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hrgeng@smail.nju.edu.cn" title="E-Mail → mailto:hrgeng@smail.nju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1444528490&auto=0&height=66"></iframe>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Haoran Geng</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script color='105,7,90' opacity='0.6' zIndex='-1' count='100' src="/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>


  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
