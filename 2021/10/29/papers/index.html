<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/G128.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/G32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/G16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"njughr.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="阅读方法知乎问题：如何有针对地高效地阅读一篇学术论文？ 回答要点总结 不要追求完美，没必要逐行读懂，关键是数量要上去，量变引起质变，而不是纠结于一句两句的意思。 带着问题读论文，目的是找到答案为止。先有个大概的、粗略的问题，随着阅读的深入，慢慢提出更细致的问题，进行更深入的阅读。一定是一整批一起读懂到某个层次，而不是逐篇逐篇地整篇一次读懂。这篇博客就是保证一批论文全部至少读到abstract看懂的">
<meta property="og:type" content="article">
<meta property="og:title" content="Pipes">
<meta property="og:url" content="https://njughr.github.io/2021/10/29/papers/index.html">
<meta property="og:site_name" content="GHR&#39;s Blog">
<meta property="og:description" content="阅读方法知乎问题：如何有针对地高效地阅读一篇学术论文？ 回答要点总结 不要追求完美，没必要逐行读懂，关键是数量要上去，量变引起质变，而不是纠结于一句两句的意思。 带着问题读论文，目的是找到答案为止。先有个大概的、粗略的问题，随着阅读的深入，慢慢提出更细致的问题，进行更深入的阅读。一定是一整批一起读懂到某个层次，而不是逐篇逐篇地整篇一次读懂。这篇博客就是保证一批论文全部至少读到abstract看懂的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://loop.frontiersin.org/images/profile/1228973/24">
<meta property="og:image" content="https://loop.frontiersin.org/images/profile/1229999/24">
<meta property="og:image" content="https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg">
<meta property="og:image" content="https://njughr.github.io/2021/10/29/papers/1.png">
<meta property="og:image" content="https://njughr.github.io/2021/10/29/papers/qiufen.jpg">
<meta property="article:published_time" content="2021-10-29T11:35:57.000Z">
<meta property="article:modified_time" content="2021-11-26T00:52:33.000Z">
<meta property="article:author" content="Haoran Geng">
<meta property="article:tag" content="ic">
<meta property="article:tag" content="cs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://loop.frontiersin.org/images/profile/1228973/24">

<link rel="canonical" href="https://njughr.github.io/2021/10/29/papers/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Pipes | GHR's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">GHR's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Yesterday is done. Tomorrow never comes. Today is here.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">20</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">18</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://njughr.github.io/2021/10/29/papers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/paperboat.jpg">
      <meta itemprop="name" content="Haoran Geng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GHR's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pipes
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-29 19:35:57" itemprop="dateCreated datePublished" datetime="2021-10-29T19:35:57+08:00">2021-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-26 08:52:33" itemprop="dateModified" datetime="2021-11-26T08:52:33+08:00">2021-11-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="阅读方法"><a href="#阅读方法" class="headerlink" title="阅读方法"></a>阅读方法</h1><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/23924014">知乎问题：如何有针对地高效地阅读一篇学术论文？</a></p>
<h2 id="回答要点总结"><a href="#回答要点总结" class="headerlink" title="回答要点总结"></a>回答要点总结</h2><hr>
<p>不要追求完美，没必要逐行读懂，关键是数量要上去，量变引起质变，而不是纠结于一句两句的意思。</p>
<p>带着问题读论文，目的是找到答案为止。先有个大概的、粗略的问题，随着阅读的深入，慢慢提出更细致的问题，进行更深入的阅读。<strong>一定是一整批一起读懂到某个层次，而不是逐篇逐篇地整篇一次读懂。</strong>这篇博客就是保证一批论文全部至少读到abstract看懂的层次。</p>
<p><strong>不要读不会用到的东西，白费的力气必须被极小化！其实，绝大部分论文都只需要了解它的主要观念</strong>（这往往比较容易），<strong>而不需要了解它的详细推导过程</strong>（这反而比较费时）。</p>
<p><strong>整批读略过一次之后，就可以规划出一个你以为比较容易懂的阅读次序。想读懂A论文，不一定非得读A论文，或许阅读引用A论文的B论文，可以看看其他人的理解</strong>。</p>
<blockquote>
<p><strong>我读论文远比学生快，分析远比学生深入，主要的是我敢想象与猜测，而且多年训练下来想象与猜测的准确度很高。所以，许多论文我根本不是「读懂」的，而是「猜对」了！</strong>猜错了就猜错了，继续读论文可以容易的知道对错。</p>
</blockquote>
<hr>
<p>批判性地阅读（下面是我自己的大略翻译，原文见<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/23924014/answer/26470331">如何有针对地高效地阅读一篇学术论文？ - Clei的回答 - 知乎 </a>）</p>
<p>分析和评估，而不是简单的总结。看完论文后，不可以只知道“作者说了什么？“，而是要回答”作者如何实现的？“”作者为什么要这样做？”“效果好不好？”，并不一定非得批评，但是一定要有批判的态度。</p>
<p>【下面这些问题比较烦，其实就是What Why How】</p>
  <span id="more"></span>
<p>问自己：</p>
<ul>
<li>作者的主要观点是？</li>
<li>作者的意图是？</li>
<li>作者的目标读者是？</li>
<li>作者用了哪些论据去证明自己的观点？</li>
<li>作者用了哪些实验去证明了自己的论据？</li>
<li>作者的基本假设是？</li>
<li>作者有什么偏见？</li>
</ul>
<p>以上问题的回答需要进行记录。</p>
<p>他还给了很多其他问题，但是个人觉得没有必要看。</p>
<hr>
<p>下面是H.B.Zhou老师课堂上的一些观点：</p>
<ul>
<li>问题是什么，写的什么，主题什么；问题的难点在哪里，必须做的点在哪里；怎么去做的，效果如何。为什么要做，创新点在哪里，打算怎么做，怎么去做的，挑战在哪里。</li>
<li>Scenario：把应用场景画出来，画出来就是：为什么要这么做。</li>
<li>Motivation：动机是什么：难，有挑战，别人没做过；我们有创新；我们有新的解决方案。</li>
<li>Solutions：如何去解决。粗略或者详细的solutions。</li>
<li>任何事情都要有<strong>What Why How</strong>的闭环。讨论问题，应该事事有回响，事事有反馈，要有ACK，总结和跟进。</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://wenku.baidu.com/view/bb3dfb7f31b765ce05081437.html">彭明辉，硕士班研究所新生手册</a></li>
<li>Rosen, Leonard J. and Laurence Behrens, eds. The Allyn &amp; Bacon Handbook. 1994.</li>
<li>栾浩 樊凯 项阳，《科研有方— 做科研和写论文的一些经验》</li>
</ol>
<hr>
<h1 id="Template"><a href="#Template" class="headerlink" title="Template"></a>Template</h1><h2 id="Published-in"><a href="#Published-in" class="headerlink" title="Published in"></a>Published in</h2><h2 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h2><h2 id="Authors"><a href="#Authors" class="headerlink" title="Authors"></a>Authors</h2><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h2 id="Keywords-Index-Terms"><a href="#Keywords-Index-Terms" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper30-1124"><a href="#Paper30-1124" class="headerlink" title="Paper30-1124"></a>Paper30-1124</h1><h2 id="Published-in-1"><a href="#Published-in-1" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/conhome/8023017/proceeding">2017 46th International Conference on Parallel Processing (ICPP)</a></p>
<p>中国计算机学会和清华推荐的<strong>B类</strong>会议</p>
<p>Y. Nagasaka, A. Nukada and S. Matsuoka, “High-Performance and Memory-Saving Sparse General Matrix-Matrix Multiplication for NVIDIA Pascal GPU,” 2017 46th International Conference on Parallel Processing (ICPP), 2017, pp. 101-110, doi: 10.1109/ICPP.2017.19.</p>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8025284">High-Performance and Memory-Saving Sparse General Matrix-Matrix Multiplication for NVIDIA Pascal GPU</a></p>
<h2 id="Authors-1"><a href="#Authors-1" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085596361">Yusuke Nagasaka</a></p>
<p>Tokyo Institute of Technology, Tokyo, Japan</p>
<p>东京工业大学，东京，日本</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37392857200">Akira Nukada</a></p>
<p>Tokyo Institute of Technology, Tokyo, Japan</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085374497">Satoshi Matsuoka</a></p>
<p>Tokyo Institute of Technology, Tokyo, Japan</p>
<h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p>Sparse general matrix-matrix multiplication (SpGEMM) is one of the key kernels of preconditioners such as algebraic multigrid method or graph algorithms. However, the performance of SpGEMM is quite low on modern processors due to random memory access to both input and output matrices. As well as the number and the pattern of non-zero elements in the output matrix, important for achieving locality, are unknown before the execution. Moreover, the state-of-the-art GPU implementations of SpGEMM requires large amounts of memory for temporary results, limiting the matrix size computable on fast GPU device memory. We propose a new fast SpGEMM algorithm requiring small amount of memory and achieving high performance. Calculation of the pattern and value in output matrix is optimized by using GPU’s on-chip shared memory and a hash table. Additionally, our algorithm launches multiple kernels running concurrently to improve the utilization of GPU resources. The kernels for the calculation of each row of output matrix are chosen based on the number of non-zero elements. Performance evaluation using matrices from the Sparse Matrix Collection of University Florida on NVIDIA’s Pascal generation GPU shows that our approach achieves speedups of up to x4.3 in single precision and x4.4 in double precision compared to existing SpGEMM libraries. Furthermore, the memory usage is reduced by 14.7% in single precision and 10.9% in double precision on average, allowing larger matrices to be computed.</p>
<p>通用稀疏矩阵矩阵乘法 (Sparse general matrix-matrix multiplication, SpGEMM) 是代数多重网格算法（algebraic multigrid method, AMG算法）或图算法等预处理器(preconditioners)的关键内核之一。然而，由于对输入和输出矩阵的随机内存访问，SpGEMM 在现代处理器上的性能非常低。对于实现局部性很重要的输出矩阵中非零元素的数量和分布(pattern)在执行之前是未知的。此外，SpGEMM 最先进的 GPU 实现需要大量显存？内存？用于临时结果，限制了可在 GPU 设备显存上计算的矩阵大小。我们提出了一种新的快速 SpGEMM 算法，只需要较少的内存即可实现高性能。我们通过使用 GPU 的片上共享内存和哈希表对输出矩阵的pattern和值的计算进行了优化。此外，我们的算法会启动多个并发运行的kernel，以提高 GPU 资源的利用率。计算每一行输出矩阵的内核是根据非零元素的数量选择的。在 NVIDIA 的 Pascal GPU 上使用佛罗里达大学稀疏矩阵集合(Sparse Matrix Collection of University Florida)中的矩阵进行的性能评估表明，与现有的 SpGEMM 库相比，<strong>我们的方法在单精度和双精度下实现了高达 x4.3 的加速</strong>。此外，单精度内存使用量平均减少 14.7%，双精度内存使用量平均减少 10.9%，同时允许计算更大的矩阵。</p>
<h2 id="Keywords-Index-Terms-1"><a href="#Keywords-Index-Terms-1" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><ul>
<li><p>IEEE Keywords</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Sparse matrices&amp;newsearch=true">Sparse matrices</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Graphics processing units&amp;newsearch=true">Graphics processing units</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Memory management&amp;newsearch=true">Memory management</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Acceleration&amp;newsearch=true">Acceleration</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Instruction sets&amp;newsearch=true">Instruction sets</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Kernel&amp;newsearch=true">Kernel</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Parallel processing&amp;newsearch=true">Parallel processing</a></p>
<p>稀疏矩阵，GPU，内存管理，加速，指令集，内核，并行处理</p>
</li>
<li><p>INSPEC: Controlled Indexing</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:graphics processing units&amp;newsearch=true">graphics processing units</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:matrix multiplication&amp;newsearch=true">matrix multiplication</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:parallel processing&amp;newsearch=true">parallel processing</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:shared memory systems&amp;newsearch=true">shared memory systems</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:software performance evaluation&amp;newsearch=true">software performance evaluation</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:storage management&amp;newsearch=true">storage management</a></p>
<p>GPU，矩阵乘法，并行处理，shared memory system，软件性能评估，存储管理</p>
</li>
<li><p>INSPEC: Non-Controlled Indexing</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:sparse general matrix-matrix multiplication&amp;newsearch=true">sparse general matrix-matrix multiplication</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:SpGEMM&amp;newsearch=true">SpGEMM</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:NVIDIA Pascal GPU&amp;newsearch=true">NVIDIA Pascal GPU</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:graphics processing unit&amp;newsearch=true">graphics processing unit</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:high performance computing&amp;newsearch=true">high performance computing</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:on-chip shared memory&amp;newsearch=true">on-chip shared memory</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:hash table&amp;newsearch=true">hash table</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:performance evaluation&amp;newsearch=true">performance evaluation</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:memory usage reduction&amp;newsearch=true">memory usage reduction</a></p>
<p>SpGEMM, NV Pascal GPU, GPU，高性能计算，片上共享内存，哈希表，性能评估，减少内存用量</p>
</li>
<li><p>Author Keywords</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Sparse matrix&amp;newsearch=true">Sparse matrix</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:SpGEMM&amp;newsearch=true">SpGEMM</a>,<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:GPU&amp;newsearch=true">GPU</a></p>
<p>稀疏矩阵，SpGEMM, GPU</p>
</li>
</ul>
<h2 id="Comments-1"><a href="#Comments-1" class="headerlink" title="Comments"></a>Comments</h2><p>本工作是Performance optimization, modeling and analysis of sparse matrix-matrix products on multi-core and many-core processors的前置。paper30的一作是paper5的一作。这篇文章是基于GPU的，paper5是基于CPU的。</p>
<p>提到的佛罗里达大学稀疏矩阵集合(Sparse Matrix Collection of University Florida)可以下载下来看看。</p>
<h2 id="References-2"><a href="#References-2" class="headerlink" title="References"></a>References</h2><p><a target="_blank" rel="noopener" href="https://www.nvidia.cn/data-center/pascal-gpu-architecture/">NVIDIA Pascal GPU Architecture</a></p>
<p>产品包括：</p>
<p><strong>GeForce系列游戏显卡</strong></p>
<p><strong>GTX1050、1050Ti、1060(3G, 5G, 6G)、1070、1070Ti、1080、1080Ti</strong>等</p>
<p><strong>QUADRO系列专业显卡</strong></p>
<p>GP100、P6000、P5000、P4000、P2000、P1000、P600、P400等 [13] </p>
<p><strong>Tesla系列加速计算卡</strong></p>
<p><strong>P100</strong>、P4、P40</p>
<p><strong>NVS系列多显示器商用显卡</strong></p>
<p>暂无Pascal产品</p>
<p><strong>TITAN显卡</strong></p>
<p>TITAN Xp</p>
<hr>
<h1 id="Paper29-1123"><a href="#Paper29-1123" class="headerlink" title="Paper29-1123"></a>Paper29-1123</h1><h2 id="Published-in-2"><a href="#Published-in-2" class="headerlink" title="Published in"></a>Published in</h2><p>SIAM Journal on Scientific Computing</p>
<p>期刊一般。不要考虑。</p>
<h2 id="Title-2"><a href="#Title-2" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://epubs.siam.org/doi/10.1137/15M104253X">Exploiting Multiple Levels of Parallelism in Sparse Matrix-Matrix Multiplication</a></p>
<h2 id="Authors-2"><a href="#Authors-2" class="headerlink" title="Authors"></a>Authors</h2><p><strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Azad%2C+Ariful">Ariful Azad</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Ballard%2C+Grey">Grey Ballard</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Buluç%2C+Aydin">Aydin Buluç</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Demmel%2C+James">James Demmel</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Grigori%2C+Laura">Laura Grigori</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Schwartz%2C+Oded">Oded Schwartz</a></strong>, <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Toledo%2C+Sivan">Sivan Toledo</a></strong>, and <strong><a target="_blank" rel="noopener" href="https://epubs.siam.org/author/Williams%2C+Samuel">Samuel Williams</a></strong></p>
<p>ARIFUL AZAD†, GREY BALLARD‡, AYDIN BULUC¸†, JAMES DEMMEL§, LAURA GRIGORI¶, ODED SCHWARTZk, SIVAN TOLEDO#, AND SAMUEL WILLIAMS†</p>
<p>† CRD, Lawrence Berkeley National Laboratory, Berkeley, CA 94720 (azad@lbl.gov, abuluc@lbl.gov, swwilliams@lbl.gov).劳伦斯伯克利国家实验室，伯克利</p>
<p>‡ Computer    Science    Department,    Wake    Forest    University,    Winston    Salem,    NC    94551 (ballard@wfu.edu).计算机科学系，维克森林大学，温斯顿塞勒姆，北卡罗来纳州</p>
<p>§ EECS, University of California, Berkeley, Berkeley, CA 94720 (demmel@eecs.berkeley.edu).EECS，UCB加州大学伯克利分校，伯克利</p>
<p>¶ INRIA Paris-Rocquencourt, Alpines, Paris, 75005, France (laura.grigori@inria.fr). kThe Hebrew University, Rothberg A405, Jerusalem, Israel (odedsc@cs.huji.ac.il).希伯来大学，以色列耶路撒冷</p>
<p># Blavatnik School of Computer Science, Tel Aviv University, Ramot Aviv, Tel-Aviv 69978, Israel (stoledo@tau.ac.il).Blavatnik 计算机科学学院，特拉维夫大学，Ramot Aviv，特拉维夫 69978，以色列 (stoledo@tau.ac.il)。</p>
<p>† ‡ § ¶ # </p>
<h2 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h2><p>Sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many high-performance graph algorithms as well as for some linear solvers, such as algebraic multigrid. The <strong>scaling</strong> of existing parallel implementations of SpGEMM is heavily bound by communication. Even though 3D (or 2.5D) algorithms have been proposed and theoretically analyzed in the flat MPI model on Erdös—Rényi matrices, those algorithms had not been implemented in practice and their complexities had not been analyzed for the general case. In this work, <strong>we present the first implementation of the 3D SpGEMM formulation</strong> that exploits multiple (intranode and internode) levels of parallelism, achieving significant speedups over the state-of-the-art publicly available codes at <strong>all levels of concurrencies</strong>. We extensively evaluate our implementation and identify bottlenecks that should be subject to further research.</p>
<p>稀疏矩阵乘法（或 SpGEMM）是许多高性能图算法以及一些linear solvers（例如代数多重网格算法（algebraic multigrid method, AMG算法））的关键原语（key primitive）。 SpGEMM 现有并行实现的scaling(扩展？）在很大程度上受到通信的约束。 尽管已经在 <strong>Erdös—Rényi 矩阵的平面 MPI 模型</strong>中提出了 3D（或 2.5D）算法并对其进行了理论分析，但这些算法尚未在实践中实现，并且尚未针对一般情况分析其复杂性。 在这项工作中，<strong>我们展示了 3D SpGEMM 公式的第一个实现</strong>，它利用了多个（节点内和节点间）并行性，在所有并发级别上实现了对最先进的公开可用代码的显着加速。 我们广泛评估了我们的实施并确定了下一步研究的瓶颈。</p>
<h2 id="Keywords-Index-Terms-2"><a href="#Keywords-Index-Terms-2" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>parallel computing, numerical linear algebra, sparse matrix-matrix multiplication, 2.5D algorithms, 3D algorithms, multithreading, SpGEMM, 2D decomposition, graph algorithms</p>
<p>并行计算、数值线性代数、SpGEMM、2.5D 算法、3D 算法、多线程、SpGEMM、2D 分解、图算法</p>
<h2 id="Comments-2"><a href="#Comments-2" class="headerlink" title="Comments"></a>Comments</h2><p>本工作是Performance optimization, modeling and analysis of sparse matrix-matrix products on multi-core and many-core processors的前置。paper29的一作是paper5的三作。文章期刊似乎不是很有名，<strong>IEEE上也查不到</strong>。</p>
<h2 id="References-3"><a href="#References-3" class="headerlink" title="References"></a>References</h2><hr>
<h1 id="Paper28-1113"><a href="#Paper28-1113" class="headerlink" title="Paper28-1113"></a>Paper28-1113</h1><h2 id="Published-in-3"><a href="#Published-in-3" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/conhome/9567713/proceeding">ESSCIRC 2021 - IEEE 47th European Solid State Circuits Conference (ESSCIRC)</a></p>
<p>T. Iizuka, H. Xu and A. A. Abidi, “A Tutorial on Systematic Design of CMOS A/D Converters: Illustrated by a 10 b, 500 MS/s SAR ADC with 2 GHz RBW,” ESSCIRC 2021 - IEEE 47th European Solid State Circuits Conference (ESSCIRC), 2021, pp. 381-386, doi: 10.1109/ESSCIRC53450.2021.9567842.</p>
<h2 id="Title-3"><a href="#Title-3" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9567842/">A Tutorial on Systematic Design of CMOS A/D Converters: Illustrated by a 10 b, 500 MS/s SAR ADC with 2 GHz RBW</a></p>
<h2 id="Authors-3"><a href="#Authors-3" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088570233">Tetsuya Iizuka</a></p>
<p>Systems Design Lab., School of Engineering, The University of Tokyo, Tokyo, Japan</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085438096">Hao Xu</a></p>
<p>University of California, Los Angeles, CA, USA</p>
<p><strong>Apple Inc.,</strong> CA, USA</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37275349700">Asad A. Abidi</a></p>
<p>University of California, Los Angeles, CA, USA</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088570233">饭冢哲也</a></p>
<p>日本东京大学工程学院系统设计实验室</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085438096">徐浩</a></p>
<p>美国加州大学洛杉矶分校</p>
<p>美国加利福尼亚州苹果公司</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37275349700">Asad A. Abidi</a></p>
<p>美国加州大学洛杉矶分校</p>
<h2 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper presents a systematic design framework for ADC optimization. Our emphasis is on a robust design that is highly repeatable, which is driven by a deep understanding of the behavior of circuit building blocks. A 10 b 500 MS/s single-channel SAR ADC designed in this framework displays uniform performance for inputs up to 2 GHz at state-of-the-art FoM, which demonstrates the power of design based on analytical expressions.</p>
<p>本文提出了一个用于 ADC 优化的系统设计框架。 我们的重点是高度可重复的稳健设计，这是由对电路构建块行为的深入理解驱动的。 在此框架中设计的 10 b 500 MS/s 单通道 SAR ADC 在最先进的 FoM 下对高达 2 GHz 的输入显示出一致的性能，这展示了基于分析表达式的设计的力量。</p>
<p>类似Artificial Intelligence Designs Digital Circuits【AIDDC】</p>
<h2 id="Keywords-Index-Terms-3"><a href="#Keywords-Index-Terms-3" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><ul>
<li>IEEE Keywords <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Systematics&amp;newsearch=true">Systematics</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Conferences&amp;newsearch=true">Conferences</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Europe&amp;newsearch=true">Europe</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Tutorials&amp;newsearch=true">Tutorials</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Solid state circuit design&amp;newsearch=true">Solid state circuit design</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Tools&amp;newsearch=true">Tools</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Frequency conversion&amp;newsearch=true">Frequency conversion</a></li>
<li>Author Keywords <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Systematic design&amp;newsearch=true">Systematic design</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:analog-to-digital converter&amp;newsearch=true">analog-to-digital converter</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:regenerative comparator&amp;newsearch=true">regenerative comparator</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:sample and hold&amp;newsearch=true">sample and hold</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:harmonic distortion&amp;newsearch=true">harmonic distortion</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:jitter&amp;newsearch=true">jitter</a></li>
</ul>
<h2 id="Comments-3"><a href="#Comments-3" class="headerlink" title="Comments"></a>Comments</h2><p>老师让打印的</p>
<hr>
<h1 id="Paper27-1111"><a href="#Paper27-1111" class="headerlink" title="Paper27-1111"></a>Paper27-1111</h1><h2 id="Published-in-4"><a href="#Published-in-4" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Madhubabu Anumukonda, Prasadraju Lakkamraju, Shubhajit Roy Chowdhury, “FPGA-Based High-Performance Phonocardiography System for Extraction of Cardiac Sound Components Using Inverse Delayed Neuron Model”, <em>Frontiers in Medical Technology</em>, vol. 3, 2021.</p>
<p>IEEE搜不到</p>
<h2 id="Title-4"><a href="#Title-4" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.frontiersin.org/articles/10.3389/fmedt.2021.666650/full">FPGA-Based High-Performance Phonocardiography System for Extraction of Cardiac Sound Components Using Inverse Delayed Neuron Model</a></p>
<p>使用逆延迟神经元模型提取心脏声音成分的基于 FPGA 的高性能心音图系统（？）</p>
<h2 id="Authors-4"><a href="#Authors-4" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://www.frontiersin.org/people/u/1228973"><img src="https://loop.frontiersin.org/images/profile/1228973/24" alt="img">Madhubabu Anumukonda</a><sup>1*</sup>, <a target="_blank" rel="noopener" href="https://www.frontiersin.org/people/u/1229999"><img src="https://loop.frontiersin.org/images/profile/1229999/24" alt="img">Prasadraju Lakkamraju</a><sup>1</sup> and <img src="https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg" alt="img">Shubhajit Roy Chowdhury<sup>2</sup></p>
<p>玛达胡巴布 阿努木空达； 朴冉撒达瑞居 啦咔咔马冉居；蜀布哈吉特 罗伊 朱迪胡瑞</p>
<p>1 Center for Very Large Scale Integration and Embedded Systems Technology, International Institute of Information Technology Hyderabad, Hyderabad, India</p>
<p>1 海得拉巴国际信息技术研究所超大规模集成和嵌入式系统技术中心，印度海得拉巴</p>
<p>2 School for Computing and Electrical Engineering, Indian Institute of Technology Mandi, Suran, India</p>
<p>2 印度理工学院曼迪计算与电气工程学院，印度苏兰</p>
<h2 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h2><p>The study focuses on the extraction of cardiac sound components using a multi-channel micro-electromechanical system (MEMS) microphone-based phonocardiography system. The proposed multi-channel phonocardiography system classifies the cardiac sound components using artificial neural networks (ANNs) and synaptic weights that are calculated using the inverse delayed (ID) function model of the neuron. The proposed ANN model was simulated in MATLABR and implemented in a field-programmable gate array (FPGA). The proposed system examined both abnormal and normal samples collected from 30 patients. Experimental results revealed a good sensitivity of 99.1% and an accuracy of 0.9.</p>
<p>该研究的重点是使用基于多通道微机电系统 (micro-electromechanical system, MEMS) 麦克风的心音图(phonocardiography)系统提取心脏声音成分。 所提出的多通道心音图系统使用人工神经网络 (artificial neural networks, ANN) 和使用神经元的逆延迟 (inverse delayed, ID) 函数模型计算的突触权重？对心脏声音分量进行分类。 提出的 ANN 模型在 MATLABR 中进行了模拟，并在现场可编程门阵列 (field-programmable gate array, FPGA) 中实现。 所提出的系统检查了从 30 名患者收集的异常和正常样本。 实验结果显示良好的灵敏度为 99.1%，准确度为 0.9。</p>
<h2 id="Keywords-Index-Terms-4"><a href="#Keywords-Index-Terms-4" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>phonocardiography, cardiac sounds, inverse delayed function model of neuron, artificial neural networks, field programmable gate array</p>
<p>心音图、心音、神经元逆延迟函数模型、人工神经网络、现场可编程门阵列</p>
<h2 id="Comments-4"><a href="#Comments-4" class="headerlink" title="Comments"></a>Comments</h2><p>没有看的必要。</p>
<h2 id="References-4"><a href="#References-4" class="headerlink" title="References"></a>References</h2><p>MATLABR应该就是MATLAB</p>
<hr>
<h1 id="Paper26-1112"><a href="#Paper26-1112" class="headerlink" title="Paper26-1112"></a>Paper26-1112</h1><h2 id="Published-in-5"><a href="#Published-in-5" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Samba Raju Chiluveru, Manoj Tripathy, Bibhudutta, “Non‐linear activation function approximation using a REMEZ algorithm”, <em>IET Circuits, Devices &amp; Systems</em>, 2021.</p>
<p>IET Circuits, Devices &amp; Systems，该期刊是SCI但是在南大目录里没有</p>
<h2 id="Title-5"><a href="#Title-5" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/cds2.12058">Non-linear activation function approximation using a REMEZ algorithm</a></p>
<h2 id="Authors-5"><a href="#Authors-5" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ietresearch.onlinelibrary.wiley.com/action/doSearch?ContribAuthorRaw=Chiluveru%2C+Samba+Raju">Samba Raju Chiluveru</a></p>
<p><a target="_blank" rel="noopener" href="https://ietresearch.onlinelibrary.wiley.com/action/doSearch?ContribAuthorRaw=Tripathy%2C+Manoj">Manoj Tripathy</a></p>
<p><a target="_blank" rel="noopener" href="https://ietresearch.onlinelibrary.wiley.com/action/doSearch?ContribAuthorRaw=Bibhudutta">Bibhudutta</a></p>
<p>印度电气工程系</p>
<p>印度北阿坎德邦 Roorkee 理工学院</p>
<h2 id="Abstract-5"><a href="#Abstract-5" class="headerlink" title="Abstract"></a>Abstract</h2><p>Here a more accurate piecewise approximation (PWA) scheme for non-linear activation function is proposed. It utilizes a precision-controlled recursive algorithm to predict a sub-range; after that, the REMEZ algorithm is used to find the corresponding approximation function. The PWA realized in three ways: using first-order functions, that is, piecewise linear model, second-order functions (piecewise non-linear model), and hybrid-order model (a mixture of first-order and second-order functions). The hybrid-order approximation employs the second-order derivative of non-linear activation function to decide the linear and non-linear sub-regions, correspondingly the first-order and second-order functions are predicted, respectively. The accuracy is compared to the present state-of-the-art approximation schemes. The multi-layer perceptron model is designed to implement XOR-gate, and it uses an approximate activation function. The hardware utilization is measured using the TSMC 0.18-μm library with the Synopsys Design Compiler. Result reveals that the proposed approximation scheme efficiently approximates the non-linear activation functions.</p>
<p>这里提出了一种更准确的非线性激活函数分段逼近 (PWA) 方案。它利用精确控制的递归算法来预测子范围；之后使用REMEZ算法寻找对应的逼近函数。 PWA通过三种方式实现：使用一阶函数，即分段线性模型、二阶函数（分段非线性模型）和混合阶模型（一阶和二阶函数的混合） . 混合阶逼近采用非线性激活函数的二阶导数来决定线性和非线性子区域，相应地分别预测一阶和二阶函数。将精度与当前最先进的近似方案进行比较。多层感知器模型旨在实现异或门，它使用近似激活函数。硬件利用率是使用 TSMC 0.18-μm 库和 Synopsys 设计编译器测量的。结果表明，所提出的逼近方案有效地逼近了非线性激活函数。</p>
<h2 id="Comments-5"><a href="#Comments-5" class="headerlink" title="Comments"></a>Comments</h2><p>需要好好读一下。</p>
<h2 id="References-5"><a href="#References-5" class="headerlink" title="References"></a>References</h2><p>Sigmoid Generators for Neural Computing Using Piecewise Approximations</p>
<hr>
<h1 id="Paper25-1117"><a href="#Paper25-1117" class="headerlink" title="Paper25-1117"></a>Paper25-1117</h1><h2 id="Published-in-6"><a href="#Published-in-6" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Samba Raju Chiluveru, Manoj Tripathy, Snehit Chunarkar, “A Controlled Accuracy-Based Recursive Algorithm for Approximation of Sigmoid Activation”, <em>National Academy Science Letters</em>, 2021.</p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/journal/40009"><em>National Academy Science Letters</em></a></p>
<p>SCI但是南大目录里没有</p>
<h2 id="Title-6"><a href="#Title-6" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s40009-020-01037-4">A Controlled Accuracy-Based Recursive Algorithm for Approximation of Sigmoid Activation</a></p>
<h2 id="Authors-6"><a href="#Authors-6" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s40009-020-01037-4#auth-Samba_Raju-Chiluveru">Samba Raju Chiluveru</a></p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s40009-020-01037-4#auth-Manoj-Tripathy">Manoj Tripathy</a></p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s40009-020-01037-4#auth-Snehit-Chunarkar">Snehit Chunarkar</a></p>
<p>Department of Electrical Engineering, Indian Institute of Technology, Roorkee, India</p>
<p>印度理工学院电气工程系，印度 Roorkee</p>
<p>和上面那篇文章如出一辙</p>
<h2 id="Abstract-6"><a href="#Abstract-6" class="headerlink" title="Abstract"></a>Abstract</h2><p>The sigmoid activation function is widely used in the development of neural networks. It is implemented in hardware development using an approximation scheme. In this paper, a piecewise linear (PWL) approximation scheme is developed as an accuracy controlled recursive algorithm. It uses a maximum allowable error as an input parameter and the least square method for finding the PWL function. The algorithm’s accuracy is calculated with an absolute average and maximum error and compared to the state of the art. The approximation scheme’s hardware implementation complexity is compared in terms of the number of PWL functions required. Results show that the proposed algorithm requires comparatively fewer PWL functions for the approximate sigmoid function for the specified accuracy.</p>
<p>sigmoid 激活函数广泛用于神经网络的开发。 它使用近似方案在硬件开发中实现。 在本文中，分段线性 (PWL) 近似方案被开发为一种精度控制递归算法。 它使用最大允许误差作为输入参数，并使用最小二乘法来寻找 PWL 函数。 该算法的准确度是通过绝对平均误差和最大误差计算得出的，并与现有技术进行比较。 近似方案的硬件实现复杂性是根据所需的 PWL 函数的数量来比较的。 结果表明，对于指定精度的近似 sigmoid 函数，所提出的算法需要相对较少的 PWL 函数。</p>
<h2 id="Keywords-Index-Terms-5"><a href="#Keywords-Index-Terms-5" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-6"><a href="#Comments-6" class="headerlink" title="Comments"></a>Comments</h2><p>需要仔细读一读</p>
<h2 id="References-6"><a href="#References-6" class="headerlink" title="References"></a>References</h2><h1 id="Paper24-1110"><a href="#Paper24-1110" class="headerlink" title="Paper24-1110"></a>Paper24-1110</h1><h2 id="Published-in-7"><a href="#Published-in-7" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Hui Chen, Lin Jiang, Heping Yang, Zhonghai Lu, Yuxiang Fu, Li Li, Zongguang Yu, “An Efficient Hardware Architecture with Adjustable Precision and Extensible Range to Implement Sigmoid and Tanh Functions”, <em>Electronics</em>, vol. 9, pp. 1739, 2020.</p>
<h2 id="Title-7"><a href="#Title-7" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.mdpi.com/2079-9292/9/10/1739">An Efficient Hardware Architecture with Adjustable Precision and Extensible Range to Implement Sigmoid and Tanh Functions</a></p>
<h2 id="Authors-7"><a href="#Authors-7" class="headerlink" title="Authors"></a>Authors</h2><p>Hui Chen, Lin Jiang, Heping Yang, Zhonghai Lu, Yuxiang Fu, Li Li, Zongguang Yu</p>
<h2 id="Abstract-7"><a href="#Abstract-7" class="headerlink" title="Abstract"></a>Abstract</h2><p>The efficient and precise hardware implementations of tanh and sigmoid functions play an important role in various neural network algorithms. Different applications have different requirements for accuracy. However, it is difficult for traditional methods to achieve adjustable precision. Therefore, we propose an efficient-hardware, adjustable-precision and high-speed architecture to implement them for the first time. Firstly, we present two methods to implement sigmoid and tanh functions. One is based on the rotation mode of hyperbolic CORDIC and the vector mode of linear CORDIC (called RHC-VLC), another is based on the carry-save method and the vector mode of linear CORDIC (called CSM-VLC). We validate the two methods by MATLAB and RTL implementations. Synthesized under the TSMC 40 nm CMOS technology, we find that a special case , based on RHC-VLC method, has the area of 4290.98 m and the power of 1.69 mW at the frequency of 1.5 GHz. However, under the same frequency, (a special case based on CSM-VLC method) costs 3196.36 m area and 1.38 mW power. They are both superior to existing methods for implementing such an architecture with adjustable precision.</p>
<p>tanh 和 sigmoid 函数的高效和精确的硬件实现在各种神经网络算法中发挥着重要作用。不同的应用对精度有不同的要求。然而，传统方法难以达到可调节的精度。因此，我们首次提出了一种高效硬件、可调精度和高速架构来实现它们。首先，我们提出了两种实现 sigmoid 和 tanh 函数的方法。一种是基于双曲线CORDIC的旋转模式和线性CORDIC的向量模式（称为RHC-VLC），另一种是基于进位保存方法和线性CORDIC的向量模式（称为CSM-VLC）。我们通过 MATLAB 和 RTL 实现验证了这两种方法。在台积电 40 nm CMOS 工艺下合成，我们发现一个特例，基于 RHC-VLC 方法，在 1.5 GHz 频率下的面积为 4290.98 m，功率为 1.69 mW。然而，在相同频率下，（基于CSM-VLC方法的特例）成本为3196.36 m面积和1.38 mW功率。它们都优于以可调精度实现这种架构的现有方法。</p>
<h2 id="Keywords-Index-Terms-6"><a href="#Keywords-Index-Terms-6" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p><a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=sigmoid and tanh">sigmoid and tanh</a>; <a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=hyperbolic and linear CORDIC">hyperbolic and linear CORDIC</a>; <a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=carry-save method">carry-save method</a>; <a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=efficient hardware">efficient hardware</a>; <a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=adjustable precision">adjustable precision</a>; <a target="_blank" rel="noopener" href="https://www.mdpi.com/search?q=high speed">high speed</a></p>
<h2 id="Comments-7"><a href="#Comments-7" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-7"><a href="#References-7" class="headerlink" title="References"></a>References</h2><h1 id="Paper23-1115"><a href="#Paper23-1115" class="headerlink" title="Paper23-1115"></a>Paper23-1115</h1><h2 id="Published-in-8"><a href="#Published-in-8" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Yunqi Gao, Feng Luan, Jiaqi Pan, Xu Li, Yaodong He, “FPGA-Based Implementation of Stochastic Configuration Networks for Regression Prediction”, <em>Sensors</em>, vol. 20, pp. 4191, 2020.</p>
<h2 id="Title-8"><a href="#Title-8" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.mdpi.com/1424-8220/20/15/4191">FPGA-Based Implementation of Stochastic Configuration Networks for Regression Prediction</a></p>
<h2 id="Authors-8"><a href="#Authors-8" class="headerlink" title="Authors"></a>Authors</h2><p>Yunqi Gao</p>
<p>Feng Luan </p>
<p>Jiaqi Pan</p>
<p>Xu Li</p>
<p>Yaodong He </p>
<h2 id="Abstract-8"><a href="#Abstract-8" class="headerlink" title="Abstract"></a>Abstract</h2><p>The implementation of neural network regression prediction based on digital circuits is one of the challenging problems in the field of machine learning and cognitive recognition, and it is also an effective way to relieve the pressure of the Internet in the era of intelligence. As a nonlinear network, the stochastic configuration network (SCN) is considered to be an effective method for regression prediction due to its good performance in learning and generalization. Therefore, in this paper, we adapt the SCN to regression analysis, and design and verify the field programmable gate array (FPGA) framework to implement SCN model for the first time. In addition, in order to improve the performance of the SCN model based on the FPGA, the implementation of the nonlinear activation function on the FPGA is optimized, which effectively improves the prediction accuracy while considering the utilization rate of hardware resources. Experimental results based on the simulation data set and the real data set prove that the proposed FPGA framework successfully implements the SCN regression prediction model, and the improved SCN model has higher accuracy and a more stable performance. Compared with the extreme learning machine (ELM), the prediction performance of the proposed SCN implementation model based on the FPGA for the simulation data set and the real data set is improved by 56.37% and 17.35%, respectively.</p>
<p>基于数字电路的神经网络回归预测的实现是机器学习和认知识别领域的挑战性问题之一，也是智能时代缓解互联网压力的有效途径。作为一种非线性网络，随机配置网络（SCN）因其在学习和泛化方面的良好性能而被认为是一种有效的回归预测方法。因此，在本文中，我们将 SCN 应用于回归分析，并首次设计并验证了现场可编程门阵列 (FPGA) 框架来实现 SCN 模型。此外，为了提高基于FPGA的SCN模型的性能，对非线性激活函数在FPGA上的实现进行了优化，在考虑硬件资源利用率的同时有效提高了预测精度。基于仿真数据集和真实数据集的实验结果证明，所提出的FPGA框架成功实现了SCN回归预测模型，改进后的SCN模型具有更高的精度和更稳定的性能。与极限学习机（ELM）相比，所提出的基于FPGA的SCN实现模型对仿真数据集和真实数据集的预测性能分别提高了56.37%和17.35%。</p>
<h2 id="Keywords-Index-Terms-7"><a href="#Keywords-Index-Terms-7" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-8"><a href="#Comments-8" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-8"><a href="#References-8" class="headerlink" title="References"></a>References</h2><h1 id="Paper22-1116"><a href="#Paper22-1116" class="headerlink" title="Paper22-1116"></a>Paper22-1116</h1><h2 id="Published-in-9"><a href="#Published-in-9" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>S.R. Chiluveru, M. Tripathy, B. Mohapatra, “Accuracy controlled iterative method for efficient sigmoid function approximation”, <em>Electronics Letters</em>, 2020.</p>
<p>IEEE 404了，不知道咋回事</p>
<h2 id="Title-9"><a href="#Title-9" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/el.2020.0854">Accuracy controlled iterative method for efficient sigmoid function approximation</a></p>
<h2 id="Authors-9"><a href="#Authors-9" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ietresearch.onlinelibrary.wiley.com/action/doSearch?ContribAuthorRaw=Chiluveru%2C+SR">S.R. Chiluveru</a></p>
<p><a target="_blank" rel="noopener" href="https://ietresearch.onlinelibrary.wiley.com/action/doSearch?ContribAuthorRaw=Tripathy%2C+M">M. Tripathy</a></p>
<p><a target="_blank" rel="noopener" href="https://ietresearch.onlinelibrary.wiley.com/action/doSearch?ContribAuthorRaw=Mohapatra%2C+B">B. Mohapatra</a></p>
<h2 id="Abstract-9"><a href="#Abstract-9" class="headerlink" title="Abstract"></a>Abstract</h2><p>The sigmoid activation function plays an essential role in implementing neural networks in hardware. However, due to a high degree of non-linearity, the hardware implementation of the activation function itself is a daunting job. In this Letter, the authors proposed an accuracy controlled iterative algorithm for the piecewise linear approximation of the sigmoid function. The proposed algorithm approximates sigmoid activation with an optimised number of linear functions for a given error. The accuracy and hardware complexity of the resulting approximation function are compared with the state-of-the-art literature.</p>
<p>sigmoid 激活函数在硬件中实现神经网络中起着至关重要的作用。 然而，由于高度非线性，激活函数本身的硬件实现是一项艰巨的工作。 在本文中，作者为 sigmoid 函数的分段线性逼近提出了一种精度控制迭代算法。 对于给定的误差，所提出的算法使用优化数量的线性函数来近似 sigmoid 激活。 所得近似函数的准确性和硬件复杂性与最先进的文献进行了比较。</p>
<h2 id="Keywords-Index-Terms-8"><a href="#Keywords-Index-Terms-8" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-9"><a href="#Comments-9" class="headerlink" title="Comments"></a>Comments</h2><p>还是那几个人。Paper26 25 22都是差不多的人做的。这篇的数据很像Paper25的，图也非常像。可能是journal和conference？</p>
<h2 id="References-9"><a href="#References-9" class="headerlink" title="References"></a>References</h2><h1 id="Paper21-1118"><a href="#Paper21-1118" class="headerlink" title="Paper21-1118"></a>Paper21-1118</h1><h2 id="Published-in-10"><a href="#Published-in-10" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Linyu Wei, Jueping Cai, Vantruong Nguyen, Jie Chu, Kailin Wen, “P-SFA: Probability based Sigmoid Function Approximation for Low-complexity Hardware Implementation”, <em>Microprocessors and Microsystems</em>, pp. 103105, 2020.</p>
<h2 id="Title-10"><a href="#Title-10" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726">P-SFA: Probability based Sigmoid Function Approximation for Low-complexity Hardware Implementation</a></p>
<h2 id="Authors-10"><a href="#Authors-10" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726#!">Linyu Wei</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726#!">Jueping Cai</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726#!">Vantruong Nguyen</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726#!">Jie Chu</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0141933120302726#!">Kailin Wen</a></p>
<h2 id="Abstract-10"><a href="#Abstract-10" class="headerlink" title="Abstract"></a>Abstract</h2><p>A probability-based sigmoid function approximation (P-SFA), which is based on piecewise linear function and neuron’s values statistical probability distribution in each layer, is proposed to lower the complexity of neural network hardware implementation with only addition circuit. The sigmoid function is divided into three fixed regions, and the number of sub-regions with different sizes in each fixed region is adapted to neuron’s values distribution in each layer to reduce the error between the sigmoid function and P-SFA function. The experimental results on FPGA show that the P-SFA function is efficient in terms of power and speed, and the recognition accuracies in DNN and CNN for MNIST with P-SFA are the highest among the state-of-the-art methods, up to 97.46% and 99.02% respectively.</p>
<p>为了降低神经网络硬件实现的复杂度，仅用加法电路，提出了一种基于分段线性函数和神经元值统计概率分布的基于概率的sigmoid函数逼近（P-SFA）。 sigmoid 函数被划分为三个固定区域，每个固定区域中不同大小的子区域的数量适应每层神经元的值分布，以减少 sigmoid 函数和 P-SFA 函数之间的误差。 在 FPGA 上的实验结果表明，P-SFA 函数在功率和速度方面是高效的，并且在 DNN 和 CNN 中对带有 P-SFA 的 MNIST 的识别准确率是最先进的方法中最高的，高达 分别为 97.46% 和 99.02%。</p>
<h2 id="Keywords-Index-Terms-9"><a href="#Keywords-Index-Terms-9" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>Sigmoid function, Neural networks, Piecewise linear function</p>
<h2 id="Comments-10"><a href="#Comments-10" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-10"><a href="#References-10" class="headerlink" title="References"></a>References</h2><h1 id="Paper20-1119"><a href="#Paper20-1119" class="headerlink" title="Paper20-1119"></a>Paper20-1119</h1><h2 id="Published-in-11"><a href="#Published-in-11" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Xuhui Yang, Qingguo Zhou, Jinqiang Wang, Lihong Han, Fang Feng, Rui Zhou, Kuan-Ching Li, “FPGA-based approximate calculation system of General Vector Machine”, <em>Microelectronics Journal</em>, 2019.</p>
<h2 id="Title-11"><a href="#Title-11" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918">FPGA-based approximate calculation system of General Vector Machine</a></p>
<h2 id="Authors-11"><a href="#Authors-11" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Xuhui Yang</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Qingguo Zhou</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Jinqiang Wang</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Lihong Han</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Fang Feng</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Rui Zhou</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0026269218306918#!">Kuan-Ching Li</a></p>
<h2 id="Abstract-11"><a href="#Abstract-11" class="headerlink" title="Abstract"></a>Abstract</h2><p>In this paper, a General Vector Machine (GVM) approximate calculation system based on FPGA is introduced. The author designed a parallel computing architecture of GVM on FPGA, discussed a matrix parallel computing method, developed approximation calculation methods of sigmoid function and softmax layer on FPGA. As an example, the paper implemented GVM on FPGA to identify MNIST data sets and tested the recognition rate with 14 different data accuracy of parameters, then gave some suggestions for data accuracy selection for parameters. Finally, the approximate calculation system is implemented and tested on XCKU3P, XC7Z020, XC7VX690 and XCUV190 FPGA chips. The results demonstrate that the computation speed is 112 times higher than CPU (Intel Core(TM) i9-7900X), and the performance can be equivalent to the GPU (NVIDIA GTX 1080 Ti). The approximate calculation system can effectively accelerate the calculation of GVM. It has the characteristics of good acceleration, and is suitable for embedded intelligent devices.</p>
<p>本文介绍了一种基于FPGA的通用向量机(GVM)近似计算系统。作者在FPGA上设计了GVM并行计算架构，讨论了矩阵并行计算方法，开发了FPGA上sigmoid函数和softmax层的近似计算方法。例如，论文在FPGA上实现GVM对MNIST数据集进行识别，并测试了14种不同数据精度参数的识别率，然后给出了参数数据精度选择的一些建议。最后，在XCKU3P、XC7Z020、XC7VX690和XCUV190 FPGA芯片上实现并测试了近似计算系统。结果表明，计算速度比CPU（Intel Core(TM) i9-7900X）高112倍，性能可与GPU（NVIDIA GTX 1080 Ti）相当。近似计算系统可以有效加速GVM的计算。具有加速性好等特点，适用于嵌入式智能设备。</p>
<h2 id="Keywords-Index-Terms-10"><a href="#Keywords-Index-Terms-10" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-11"><a href="#Comments-11" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-11"><a href="#References-11" class="headerlink" title="References"></a>References</h2><h1 id="Paper19-1120"><a href="#Paper19-1120" class="headerlink" title="Paper19-1120"></a>Paper19-1120</h1><h2 id="Published-in-12"><a href="#Published-in-12" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Samba Raju Chiluveru, Gyanendra, Snehit Chunarkar, Manoj Tripathy, Brajesh Kumar Kaushik, “Efficient Hardware Implementation of DNN-Based Speech Enhancement Algorithm With Precise Sigmoid Activation Function”, <em>Circuits and Systems II: Express Briefs IEEE Transactions on</em>, vol. 68, no. 11, pp. 3461-3465, 2021.</p>
<h2 id="Title-12"><a href="#Title-12" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9439489">Efficient Hardware Implementation of DNN-Based Speech Enhancement Algorithm With Precise Sigmoid Activation Function</a></p>
<h2 id="Authors-12"><a href="#Authors-12" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088523310">Samba Raju Chiluveru</a></p>
<p>Instrumentation and Signal Processing Group, Indian Institute of Technology Roorkee, Roorkee, India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088563152">Gyanendra</a></p>
<p>Microelectronics and VLSI Group, Indian Institute of Technology Roorkee, Roorkee, India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37089005346">Snehit Chunarkar</a></p>
<p>Instrumentation and Signal Processing Group, Indian Institute of Technology Roorkee, Roorkee, India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37301125900">Manoj Tripathy</a></p>
<p>Instrumentation and Signal Processing Group, Indian Institute of Technology Roorkee, Roorkee, India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37394845100">Brajesh Kumar Kaushik</a></p>
<p>Microelectronics and VLSI Group, Indian Institute of Technology Roorkee, Roorkee, India</p>
<h2 id="Abstract-12"><a href="#Abstract-12" class="headerlink" title="Abstract"></a>Abstract</h2><p>This brief presents the hardware implementation of deep neural network-based speech enhancement algorithm (DNN-SEA) with a precise sigmoid activation function. Further, an adaptive step-size-based slope and intercept method (AS-SIM) has been developed to approximate the sigmoid function that uses the maximum allowable error (ϵ) as an input parameter. The performance of the DNN-SEA is measured in terms of speech quality and intelligibility using a perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI) parameters, respectively. Hardware implementation is performed by Synopsys Design Compiler with a TSMC 90-nm library. The performance comparison has been conducted in terms of area, gate count, delay, and power consumption. Results confirm that the proposed AS-SIM approximation improves performance in terms of PESQ and STOI value and significantly reduces the area requirement and power consumption.</p>
<p>本简介介绍了具有精确 sigmoid 激活函数的基于深度神经网络的语音增强算法 (DNN-SEA) 的硬件实现。 此外，已经开发了一种基于自适应步长的斜率和截距方法 (AS-SIM) 来近似使用最大允许误差 (ϵ) 作为输入参数的 sigmoid 函数。 DNN-SEA 的性能分别使用语音质量感知评估 (PESQ) 和短时客观可懂度 (STOI) 参数在语音质量和可懂度方面进行测量。 硬件实现由 Synopsys 设计编译器使用 TSMC 90-nm 库执行。 在面积、门数、延迟和功耗方面进行了性能比较。 结果证实，所提出的 AS-SIM 近似提高了 PESQ 和 STOI 值方面的性能，并显着降低了面积要求和功耗。</p>
<h2 id="Keywords-Index-Terms-11"><a href="#Keywords-Index-Terms-11" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-12"><a href="#Comments-12" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-12"><a href="#References-12" class="headerlink" title="References"></a>References</h2><h1 id="Paper18-1121"><a href="#Paper18-1121" class="headerlink" title="Paper18-1121"></a>Paper18-1121</h1><h2 id="Published-in-13"><a href="#Published-in-13" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Fatemeh Mohammadi Shakiba, MengChu Zhou, “Novel Analog Implementation of a Hyperbolic Tangent Neuron in Artificial Neural Networks”, <em>Industrial Electronics IEEE Transactions on</em>, vol. 68, no. 11, pp. 10856-10867, 2021.</p>
<h2 id="Title-13"><a href="#Title-13" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9248592">Novel Analog Implementation of a Hyperbolic Tangent Neuron in Artificial Neural Networks</a></p>
<h2 id="Authors-13"><a href="#Authors-13" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088922980">Fatemeh Mohammadi Shakiba</a></p>
<p>Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA</p>
<p>Fatemeh Mohammadi Shakiba (Graduate Student Member, IEEE) was born in Tehran, Iran. She received the M.S. degree in computer engineering from Southern Illinois University in Carbondale, Carbondale, IL, USA, in 2018. She is currently working toward the Ph.D. degree in computer engineering with New Jersey Institute of Technology, Newark, NJ, USA.</p>
<p>Her research interests include artificial neural networks, machine learning, deep learning, data classification, and data analysis.</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37273591600">MengChu Zhou</a></p>
<p>Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA</p>
<p>Institute of Systems Engineering and Collaborative Laboratory for Intelligent Science and Systems, Macau University of Science and Technology, Macau, China</p>
<p>Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China</p>
<p>MengChu Zhou (Fellow, IEEE) received the B.S. degree in control engineering from the Nanjing University of Science and Technology, Nanjing, China, in 1983, the M.S. degree in automatic control from the Beijing Institute of Technology, Beijing, China, in 1986, and the Ph.D. degree in computer and systems engineering from Rensselaer Polytechnic Institute, Troy, NY, USA, in 1990.</p>
<p>He is a Distinguished Professor of New Jersey Institute of Technology. He has authored or coauthored 900 publications including 12 books, 600+ journal papers (500+ in IEEE transactions), 27 patents and 29 book-chapters. His interests interests include Petri nets, intelligent automation, and big data.</p>
<p>Prof. Zhou is the founding Editor of IEEE Press Book Series on Systems Science and Engineering and Editor-in-Chief of IEEE/CAA Journal of Automatica Sinica. He is Fellow of IFAC, AAAS, CAA, and NAI.</p>
<h2 id="Abstract-13"><a href="#Abstract-13" class="headerlink" title="Abstract"></a>Abstract</h2><p>Recently, enormous datasets have made power dissipation and area usage lie at the heart of designs for artificial neural networks (ANNs). Considering the significant role of activation functions in neurons and the growth of hardware-based neural networks like memristive neural networks, this work proposes a novel design for a hyperbolic tangent activation function (Tanh) to be used in memristive-based neuromorphic architectures. The purpose of implementing a CMOS-based design for Tanh is to decrease power dissipation and area usage. This design also increases the overall speed of computation in ANNs, while keeping the accuracy in an acceptable range. The proposed design is one of the first analog designs for the hyperbolic tangent and its performance is analyzed by using two well-known datsets, including the Modified National Institute of Standards and Technology (MNIST) and Fashion-MNIST. The direct implementation of the proposed design for Tanh is proposed and investigated via software and hardware modeling.</p>
<p>最近，大量数据集使功耗和面积使用成为人工神经网络 (ANN) 设计的核心。考虑到激活函数在神经元中的重要作用以及基于硬件的神经网络（如忆阻神经网络）的增长，这项工作提出了一种用于基于忆阻的神经形态架构的双曲正切激活函数 (Tanh) 的新设计。为 Tanh 实施基于 CMOS 的设计的目的是降低功耗和面积使用。这种设计还提高了人工神经网络的整体计算速度，同时将精度保持在可接受的范围内。所提议的设计是最早的双曲正切模拟设计之一，其性能通过使用两个众所周知的数据集进行分析，包括修改后的国家标准与技术研究所 (MNIST) 和 Fashion-MNIST。通过软件和硬件建模提出并研究了 Tanh 所提议设计的直接实现。</p>
<h2 id="Keywords-Index-Terms-12"><a href="#Keywords-Index-Terms-12" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-13"><a href="#Comments-13" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-13"><a href="#References-13" class="headerlink" title="References"></a>References</h2><h1 id="Paper17-1122"><a href="#Paper17-1122" class="headerlink" title="Paper17-1122"></a>Paper17-1122</h1><h2 id="Published-in-14"><a href="#Published-in-14" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Muhammad Awais Hussain, Tsung-Han Tsai, “An Efficient and Fast Softmax Hardware Architecture (EFSHA) for Deep Neural Networks”, <em>Artificial Intelligence Circuits and Systems (AICAS) 2021 IEEE 3rd International Conference on</em>, pp. 1-4, 2021.</p>
<h2 id="Title-14"><a href="#Title-14" class="headerlink" title="Title"></a>Title</h2><p>[An Efficient and Fast Softmax Hardware Architecture (EFSHA) for Deep Neural Networks]9<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9458541">https://ieeexplore.ieee.org/abstract/document/9458541</a></p>
<h2 id="Authors-14"><a href="#Authors-14" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086446904">Muhammad Awais Hussain</a></p>
<p>National Central University, Zhongli, Taiwan</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085650268">Tsung-Han Tsai</a></p>
<p>National Central University, Zhongli, Taiwan</p>
<h2 id="Abstract-14"><a href="#Abstract-14" class="headerlink" title="Abstract"></a>Abstract</h2><p>Deep neural networks are widely used in computer vision applications due to their high performance. However, DNNs involve a large number of computations in the training and inference phase. Among the different layers of a DNN, the softmax layer has one of the most complex computations as it involves exponent and division operations. So, a hardware-efficient implementation is required to reduce the on-chip resources. In this paper, we propose a new hardware-efficient and fast implementation of the softmax activation function. The proposed hardware implementation consumes fewer hardware resources and works at high speed as compared to the state-of-the-art techniques.</p>
<p>深度神经网络由于其高性能而广泛用于计算机视觉应用。 然而，DNN 在训练和推理阶段涉及大量计算。 在 DNN 的不同层中，softmax 层具有最复杂的计算之一，因为它涉及指数和除法运算。 因此，需要一种硬件高效的实现来减少片上资源。 在本文中，我们提出了一种新的硬件高效且快速的 softmax 激活函数实现。 与最先进的技术相比，所提出的硬件实现消耗更少的硬件资源并高速工作。</p>
<h2 id="Keywords-Index-Terms-13"><a href="#Keywords-Index-Terms-13" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-14"><a href="#Comments-14" class="headerlink" title="Comments"></a>Comments</h2><p>这篇文章的结果可能可以作为对比。相对友好。</p>
<h2 id="References-14"><a href="#References-14" class="headerlink" title="References"></a>References</h2><h1 id="Paper16-1125"><a href="#Paper16-1125" class="headerlink" title="Paper16-1125"></a>Paper16-1125</h1><h2 id="Published-in-15"><a href="#Published-in-15" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Mahmoud Masadeh, Osman Hasan, Sofiène Tahar, “Machine-Learning-Based Self-Tunable Design of Approximate Computing”, <em>Very Large Scale Integration (VLSI) Systems IEEE Transactions on</em>, vol. 29, no. 4, pp. 800-813, 2021.</p>
<h2 id="Title-15"><a href="#Title-15" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9360869">Machine-Learning-Based Self-Tunable Design of Approximate Computing</a></p>
<h2 id="Authors-15"><a href="#Authors-15" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085711380">Mahmoud Masadeh</a></p>
<p>Concordia University, Montreal, Canada</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37601575700">Osman Hasan</a></p>
<p>Concordia University, Montreal, Canada</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37277153600">Sofiène Tahar</a></p>
<p>Concordia University, Montreal, Canada</p>
<h2 id="Abstract-15"><a href="#Abstract-15" class="headerlink" title="Abstract"></a>Abstract</h2><p>Approximate computing (AC) is an emerging computing paradigm suitable for intrinsic error-tolerant applications to reduce energy consumption and execution time. Different approximate techniques and designs, at both hardware and software levels, have been proposed and demonstrated the effectiveness of relaxing the average output quality constraint. However, the output quality of AC is highly input-dependent, i.e., for some input data, the output errors may reach unacceptable levels. Therefore, there is a dire need for an input-dependent tunable approximate design. With this motivation, in this article, we propose a lightweight and efficient machine-learning-based approach to build an input-aware design selector, i.e., quality controller, to adapt the approximate design in order to meet the target output quality (TOQ). For illustration purposes, we use a library of 8-bit and 16-bit energy-efficient approximate array multipliers with 20 different settings, which are commonly used in image and audio processing applications. The simulation results, based on two sets of images, including an 8 Scene Categories Dataset, which is a benchmark of images data set, demonstrate the effectiveness of the lightweight selector where the proposed tunable design achieves a significant reduction in quality loss with relatively low overhead.</p>
<p>近似计算 (AC) 是一种新兴的计算范式，适用于内在容错应用程序，以减少能耗和执行时间。已经在硬件和软件级别提出了不同的近似技术和设计，并证明了放宽平均输出质量约束的有效性。然而，AC 的输出质量高度依赖于输入，即对于某些输入数据，输出误差可能达到不可接受的水平。因此，迫切需要一种依赖于输入的可调近似设计。出于这个动机，在本文中，我们提出了一种轻量级且高效的基于机器学习的方法来构建输入感知设计选择器，即质量控制器，以适应近似设计以满足目标输出质量 (TOQ) .出于说明目的，我们使用了 8 位和 16 位节能近似阵列乘法器库，它们具有 20 种不同的设置，这些乘法器通常用于图像和音频处理应用程序。模拟结果基于两组图像，包括 8 个场景类别数据集，这是图像数据集的基准，证明了轻量级选择器的有效性，其中所提出的可调设计以相对较低的开销实现了质量损失的显着降低.</p>
<h2 id="Keywords-Index-Terms-14"><a href="#Keywords-Index-Terms-14" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-15"><a href="#Comments-15" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大。</p>
<h2 id="References-15"><a href="#References-15" class="headerlink" title="References"></a>References</h2><h1 id="Paper15"><a href="#Paper15" class="headerlink" title="Paper15"></a>Paper15</h1><h2 id="Published-in-16"><a href="#Published-in-16" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Wen-Chang Yang, Shu-Yun Lin, Tsung-Chu Huang, “Range-Lookup Approximate Computing Acceleration for Any Activation Functions in Low-Power Neural Network”, <em>Consumer Electronics - Taiwan (ICCE-Taiwan) 2020 IEEE International Conference on</em>, pp. 1-2, 2020.</p>
<h2 id="Title-16"><a href="#Title-16" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9258057">Range-Lookup Approximate Computing Acceleration for Any Activation Functions in Low-Power Neural Network</a></p>
<h2 id="Authors-16"><a href="#Authors-16" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088560962">Wen-Chang Yang</a></p>
<p>National Changhua University of Education, Changhua, Taiwan</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088564479">Shu-Yun Lin</a></p>
<p>National Changhua University of Education, Changhua, Taiwan</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/38183099400">Tsung-Chu Huang</a></p>
<p>National Changhua University of Education, Changhua, Taiwan</p>
<h2 id="Abstract-16"><a href="#Abstract-16" class="headerlink" title="Abstract"></a>Abstract</h2><p>Consumer electronics have become versatile for processing a lot of signals in any distribution. This results in that high-speed activating of neural network should fit for any distribution of error functions. In this paper, we propose a set of transistor-level magnitude-comparators. Then we apply them to develop range-addressable memory to design a lightweight-slope lookup table. We then develop an efficient algorithm for constructing a lightweight-slope piecewise line. The proposed techniques are suitable to design any sigmoidal activation functions in neural network. From experiments and comparisons, the proposed LUT can be more efficient effective than previous looking tables. Especially applied in the proposed range-addressable memory, the power-delay product can be reduced by more than 30 folds.</p>
<p>消费电子产品已成为处理任何分布中的大量信号的通用工具。 这导致神经网络的高速激活应该适合误差函数的任何分布。 在本文中，我们提出了一组晶体管级幅度比较器。 然后我们应用它们来开发范围可寻址内存来设计一个轻量级斜率查找表。 然后，我们开发了一种有效的算法来构建轻量级斜率分段线。 所提出的技术适用于设计神经网络中的任何 sigmoidal 激活函数。 从实验和比较来看，提议的 LUT 可以比以前的查找表更有效。 特别是应用在提出的范围可寻址存储器中，功率延迟积可以减少30倍以上。</p>
<h2 id="Keywords-Index-Terms-15"><a href="#Keywords-Index-Terms-15" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-16"><a href="#Comments-16" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-16"><a href="#References-16" class="headerlink" title="References"></a>References</h2><h1 id="Paper14"><a href="#Paper14" class="headerlink" title="Paper14"></a>Paper14</h1><h2 id="Published-in-17"><a href="#Published-in-17" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Hui Chen, Lin Jiang, Yuanyong Luo, Zhonghai Lu, Yuxiang Fu, Li Li, Zongguang Yu, “A CORDIC-Based Architecture with Adjustable Precision and Flexible Scalability to Implement Sigmoid and Tanh Functions”, <em>Circuits and Systems (ISCAS) 2020 IEEE International Symposium on</em>, pp. 1-5, 2020.</p>
<h2 id="Title-17"><a href="#Title-17" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9180864">A CORDIC-Based Architecture with Adjustable Precision and Flexible Scalability to Implement Sigmoid and Tanh Functions</a></p>
<h2 id="Authors-17"><a href="#Authors-17" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088540313">Hui Chen</a></p>
<p>Nanjing University, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088931162">Lin Jiang</a></p>
<p>Nanjing University, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086493094">Yuanyong Luo</a></p>
<p>Nanjing University, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088687446">Zhonghai Lu</a></p>
<p>KTH Royal Institute of Technology, Stockholm, Sweden</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085833583">Yuxiang Fu</a></p>
<p>Nanjing University, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085682466">Li Li</a></p>
<p>Nanjing University, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088687271">Zongguang Yu</a></p>
<p>Nanjing University, Nanjing, China</p>
<h2 id="Abstract-17"><a href="#Abstract-17" class="headerlink" title="Abstract"></a>Abstract</h2><p>In the artificial neural networks, tanh (hyperbolic tangent) and sigmoid functions are widely used as activation functions. Past methods to compute them may have shortcomings such as low precision or inflexible architecture that is difficult to expand, so we propose a CORDIC-based architecture to implement sigmoid and tanh functions, which has adjustable precision and flexible scalability. It just needs shift-add-or-subtract operations to compute high-accuracy results and is easy to expand the input range through scaling the negative iterations of CORDIC without changing the original architecture. We adopt the control variable method to explore the accuracy distribution through software simulation. A specific case (ARCH. (1, 15, 18), RMSE: 10 -6 ) is designed and synthesized under the TSMC 40nm CMOS technology, the report shows that it has the area of 36512.78μm 2 and power of 12.35mW at the frequency of 1GHz. The maximum work frequency can reach 1.5GHz, which is better than the state-of-the-art methods.</p>
<p>在人工神经网络中，tanh（双曲正切）和 sigmoid 函数被广泛用作激活函数。过去的计算方法可能存在精度低或架构不灵活、难以扩展等缺点，因此我们提出了一种基于 CORDIC 的架构来实现 sigmoid 和 tanh 函数，该架构具有可调节的精度和灵活的可扩展性。它只需要移位加减运算即可计算出高精度结果，并且在不改变原始架构的情况下，可以通过缩放 CORDIC 的负迭代轻松扩展输入范围。我们采用控制变量法，通过软件仿真探索精度分布。一个具体案例（ARCH. (1, 15, 18), RMSE: 10 -6 ）是在台积电40nm CMOS工艺下设计合成的，报告显示其面积为36512.78μm 2 ，功率为12.35mW频率为 1GHz。最高工作频率可达1.5GHz，优于最先进的方法。</p>
<h2 id="Keywords-Index-Terms-16"><a href="#Keywords-Index-Terms-16" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-17"><a href="#Comments-17" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-17"><a href="#References-17" class="headerlink" title="References"></a>References</h2><h1 id="Paper13"><a href="#Paper13" class="headerlink" title="Paper13"></a>Paper13</h1><h2 id="Published-in-18"><a href="#Published-in-18" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Yue Gao, Weiqiang Liu, Fabrizio Lombardi, “Design and Implementation of an Approximate Softmax Layer for Deep Neural Networks”, <em>Circuits and Systems (ISCAS) 2020 IEEE International Symposium on</em>, pp. 1-5, 2020.</p>
<h2 id="Title-18"><a href="#Title-18" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9180870">Design and Implementation of an Approximate Softmax Layer for Deep Neural Networks</a></p>
<h2 id="Authors-18"><a href="#Authors-18" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088932960">Yue Gao</a></p>
<p>Nanjing University of Aeronautics and Astronautics, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085338613">Weiqiang Liu</a></p>
<p>Nanjing University of Aeronautics and Astronautics, Nanjing, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37279999000">Fabrizio Lombardi</a></p>
<p>Northeastern University, Boston, MA, USA</p>
<h2 id="Abstract-18"><a href="#Abstract-18" class="headerlink" title="Abstract"></a>Abstract</h2><p>Deep neural networks (DNNs) have been widely used in classification due to their high accuracy. The softmax function is one of the important non-linear functions in DNNs. Therefore, high performance and efficient hardware design are sought. However, the improvement of the softmax function is difficult because the exponent and the division units are complex. In this paper, we propose new approximate hardware architectures for both the exponent and the division units. Compared with the state-of-the-art designs, the proposed approximate softmax design consumes significantly less resources and also achieves high performance while maintaining a very high accuracy.</p>
<p>深度神经网络（DNN）由于其高精度而被广泛用于分类。 softmax 函数是 DNN 中重要的非线性函数之一。 因此，寻求高性能和高效的硬件设计。 但是softmax函数的改进难度很大，因为指数和除法单位比较复杂。 在本文中，我们为指数和除法单元提出了新的近似硬件架构。 与最先进的设计相比，所提出的近似 softmax 设计消耗的资源明显更少，并且在保持非常高的精度的同时还实现了高性能。</p>
<h2 id="Keywords-Index-Terms-17"><a href="#Keywords-Index-Terms-17" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-18"><a href="#Comments-18" class="headerlink" title="Comments"></a>Comments</h2><p>可以作为对比。同时好几篇文章都在提softmax，有必要考虑一下PQAC的softmax性能。</p>
<h2 id="References-18"><a href="#References-18" class="headerlink" title="References"></a>References</h2><h1 id="Paper12-1109"><a href="#Paper12-1109" class="headerlink" title="Paper12-1109"></a>Paper12-1109</h1><h2 id="Published-in-19"><a href="#Published-in-19" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>T.K.R Arvind, Marcel Brand, Christian Heidorn, Srinivas Boppu, Frank Hannig, Jürgen Teich, “Hardware Implementation of Hyperbolic Tangent Activation Function for Floating Point Formats”, <em>VLSI Design and Test (VDAT) 2020 24th International Symposium on</em>, pp. 1-6, 2020.</p>
<h2 id="Title-19"><a href="#Title-19" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9190305">Hardware Implementation of Hyperbolic Tangent Activation Function for Floating Point Formats</a></p>
<h2 id="Authors-19"><a href="#Authors-19" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088503670">T.K.R Arvind</a></p>
<p>School of Electrical Sciences, Indian Institute of Technology Bhubaneswar (IITBBS), India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086153198">Marcel Brand</a></p>
<p>Hardware/Software Co-Design, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37088368833">Christian Heidorn</a></p>
<p>Hardware/Software Co-Design, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/38296883500">Srinivas Boppu</a></p>
<p>School of Electrical Sciences, Indian Institute of Technology Bhubaneswar (IITBBS), India</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37266978900">Frank Hannig</a></p>
<p>Hardware/Software Co-Design, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37276355800">Jürgen Teich</a></p>
<p>Hardware/Software Co-Design, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany</p>
<h2 id="Abstract-19"><a href="#Abstract-19" class="headerlink" title="Abstract"></a>Abstract</h2><p>In this paper, we present the efficient hardware implementation of hyperbolic tangent activation function, which is most widely used in artificial neural networks for accelerating machine learning applications. The proposed design considers the floating point representation of numbers for the first time, the nonlinear nature of the activation function while sampling, and uses a lookup table for implementation. The unique way of dividing the input range into bins which follows the binary pattern reduces the hardware implementation cost. Furthermore, the input data itself is used as the address for lookup table; thus, no extra cost involved in hashing the lookup table and involves only one memory access time resulting in faster and efficient hardware implementation. Our design proves to be 3× faster when compared to similar hardware implementations using CMOS 90 nm process.</p>
<p>在本文中，我们介绍了双曲正切激活函数的高效硬件实现，该函数在人工神经网络中最广泛用于加速机器学习应用。 所提出的设计首次考虑了数字的浮点表示，采样时激活函数的非线性特性，并使用查找表来实现。 将输入范围划分为遵循二进制模式的 bin 的独特方式降低了硬件实现成本。 此外，输入数据本身用作查找表的地址； 因此，散列查找表不涉及额外成本，并且只涉及一次内存访问时间，从而实现更快、更高效的硬件实现。 与使用 CMOS 90 nm 工艺的类似硬件实现相比，我们的设计被证明要快 3 倍。</p>
<h2 id="Keywords-Index-Terms-18"><a href="#Keywords-Index-Terms-18" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-19"><a href="#Comments-19" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-19"><a href="#References-19" class="headerlink" title="References"></a>References</h2><h1 id="Paper11"><a href="#Paper11" class="headerlink" title="Paper11"></a>Paper11</h1><h2 id="Published-in-20"><a href="#Published-in-20" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Tao Yang, Yadong Wei, Zhijun Tu, Haolun Zeng, Michel A. Kinsy, Nanning Zheng, Pengju Ren, “Design Space Exploration of Neural Network Activation Function Circuits”, <em>Computer-Aided Design of Integrated Circuits and Systems IEEE Transactions on</em>, vol. 38, no. 10, pp. 1974-1978, 2019.</p>
<h2 id="Title-20"><a href="#Title-20" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8467987/">Design Space Exploration of Neural Network Activation Function Circuits</a></p>
<h2 id="Authors-20"><a href="#Authors-20" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37087012027">Tao Yang</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086610958">Yadong Wei</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37087010777">Zhijun Tu</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37087010928">Haolun Zeng</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37318978700">Michel A. Kinsy</a></p>
<p>Department of Electrical and Computer Engineering, Boston University, Boston, MA, USA</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37271536700">Nanning Zheng</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37069213800">Pengju Ren</a></p>
<p>Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China</p>
<h2 id="Abstract-20"><a href="#Abstract-20" class="headerlink" title="Abstract"></a>Abstract</h2><p>The widespread application of artificial neural networks has prompted researchers to experiment with field-programmable gate array and customized ASIC designs to speed up their computation. These implementation efforts have generally focused on weight multiplication and signal summation operations, and less on activation functions used in these applications. Yet, efficient hardware implementations of nonlinear activation functions like exponential linear units (ELU), scaled ELU (SELU), and hyperbolic tangent (tanh), are central to designing effective neural network accelerators, since these functions require lots of resources. In this paper, we explore efficient hardware implementations of activation functions using purely combinational circuits, with a focus on two widely used nonlinear activation functions, i.e., SELU and tanh. Our experiments demonstrate that neural networks are generally insensitive to the precision of the activation function. The results also prove that the proposed combinational circuit-based approach is very efficient in terms of speed and area, with negligible accuracy loss on the MNIST, CIFAR-10, and IMAGE NET benchmarks. Synopsys design compiler synthesis results show that circuit designs for tanh and SELU can save between ×3.13∼×7.69 and ×4.45∼×8.45 area compared to the look-up table/memory-based implementations, and can operate at 5.14 GHz and 4.52 GHz using the 28-nm SVT library, respectively. The implementation is available at: <a target="_blank" rel="noopener" href="https://github.com/ThomasMrY/ActivationFunctionDemo">https://github.com/ThomasMrY/ActivationFunctionDemo</a>.</p>
<p>人工神经网络的广泛应用促使研究人员尝试现场可编程门阵列和定制 ASIC 设计，以加快计算速度。这些实现工作通常集中在权重乘法和信号求和操作上，较少关注这些应用程序中使用的激活函数。然而，非线性激活函数的高效硬件实现，如指数线性单元 (ELU)、缩放 ELU (SELU) 和双曲正切 (tanh)，对于设计有效的神经网络加速器至关重要，因为这些函数需要大量资源。在本文中，我们探索使用纯组合电路的激活函数的有效硬件实现，重点是两个广泛使用的非线性激活函数，即 SELU 和 tanh。我们的实验表明，神经网络通常对激活函数的精度不敏感。结果还证明，所提出的基于组合电路的方法在速度和面积方面非常有效，在 MNIST、CIFAR-10 和 IMAGE NET 基准上的精度损失可以忽略不计。 Synopsys 设计编译器综合结果表明，与基于查找表/内存的实现相比，tanh 和 SELU 的电路设计可以节省 ×3.13∼×7.69 和 ×4.45∼×8.45 之间的面积，并且可以在 5.14 GHz 和 4.52 GHz 下运行分别使用 28-nm SVT 库。该实现可从以下网址获得：<a target="_blank" rel="noopener" href="https://github.com/ThomasMrY/ActivationFunctionDemo。">https://github.com/ThomasMrY/ActivationFunctionDemo。</a></p>
<h2 id="Keywords-Index-Terms-19"><a href="#Keywords-Index-Terms-19" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-20"><a href="#Comments-20" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-20"><a href="#References-20" class="headerlink" title="References"></a>References</h2><h1 id="Paper10"><a href="#Paper10" class="headerlink" title="Paper10"></a>Paper10</h1><h2 id="Published-in-21"><a href="#Published-in-21" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Leandro D. Medus, Taras Iakymchuk, Jose Vicente Frances-Villora, Manuel Bataller-Mompeán, Alfredo Rosado-Muñoz, “A Novel Systolic Parallel Hardware Architecture for the FPGA Acceleration of Feedforward Neural Networks”, <em>Access IEEE</em>, vol. 7, pp. 76084-76103, 2019.</p>
<h2 id="Title-21"><a href="#Title-21" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8731886">A Novel Systolic Parallel Hardware Architecture for the FPGA Acceleration of Feedforward Neural Networks</a></p>
<h2 id="Authors-21"><a href="#Authors-21" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086863367">Leandro D. Medus</a></p>
<p>Group for Processing and Digital Design, Universitat de Valencia, Burjassot, Spain</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/38228373600">Taras Iakymchuk</a></p>
<p>Group for Processing and Digital Design, Universitat de Valencia, Burjassot, Spain</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085356350">Jose Vicente Frances-Villora</a></p>
<p>Group for Processing and Digital Design, Universitat de Valencia, Burjassot, Spain</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/38256528200">Manuel Bataller-Mompeán</a></p>
<p>Group for Processing and Digital Design, Universitat de Valencia, Burjassot, Spain</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37085364609">Alfredo Rosado-Muñoz</a></p>
<p>Group for Processing and Digital Design, Universitat de Valencia, Burjassot, Spain</p>
<h2 id="Abstract-21"><a href="#Abstract-21" class="headerlink" title="Abstract"></a>Abstract</h2><p>New chips for machine learning applications appear, they are tuned for a specific topology, being efficient by using highly parallel designs at the cost of high power or large complex devices. However, the computational demands of deep neural networks require flexible and efficient hardware architectures able to fit different applications, neural network types, number of inputs, outputs, layers, and units in each layer, making the migration from software to hardware easy. This paper describes novel hardware implementing any feedforward neural network (FFNN): multilayer perceptron, autoencoder, and logistic regression. The architecture admits an arbitrary input and output number, units in layers, and a number of layers. The hardware combines matrix algebra concepts with serial-parallel computation. It is based on a systolic ring of neural processing elements (NPE), only requiring as many NPEs as neuron units in the largest layer, no matter the number of layers. The use of resources grows linearly with the number of NPEs. This versatile architecture serves as an accelerator in real-time applications and its size does not affect the system clock frequency. Unlike most approaches, a single activation function block (AFB) for the whole FFNN is required. Performance, resource usage, and accuracy for several network topologies and activation functions are evaluated. The architecture reaches 550 MHz clock speed in a Virtex7 FPGA. The proposed implementation uses 18-bit fixed point achieving similar classification performance to a floating point approach. A reduced weight bit size does not affect the accuracy, allowing more weights in the same memory. Different FFNN for Iris and MNIST datasets were evaluated and, for a real-time application of abnormal cardiac detection, a ${\hspace{-0.112em}\times \hspace{-0.112em}}256$ acceleration was achieved. The proposed architecture can perform up to 1980 Giga operations per second (GOPS), implementing the multilayer FFNN of up to 3600 neurons per layer in a single chip. The architecture can be extended to bigger capacity devices or multi-chip by the simple NPE ring extension.</p>
<p>出现了用于机器学习应用的新芯片，它们针对特定拓扑进行了调整，通过以高功率或大型复杂设备为代价使用高度并行的设计来提高效率。然而，深度神经网络的计算需求需要灵活高效的硬件架构，能够适应不同的应用、神经网络类型、输入、输出、层数和每层单元的数量，从而使从软件到硬件的迁移变得容易。本文描述了实现任何前馈神经网络 (FFNN) 的新型硬件：多层感知器、自动编码器和逻辑回归。该体系结构允许任意输入和输出数量、层中的单元和层数。硬件将矩阵代数概念与串行-并行计算相结合。它基于神经处理元件 (NPE) 的收缩环，只需要与最大层中的神经元单元一样多的 NPE，无论层数如何。资源的使用随着 NPE 的数量线性增长。这种通用架构可用作实时应用程序中的加速器，其大小不会影响系统时钟频率。与大多数方法不同，整个 FFNN 需要单个激活功能块 (AFB)。评估了几种网络拓扑和激活函数的性能、资源使用和准确性。该架构在 Virtex7 FPGA 中达到 550 MHz 时钟速度。建议的实现使用 18 位定点实现与浮点方法相似的分类性能。减小的权重位大小不会影响准确性，从而在同一内存中允许更多的权重。评估了 Iris 和 MNIST 数据集的不同 FFNN，对于异常心脏检测的实时应用，实现了 ${\hspace{-0.112em}\times \hspace{-0.112em}}256$ 的加速度。所提出的架构可以执行高达每秒 1980 次 Giga 操作 (GOPS)，在单个芯片中实现每层高达 3600 个神经元的多层 FFNN。通过简单的 NPE 环扩展，该架构可以扩展到更大容量的设备或多芯片。</p>
<h2 id="Keywords-Index-Terms-20"><a href="#Keywords-Index-Terms-20" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-21"><a href="#Comments-21" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-21"><a href="#References-21" class="headerlink" title="References"></a>References</h2><h1 id="Paper9"><a href="#Paper9" class="headerlink" title="Paper9"></a>Paper9</h1><h2 id="Published-in-22"><a href="#Published-in-22" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Renato J. Cintra, Stefan Duffner, Christophe Garcia, André Leite, “Low-Complexity Approximate Convolutional Neural Networks”, <em>Neural Networks and Learning Systems IEEE Transactions on</em>, vol. 29, no. 12, pp. 5981-5992, 2018.</p>
<h2 id="Title-22"><a href="#Title-22" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8334697">Low-Complexity Approximate Convolutional Neural Networks</a></p>
<h2 id="Authors-22"><a href="#Authors-22" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37297800100">Renato J. Cintra</a></p>
<p>Signal Processing Group, Universidade Federal de Pernambuco, Recife, Brazil</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37565697300">Stefan Duffner</a></p>
<p>Université de Lyon, CNRS, INSA-Lyon, LIRIS, UMR5205, Villeurbanne, France</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37269224300">Christophe Garcia</a></p>
<p>Université de Lyon, CNRS, INSA-Lyon, LIRIS, UMR5205, Villeurbanne, France</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086523490">André Leite</a></p>
<p>Signal Processing Group, Universidade Federal de Pernambuco, Recife, Brazil</p>
<h2 id="Abstract-22"><a href="#Abstract-22" class="headerlink" title="Abstract"></a>Abstract</h2><p>In this paper, we present an approach for minimizing the computational complexity of the trained convolutional neural networks (ConvNets). The idea is to approximate all elements of a given ConvNet and replace the original convolutional filters and parameters (pooling and bias coefficients; and activation function) with an efficient approximations capable of extreme reductions in computational complexity. Low-complexity convolution filters are obtained through a binary (zero and one) linear programming scheme based on the Frobenius norm over sets of dyadic rationals. The resulting matrices allow for multiplication-free computations requiring only addition and bit-shifting operations. Such low-complexity structures pave the way for low power, efficient hardware designs. We applied our approach on three use cases of different complexities: 1) a “light” but efficient ConvNet for face detection (with around 1000 parameters); 2) another one for hand-written digit classification (with more than 180000 parameters); and 3) a significantly larger ConvNet: AlexNet with ≈1.2 million matrices. We evaluated the overall performance on the respective tasks for different levels of approximations. In all considered applications, very low-complexity approximations have been derived maintaining an almost equal classification performance.</p>
<p>在本文中，我们提出了一种最小化训练卷积神经网络 (ConvNets) 计算复杂度的方法。这个想法是近似给定 ConvNet 的所有元素，并用能够极大降低计算复杂性的有效近似值替换原始卷积滤波器和参数（池化和偏置系数；以及激活函数）。低复杂度卷积滤波器是通过基于二元有理数集的 Frobenius 范数的二元（零和一）线性规划方案获得的。结果矩阵允许只需要加法和位移操作的无乘法计算。这种低复杂度的结构为低功耗、高效的硬件设计铺平了道路。我们将我们的方法应用于三个不同复杂性的用例：1）用于人脸检测的“轻量级”但高效的 ConvNet（具有大约 1000 个参数）； 2）另一种手写数字分类（180000多个参数）；和 3) 一个明显更大的 ConvNet：AlexNet，具有约 120 万个矩阵。我们针对不同的近似水平评估了各个任务的整体性能。在所有考虑的应用程序中，已经导出了非常低复杂度的近似值，并保持了几乎相同的分类性能。</p>
<h2 id="Keywords-Index-Terms-21"><a href="#Keywords-Index-Terms-21" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><h2 id="Comments-22"><a href="#Comments-22" class="headerlink" title="Comments"></a>Comments</h2><p>问题不大</p>
<h2 id="References-22"><a href="#References-22" class="headerlink" title="References"></a>References</h2><h1 id="Paper8-1108"><a href="#Paper8-1108" class="headerlink" title="Paper8-1108"></a>Paper8-1108</h1><h2 id="Published-in-23"><a href="#Published-in-23" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://academic.oup.com/nar">Nucleic Acids Research</a> [Oxford Academic]</p>
<p>Volume 46, Issue 6, 6 April 2018, Page e33,</p>
<h2 id="Title-23"><a href="#Title-23" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://academic.oup.com/nar/article/46/6/e33/4791133?login=true">HipMCL: a high-performance parallel implementation of the Markov clustering algorithm for large-scale networks - FREE</a></p>
<p>HipMCL：大规模网络马尔可夫聚类算法的高性能并行实现 - 免费</p>
<h2 id="Authors-23"><a href="#Authors-23" class="headerlink" title="Authors"></a>Authors</h2><p>Ariful Azad, Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720-8150, USA 劳伦斯伯克利国家实验室计算研究部</p>
<p>Georgios A Pavlopoulos, DOE Joint Genome Institute, Lawrence Berkeley National Laboratory, 2800 Mitchell Drive, Walnut Creek, CA 94598, USA 美国能源部联合基因组研究所，劳伦斯伯克利国家实验室</p>
<p>Christos A Ouzounis, Biological Computation &amp; Process Laboratory, Chemical Process &amp; Energy Resources Institute, Centre for Research &amp; Technology Hellas, Thessalonica 57001, Greece 生物计算与过程实验室，化学过程与能源研究所，希腊研究与技术中心，希腊塞萨洛尼卡</p>
<p>Nikos C Kyrpides, DOE Joint Genome Institute, Lawrence Berkeley National Laboratory, 2800 Mitchell Drive, Walnut Creek, CA 94598, USA 美国能源部联合基因组研究所，劳伦斯伯克利国家实验室</p>
<p>Aydin Buluç, Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720-8150, USA and Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA 94720, USA 劳伦斯伯克利国家实验室计算研究部；加州大学伯克利分校电气工程与计算机科学系</p>
<h2 id="Abstract-23"><a href="#Abstract-23" class="headerlink" title="Abstract"></a>Abstract</h2><p>Biological networks capture structural or functional properties of relevant entities such as molecules, proteins or genes. Characteristic examples are gene expression networks or protein–protein interaction networks, which hold information about functional affinities or structural similarities. Such networks have been expanding in size due to increasing scale and abundance of biological data. While various clustering algorithms have been proposed to find highly connected regions, Markov Clustering (MCL) has been one of the most successful approaches to cluster sequence similarity or expression networks. Despite its popularity, MCL’s scalability to cluster large datasets still remains a bottleneck due to high running times and memory demands. Here, we present High-performance MCL (HipMCL), a parallel implementation of the original MCL algorithm that can run on distributed-memory computers. We show that HipMCL can efficiently utilize 2000 compute nodes and cluster a network of ∼70 million nodes with ∼68 billion edges in ∼2.4 h. By exploiting distributed-memory environments, HipMCL clusters large-scale networks several orders of magnitude faster than MCL and enables clustering of even bigger networks. HipMCL is based on MPI and OpenMP and is freely available under a modified BSD license.</p>
<p>生物网络捕获相关实体（例如分子、蛋白质或基因）的结构或功能特性。典型的例子是基因表达网络或蛋白质-蛋白质相互作用网络，它们保存有关功能亲和力或结构相似性的信息。由于生物数据的规模和丰富性不断增加，此类网络的规模一直在扩大。虽然已经提出了各种聚类算法来寻找高度连接的区域，但马尔可夫聚类 (MCL) 一直是对序列相似性或表达网络进行聚类的最成功的方法之一。尽管它很受欢迎，但由于高运行时间和内存需求，<strong>MCL 对大型数据集进行集群的可扩展性仍然是一个瓶颈</strong>。在这里，我们展示了高性能 MCL (HipMCL)，它是原始 MCL 算法的并行实现，可以在分布式内存计算机上运行。我们表明 HipMCL 可以有效地利用 2000 个计算节点，并在 2.4 小时内将一个由 7000 万个节点和 680 亿条边组成的网络聚集在一起。通过利用分布式内存环境，HipMCL 对大规模网络的集群比 MCL 快几个数量级，并支持对更大网络的集群。 HipMCL 基于 MPI 和 OpenMP，<a target="_blank" rel="noopener" href="https://bitbucket.org/azadcse/hipmcl/">可在修改后的 BSD 许可下免费获得</a>。</p>
<h2 id="Keywords"><a href="#Keywords" class="headerlink" title="Keywords"></a>Keywords</h2><p>No keywords</p>
<h2 id="Comments-23"><a href="#Comments-23" class="headerlink" title="Comments"></a>Comments</h2><p>“By exploiting distributed-memory environments, <a target="_blank" rel="noopener" href="https://bitbucket.org/azadcse/hipmcl/">HipMCL</a> clusters large-scale networks <strong>several orders of magnitude faster</strong> than MCL and enables clustering of even bigger networks. “</p>
<p>“震惊地望着虚空上的残影。<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/萧炎">萧炎</a>半晌无语，他没想到。<a target="_blank" rel="noopener" href="https://bitbucket.org/azadcse/hipmcl/">HipMCL</a>的速度，竟然恐怖如斯。”</p>
<hr>
<h1 id="Paper7-1107"><a href="#Paper7-1107" class="headerlink" title="Paper7-1107"></a>Paper7-1107</h1><h2 id="Published-in-24"><a href="#Published-in-24" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://journals.sagepub.com/home/hpc">The International Journal of High Performance Computing Applications</a></p>
<h2 id="Title-24"><a href="#Title-24" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://journals.sagepub.com/doi/abs/10.1177/1094342011403516">The Combinatorial BLAS: design, implementation, and applications</a></p>
<p><a target="_blank" rel="noopener" href="http://nidhogg.cs.ucsb.edu/research/tech_reports/reports/2010-18.pdf">Download</a></p>
<h2 id="Authors-24"><a href="#Authors-24" class="headerlink" title="Authors"></a>Authors</h2><p>Aydın Buluc: High Performance Computing Research Lawrence Berkeley National Laboratory</p>
<p>高性能计算研究劳伦斯伯克利国家实验室</p>
<p> 1 Cyclotron Road, Berkeley, CA 94720 abuluc@lbl.gov</p>
<p>John R. Gilbert: Department of Computer Science University of California, Santa Barbara </p>
<p>加州大学圣巴巴拉分校计算机科学系</p>
<p>Santa Barbara, CA 93106-5110 gilbert@cs.ucsb.edu</p>
<h2 id="Abstract-24"><a href="#Abstract-24" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper presents a scalable high-performance software library to be used for graph analysis and data mining. Large combinatorial graphs appear in many applications of high-performance computing, including computational biology, informatics, analytics, web search, dynamical systems, and sparse matrix methods. Graph computations are difficult to parallelize using traditional approaches due to their irregular nature and low operational intensity. Many graph computations, however, contain sufficient coarse-grained parallelism for thousands of processors, which can be uncovered by using the right primitives. We describe the parallel Combinatorial BLAS, which consists of a small but powerful set of linear algebra primitives specifically targeting graph and data mining applications. We provide an extensible library interface and some guiding principles for future development. The library is evaluated using two important graph algorithms, in terms of both performance and ease-of-use. The scalability and raw performance of the example applications, using the Combinatorial BLAS, are unprecedented on distributed memory clusters.</p>
<p>本文提出了一个<strong>可扩展的</strong>高性能软件库，用于图形分析和数据挖掘。大型组合图出现在高性能计算的许多应用中，包括计算生物学、信息学、分析学、网络搜索、动力系统和稀疏矩阵方法。由于其不规则性和<strong>低操作强度</strong>，图计算难以使用传统方法并行化。然而，许多图计算包含足够的用于数千个处理器的粗粒度并行性，这可以通过使用正确的primitives来发现。我们描述了并行组合 BLAS，它由一组小而强大的线性代数原语(primitives)组成，专门针对图和数据挖掘应用程序。我们提供了一个可扩展的库接口和一些未来发展的指导原则。在性能和易用性方面，该库使用两种重要的图形算法进行评估。使用组合 BLAS 的示例应用程序的可扩展性和原始性能在分布式内存集群上是前所未有的。</p>
<h2 id="Keywords-1"><a href="#Keywords-1" class="headerlink" title="Keywords"></a>Keywords</h2><p>Mathematical Software, Graph Analysis, Software Framework, Sparse Matrices, Combinatorial Scientific Computing.</p>
<p>数学软件、图形分析、软件框架、稀疏矩阵、组合科学计算。</p>
<p><a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Betweenness+Centrality">Betweenness centrality</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Combinatorial+BLAS">combinatorial BLAS</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Combinatorial+Scientific+Computing">combinatorial scientific computing</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Graph+Analysis">graph analysis</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Markov+Clustering">Markov clustering</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Mathematical+Software">mathematical software</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Parallel+Graph+Library">parallel graph library</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Software+Framework">software framework</a>, <a target="_blank" rel="noopener" href="https://journals.sagepub.com/keyword/Sparse+Matrices">sparse matrices</a></p>
<p>介数中心性、组合BLAS、组合科学计算、图分析、马尔可夫聚类、数学软件、并行图库、软件框架、稀疏矩阵</p>
<h2 id="Comments-24"><a href="#Comments-24" class="headerlink" title="Comments"></a>Comments</h2><p>看不懂。</p>
<hr>
<h1 id="Paper6-1105"><a href="#Paper6-1105" class="headerlink" title="Paper6-1105"></a>Paper6-1105</h1><h2 id="Published-in-25"><a href="#Published-in-25" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://link.springer.com/conference/para">International Workshop on Applied Parallel Computing</a></p>
<p>PARA 2006: <a target="_blank" rel="noopener" href="https://link.springer.com/book/10.1007/978-3-540-75755-9">Applied Parallel Computing. State of the Art in Scientific Computing</a> pp 260-269| <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-540-75755-9_32#citeas">Cite as</a></p>
<h2 id="Title-25"><a href="#Title-25" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-540-75755-9_32">High-Performance Graph Algorithms from Parallel Sparse Matrices</a> </p>
<p>基于(from?)并行稀疏矩阵的高性能图算法</p>
<h2 id="Authors-25"><a href="#Authors-25" class="headerlink" title="Authors"></a>Authors</h2><p>John R. Gilbert<sup>1</sup>, Steve Reinhardt<sup>2</sup>, and Viral B. Shah<sup>1</sup></p>
<ol>
<li>1.University of California, Dept. of Computer Science, Harold Frank Hall, Santa Barbara, CA 93106USA</li>
<li>2.Silicon Graphics Inc. </li>
</ol>
<h2 id="Abstract-25"><a href="#Abstract-25" class="headerlink" title="Abstract"></a>Abstract</h2><p>Large-scale computation on graphs and other discrete structures is becoming increasingly important in many applications, including computational biology, web search, and knowledge discovery. High-performance combinatorial computing is an infant field, in sharp contrast with numerical scientific computing.</p>
<p>We argue that many of the tools of high-performance numerical computing – in particular, parallel algorithms and data structures for computation with sparse matrices – can form the nucleus of a robust infrastructure for parallel computing on graphs. We demonstrate this with an implementation of a graph analysis benchmark using the sparse matrix infrastructure in Star-P, our parallel dialect of the Matlab programming language.</p>
<p>图和其他离散结构的大规模计算在许多应用中变得越来越重要，包括计算生物学、网络搜索和知识发现。 高性能组合计算是一个新生领域，与数值科学计算形成鲜明对比。</p>
<p>我们认为，许多高性能数值计算工具——特别是用于稀疏矩阵计算的并行算法和数据结构——可以构成图并行计算的强大基础架构的核心。 我们通过使用 Star-P（我们的 Matlab 编程语言的并行语言）中的稀疏矩阵基础结构实现图分析基准来证明这一点。</p>
<h2 id="Keywords-Index-Terms-22"><a href="#Keywords-Index-Terms-22" class="headerlink" title="Keywords / Index Terms"></a>Keywords / Index Terms</h2><p>Sparse Matrix, Input Graph, Large Graph, Sparse Matrice, Basic Design Principle</p>
<h2 id="Comments-25"><a href="#Comments-25" class="headerlink" title="Comments"></a>Comments</h2><p>被<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X">Performance optimization, modeling and analysis of sparse matrix-matrix products on multi-core and many-core processors</a>引用。</p>
<hr>
<h1 id="Paper5-1104"><a href="#Paper5-1104" class="headerlink" title="Paper5-1104"></a>Paper5-1104</h1><h2 id="Published-in-26"><a href="#Published-in-26" class="headerlink" title="Published in"></a>Published in</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/journal/parallel-computing">Parallel Computing</a></p>
<h2 id="Title-26"><a href="#Title-26" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X">Performance optimization, modeling and analysis of sparse matrix-matrix products on multi-core and many-core processors</a></p>
<p>稀疏矩阵乘法在多核和众核处理器上的性能优化、建模与分析</p>
<h2 id="Authors-26"><a href="#Authors-26" class="headerlink" title="Authors"></a>Authors</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X#!">Yusuke Nagasaka</a>: Tokyo Institute of Technology, Tokyo, Japan【长坂雄介？，东京工业大学】</li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X#!">Satoshi Matsuoka</a>: RIKEN Center for Computational Science, Kobe, Japan【松冈聪？，RIKEN 计算科学中心，日本神户】</li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X#!">Ariful Azad</a>: Indiana University, Bloomington, Indiana, USA【阿里夫·阿扎德，印第安纳大学，布卢明顿，印第安纳州，美国】 </li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S016781911930136X#!">Aydın Buluç</a>: Lawrence Berkeley National Laboratory, Berkeley, California, USA【艾登布卢奇，劳伦斯伯克利国家实验室，美国加利福尼亚州伯克利】</li>
</ul>
<h2 id="Abstract-26"><a href="#Abstract-26" class="headerlink" title="Abstract"></a>Abstract</h2><p>Sparse matrix-matrix multiplication (SpGEMM) is a computational primitive that is widely used in areas ranging from traditional <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/engineering/numerical-application">numerical applications</a> to recent <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/big-data-analysis">big data analysis</a> and <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/machine-learning">machine learning</a>. Although many SpGEMM algorithms have been proposed, hardware specific optimizations for multi- and many-core processors are lacking and a detailed analysis of their performance under various use cases and matrices is not available. We firstly identify and mitigate multiple bottlenecks with <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/memory-management">memory management</a> and thread scheduling on <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/intel-xeon-phi">Intel Xeon Phi</a> (Knights Landing or KNL). <strong>Specifically targeting many-core processors</strong>, we develop a hash-table-based algorithm and optimize a heap-based shared-memory SpGEMM algorithm. We examine their performance together with other publicly available codes. Different from the literature, our evaluation also includes use cases that are representative of real <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/graph-algorithms">graph algorithms</a>, such as multi-source breadth-first search or triangle counting. <strong>Our hash-table and heap-based algorithms are showing significant speedups from libraries in the majority of the cases</strong> while different algorithms dominate the other scenarios with different matrix size, <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/sparsity">sparsity</a>, compression factor and operation type. We wrap up in-depth evaluation results and make a recipe to give the best SpGEMM algorithm for target scenario. We build the performance model for hash-table and heap-based algorithms, which supports the recipe. A critical finding is that hash-table-based SpGEMM gets a significant performance boost if the nonzeros are not required to be sorted within each row of the output matrix. Finally, we integrate our implementations into a large-scale protein clustering code named HipMCL, accelerating its SpGEMM kernel by up to 10X and achieving an overall performance boost for the whole HipMCL application by 2.6X.</p>
<p>稀疏矩阵-矩阵乘法 (SpGEMM) 是一种计算原语（computational primitive），广泛应用于从传统<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/engineering/numeric-application">数值应用</a> 到最近的<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/big-data-analysis">大数据分析</a>和<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/machine-learning">机器学习</a>.尽管已经提出了许多 SpGEMM 算法，但缺乏针对多核和众核处理器的硬件特定优化，并且没有对其在各种测试样例下的性能进行详细分析。我们首先通过<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/memory-management">内存管理</a>和<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/intel-xeon-phi">Intel Xeon Phi</a>上的线程调度来识别和缓解多个瓶颈（Knights Landing 或 KNL）。<strong>我们专门针对众核处理器</strong>，开发了基于哈希表的算法并优化了基于堆的共享内存 SpGEMM 算法。我们与其他公开的代码一起检查其性能。与文献不同，我们的评估还包括代表真实<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/graph-algorithms">图算法</a>的用例，例如多源广度优先搜索或三角形计数。在大多数库中，我们的哈希表和基于堆的算法在可以显著加速，而不同的算法主导了具有不同矩阵大小，<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer -science/sparsity">稀疏程度</a>、压缩因子和操作类型的其他场景（？）。我们总结了深入的评估结果并制定了一个方法，为目标场景提供最佳 SpGEMM 算法。我们为支持这个方法的哈希表和基于堆的算法构建了性能模型。一个重要的发现是，如果不需要在输出矩阵的每一行内对非零值进行排序，那么基于哈希表的 SpGEMM 将获得显着的性能提升。最后，我们将我们的实现集成到一个名为 HipMCL 的大规模蛋白质聚类代码中，将其 SpGEMM 内核的速度提高了 10 倍，并将整个 HipMCL 应用程序的整体性能提高了 2.6 倍。</p>
<h2 id="Keywords-2"><a href="#Keywords-2" class="headerlink" title="Keywords"></a>Keywords</h2><p>Sparse matrix</p>
<p>SpGEMM</p>
<p>Intel KNL</p>
<h2 id="Comments-26"><a href="#Comments-26" class="headerlink" title="Comments"></a>Comments</h2><p>需要好好看。</p>
<p>简而言之，multi-core是多核，many-core是众核。</p>
<p><img src="/2021/10/29/papers/1.png" alt="cores"></p>
<hr>
<h1 id="Paper4-1106"><a href="#Paper4-1106" class="headerlink" title="Paper4-1106"></a>Paper4-1106</h1><h2 id="Published-in-27"><a href="#Published-in-27" class="headerlink" title="Published in"></a>Published in</h2><p>cite <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/537127/citations?tabFilter=papers#citations">Sigmoid generators for neural computing using piecewise approximations</a></p>
<p>Ahmed M. Abdelsalam, J. M. Pierre Langlois, F. Cheriet, “A Configurable FPGA Implementation of the Tanh Function Using DCT Interpolation”, <em>Field-Programmable Custom Computing Machines (FCCM) 2017 IEEE 25th Annual International Symposium on</em>, pp. 168-171, 2017.</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/conhome/7964000/proceeding">2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</a></p>
<h2 id="Title-27"><a href="#Title-27" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7966673">A Configurable FPGA Implementation of the Tanh Function Using DCT Interpolation</a></p>
<h2 id="Authors-27"><a href="#Authors-27" class="headerlink" title="Authors"></a>Authors</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37086113448">Ahmed M. Abdelsalam</a></p>
<p>Computer and Software Engineering Department, Polytechnique Montreal, Montreal, Canada</p>
<p>加拿大蒙特利尔理工学院计算机与软件工程系</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37276914800">J. M. Pierre Langlois</a></p>
<p>Computer and Software Engineering Department, Polytechnique Montreal, Montreal, Canada</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/author/37284363700">F. Cheriet</a></p>
<p>Computer and Software Engineering Department, Polytechnique Montreal, Montreal, Canada</p>
<h2 id="Abstract-27"><a href="#Abstract-27" class="headerlink" title="Abstract"></a>Abstract</h2><p>Efficient implementation of non-linear activation functions is essential to the implementation of deep learning models on FPGAs. We introduce such an implementation based on the Discrete Cosine Transform Interpolation Filter (DCTIF). The proposed interpolation architecture combines simple arithmetic operations on the stored samples of the hyperbolic tangent function and on input data. It achieves almost 3× better precision than previous works while using a similar amount computational resources and a small amount of memory. Various combinations of DCTIF parameters can be chosen to trade off the accuracy and the overall circuit complexity of the tanh function. In one case, the proposed architecture approximates the hyperbolic tangent activation function with 0.004 maximum error while requiring only 1.45 kbits BRAM memory and 21 LUTs of a Virtex-7 FPGA.</p>
<p>非线性激活函数的有效实现对于在 FPGA 上实现深度学习模型至关重要。 我们介绍了基于<strong>离散余弦变换插值滤波器 (DCTIF) 的这种实现</strong>。 所提出的插值架构结合了对双曲正切函数的存储样本和输入数据的简单算术运算。 在使用类似数量的计算资源和少量内存的情况下，它的精度比以前的工作高出近 3 倍。 可以选择 DCTIF 参数的各种组合来权衡 tanh 函数的准确性和整体电路复杂性。 在一种情况下，所提出的架构以 0.004 的最大误差逼近双曲正切激活函数，同时仅需要 1.45 kbits BRAM 存储器和 Virtex-7 FPGA 的 21 个 LUT。</p>
<h2 id="Keywords-3"><a href="#Keywords-3" class="headerlink" title="Keywords"></a>Keywords</h2><ul>
<li>IEEE Keywords <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Interpolation&amp;newsearch=true">Interpolation</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Field programmable gate arrays&amp;newsearch=true">Field programmable gate arrays</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Table lookup&amp;newsearch=true">Table lookup</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Discrete cosine transforms&amp;newsearch=true">Discrete cosine transforms</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Decoding&amp;newsearch=true">Decoding</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Hardware&amp;newsearch=true">Hardware</a></li>
<li>INSPEC: Controlled Indexing <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:circuit complexity&amp;newsearch=true">circuit complexity</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:discrete cosine transforms&amp;newsearch=true">discrete cosine transforms</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:field programmable gate arrays&amp;newsearch=true">field programmable gate arrays</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:interpolation&amp;newsearch=true">interpolation</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:learning .LB.artificial intelligence.RB.&amp;newsearch=true">learning (artificial intelligence)</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:transfer functions&amp;newsearch=true">transfer functions</a></li>
<li>INSPEC: Non-Controlled Indexing <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:configurable FPGA implementation&amp;newsearch=true">configurable FPGA implementation</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:tanh function&amp;newsearch=true">tanh function</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:nonlinear activation functions&amp;newsearch=true">nonlinear activation functions</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:deep learning models&amp;newsearch=true">deep learning models</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:discrete cosine transform interpolation filter&amp;newsearch=true">discrete cosine transform interpolation filter</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:DCTIF&amp;newsearch=true">DCTIF</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:circuit complexity&amp;newsearch=true">circuit complexity</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:BRAM memory&amp;newsearch=true">BRAM memory</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:LUT&amp;newsearch=true">LUT</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Virtex-7 FPGA&amp;newsearch=true">Virtex-7 FPGA</a></li>
<li>Author Keywords <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Deep Neural Network .LB.DNN.RB.&amp;newsearch=true">Deep Neural Network (DNN)</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Embedded FPGA&amp;newsearch=true">Embedded FPGA</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Deep learning&amp;newsearch=true">Deep learning</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Activation function&amp;newsearch=true">Activation function</a>, <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/search/searchresult.jsp?matchBoolean=true&amp;queryText=&quot;Index Terms&quot;:Hyperbolic tangent&amp;newsearch=true">Hyperbolic tangent</a></li>
</ul>
<h2 id="Comments-27"><a href="#Comments-27" class="headerlink" title="Comments"></a>Comments</h2><p>方法不同。</p>
<p>从Abstract可以看出来作者好像也很难权衡参数到底怎么配。和我挺像的。。。</p>
<hr>
<h1 id="paper3-1103"><a href="#paper3-1103" class="headerlink" title="paper3-1103"></a>paper3-1103</h1><h2 id="Published-in-28"><a href="#Published-in-28" class="headerlink" title="Published in"></a>Published in</h2><p>TCAS-I</p>
<p>IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 68, NO. 8, AUGUST 2021</p>
<h2 id="Title-28"><a href="#Title-28" class="headerlink" title="Title"></a>Title</h2><p>Low-Complexity High-Precision Method and Architecture for Computing the Logarithm of Complex Numbers</p>
<h2 id="Authors-28"><a href="#Authors-28" class="headerlink" title="Authors"></a>Authors</h2><p>Hui Chen , Graduate Student Member, IEEE, Zongguang Yu , Zhonghai Lu , Senior Member, IEEE, Yuxiang Fu , Member, IEEE, and Li Li , Member, IEEE</p>
<p>Hui Chen, Zongguang Yu, Yonggang Zhang, Yuxiang Fu, and Li Li are with the School of Electronic Science and Engineering, Nanjing University, Nanjing 210093, China (e-mail: yuxiangfu@nju.edu.cn; lili@nju.edu.cn).</p>
<p>Zhonghai Lu is with the Department of Electrical Engineering, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, 16440 Stockholm, Sweden.</p>
<h2 id="Abstract-28"><a href="#Abstract-28" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper proposes a low-complexity method and architecture to compute the logarithm of complex numbers based on coordinate rotation digital computer (CORDIC). Our method takes advantage of the vector mode of circular CORDIC and hyperbolic CORDIC, which only needs shift-add operations in its hardware implementation. Our architecture has lower design complexity and higher performance compared with conventional architectures. Through software simulation, we show that this method can achieve high precision for logarithm computation, reaching the relative error of  $10^{−7}$. Finally, we design and implementation example circuit under TSMC 28nm CMOS technology. According to the synthesis report, our architecture has smaller area, lower power consumption, higher precision and wider operation range compared with the alternative architectures.</p>
<p>本文提出了一种基于CORDIC方法计算复数对数的低复杂度方法和架构。我们的方法利用了循环 CORDIC 和双曲线 CORDIC 的矢量模式的长处，在硬件实现中只需进行移位和加法运算。与传统架构相比，我们的架构具有更低的设计复杂性和更高的性能。通过软件仿真，我们表明该方法可以实现对数计算的高精度，达到$10^{−7}$的<strong>相对误差</strong>。 最后，我们在TSMC 28nm CMOS技术下设计并实现了一个示例电路。根据综合报告，我们的架构与替代架构相比具有更小的面积、更低的功耗、更高的精度和更宽的操作范围。</p>
<h2 id="Comments-28"><a href="#Comments-28" class="headerlink" title="Comments"></a>Comments</h2><p>他们的相对误差非常低，但是最大绝对误差是多少？</p>
<hr>
<h1 id="paper2-1102"><a href="#paper2-1102" class="headerlink" title="paper2-1102"></a>paper2-1102</h1><h2 id="Published-in-29"><a href="#Published-in-29" class="headerlink" title="Published in"></a>Published in</h2><p>TCAS-I</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8919">IEEE Transactions on Circuits and Systems I: Regular Papers</a> ( Early Access )</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9585319">Paper LINK</a></p>
<h2 id="Title-29"><a href="#Title-29" class="headerlink" title="Title"></a>Title</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9585319">Memory-Efficient CNN Accelerator Based on Interlayer Feature Map Compression</a></p>
<h2 id="Authors-29"><a href="#Authors-29" class="headerlink" title="Authors"></a>Authors</h2><p>Zhuang Shao; Xiaoliang Chen; Li Du; Lei Chen; Yuan Du; Wei Zhuang; Huadong Wei; Chenjia Xie; Zhongfeng Wang</p>
<h2 id="Abstract-29"><a href="#Abstract-29" class="headerlink" title="Abstract"></a>Abstract</h2><p>Existing deep convolutional neural networks (CNNs) generate massive interlayer feature data during network inference. To maintain real-time processing in embedded systems, large on-chip memory is required to buffer the interlayer feature maps. In this paper, we propose an efficient hardware accelerator with an interlayer feature compression technique to significantly reduce the required on-chip memory size and off-chip memory access bandwidth. The accelerator compresses interlayer feature maps through transforming the stored data into frequency domain using hardware-implemented 8×8 discrete cosine transform (DCT). The high-frequency components are removed after the DCT through quantization. Sparse matrix compression is utilized to further compress the interlayer feature maps. The on-chip memory allocation scheme is designed to support dynamic configuration of the feature map buffer size and scratch pad size according to different network-layer requirements. The hardware accelerator combines compression, decompression, and CNN acceleration into one computing stream, achieving minimal compressing and processing delay. A prototype accelerator is implemented on an FPGA platform and also synthesized in TSMC 28-nm COMS technology. It achieves 403GOPS peak throughput and 1.4× 3.3× interlayer feature map reduction by adding light hardware area overhead, making it a promising hardware accelerator for intelligent IoT devices.</p>
<p>现有的深度卷积神经网络 (CNN) 在网络推理过程中会生成大量的层间特征数据。为了在嵌入式系统中保持实时处理，需要大量的片上存储器来缓冲层间特征图。在本文中，我们提出了一种具有层间特征压缩技术的高效硬件加速器，以显着减少所需的片上内存大小和片外内存访问带宽。加速器通过使用硬件实现的 8×8 离散余弦变换 (DCT) 将存储的数据转换到频域来压缩层间特征图。高频分量在 DCT 之后通过量化被去除。稀疏矩阵压缩用于进一步压缩层间特征图。片上内存分配方案旨在支持根据不同网络层要求动态配置特征图buffer大小和scratch pad大小。硬件加速器将压缩、解压和CNN加速整合到一个计算流中，实现最小的压缩和处理延迟。原型加速器在 FPGA 平台上实现，并在 TSMC 28-nm COMS 技术中综合。它通过增加很少的硬件区域开销实现了 403GOPS 峰值吞吐量和 1.4×3.3× 层间特征图减少，使其成为智能物联网设备的有前途的硬件加速器。</p>
<h2 id="Index-Terms"><a href="#Index-Terms" class="headerlink" title="Index Terms"></a>Index Terms</h2><p>Deep convolution neural networks, discrete cosine transform, quantization, interlayer feature maps compression.</p>
<p>深度卷积神经网络、离散余弦变换、量化、层间特征图压缩。</p>
<h2 id="Comments-29"><a href="#Comments-29" class="headerlink" title="Comments"></a>Comments</h2><p>iscl的第一篇TCAS-I。</p>
<hr>
<h1 id="paper1-1101"><a href="#paper1-1101" class="headerlink" title="paper1-1101"></a>paper1-1101</h1><h2 id="Published-in-30"><a href="#Published-in-30" class="headerlink" title="Published in"></a>Published in</h2><p>TVLSI。</p>
<p>IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS, VOL. 29, NO. 7, JULY 2021</p>
<h2 id="Title-30"><a href="#Title-30" class="headerlink" title="Title"></a>Title</h2><p>PWL-Based Architecture for the Logarithmic Computation of Floating-Point Numbers</p>
<h2 id="Authors-30"><a href="#Authors-30" class="headerlink" title="Authors"></a>Authors</h2><p>Fei Lyu , Zhelong Mao , Jin Zhang, Yu Wang, and Yuanyong Luo</p>
<p>Fei Lyu and Jin Zhang are with the School of Electronics and Information Engineering, Jinling Institute of Technology, Nanjing 211169, China.</p>
<p>Zhelong Mao and Yu Wang are with the School of Electronics Engineering, Nanjing Xiaozhuang University, Nanjing 211171, China.</p>
<p>Yuanyong Luo is with the Linx Laboratory, <strong>Department of Turing Architecture Design, HiSilicon, Huawei Corporation, </strong>Shenzhen 518129, China.</p>
<h2 id="Abstract-30"><a href="#Abstract-30" class="headerlink" title="Abstract"></a>Abstract</h2><p>In this brief, we propose a logarithmic converter for floating-point numbers based on the piecewise linear (PWL) approximation method. The proposed method is applicable to any customized floating-point format with a mantissa length of 16–23 bits and a maximum absolute error (MAE) larger than $10^{−6}$. The logarithmic function is automatically segmented into several maximal subsections by a software-based segmentation scheme with the restriction of a predefined MAE and a fractional word length for the computing units. Then, we make a tradeoff between the piecewise number and the fractional word length. Based on the results of the segmentor, our design is coded in the Verilog hardware description language. The synthesized results show that our design consumes less area, time, and power without compromising accuracy compared to existing techniques based on the COordinate Rotation Digital Computer (CORDIC) and PWL methods.</p>
<p>在这篇brief中，我们提出了一种基于分段线性 (PWL) 拟合方法的浮点对数转换器。 所提出的方法适用于尾数长度为 16-23 位且最大绝对误差 (MAE) 大于 10-6 的任何自定义浮点格式。 目标拟合对数函数通过软件分段器根据预定义的MAE和小数位数限制自动分成多段。 而后我们在分段数和小数字长之间进行权衡。 基于分段器的输出，我们用Verilog完成了硬件实现。 综合结果表明，与基于CORDIC方法和和 PWL 方法的现有技术相比，我们的设计在不影响精度的情况下具有更低的面积、时延和功耗。</p>
<h2 id="Index-Terms-1"><a href="#Index-Terms-1" class="headerlink" title="Index Terms"></a>Index Terms</h2><p>Logarithmic converter, maximum absolute error (MAE), piecewise linear (PWL) approximation.</p>
<p>对数转换器、最大绝对误差 (MAE)、分段线性 (PWL) 拟合。</p>
<h2 id="Comments-30"><a href="#Comments-30" class="headerlink" title="Comments"></a>Comments</h2><p>5页短文，可以用于学习如何写短paper。</p>
<hr>
<p><img src="/2021/10/29/papers/qiufen.jpg" alt="QiuFen"></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Haoran Geng
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://njughr.github.io/2021/10/29/papers/" title="Pipes">https://njughr.github.io/2021/10/29/papers/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ic/" rel="tag"># ic</a>
              <a href="/tags/cs/" rel="tag"># cs</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/10/25/modelsim/" rel="prev" title="modelsim笔记">
      <i class="fa fa-chevron-left"></i> modelsim笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/10/31/english/" rel="next" title="English Learning">
      English Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95"><span class="nav-number">1.</span> <span class="nav-text">阅读方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E7%AD%94%E8%A6%81%E7%82%B9%E6%80%BB%E7%BB%93"><span class="nav-number">1.1.</span> <span class="nav-text">回答要点总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">1.2.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Template"><span class="nav-number">2.</span> <span class="nav-text">Template</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in"><span class="nav-number">2.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title"><span class="nav-number">2.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors"><span class="nav-number">2.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">2.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms"><span class="nav-number">2.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments"><span class="nav-number">2.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-1"><span class="nav-number">2.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper30-1124"><span class="nav-number">3.</span> <span class="nav-text">Paper30-1124</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-1"><span class="nav-number">3.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-1"><span class="nav-number">3.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-1"><span class="nav-number">3.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-1"><span class="nav-number">3.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-1"><span class="nav-number">3.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-1"><span class="nav-number">3.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-2"><span class="nav-number">3.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper29-1123"><span class="nav-number">4.</span> <span class="nav-text">Paper29-1123</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-2"><span class="nav-number">4.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-2"><span class="nav-number">4.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-2"><span class="nav-number">4.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-2"><span class="nav-number">4.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-2"><span class="nav-number">4.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-2"><span class="nav-number">4.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-3"><span class="nav-number">4.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper28-1113"><span class="nav-number">5.</span> <span class="nav-text">Paper28-1113</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-3"><span class="nav-number">5.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-3"><span class="nav-number">5.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-3"><span class="nav-number">5.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-3"><span class="nav-number">5.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-3"><span class="nav-number">5.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-3"><span class="nav-number">5.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper27-1111"><span class="nav-number">6.</span> <span class="nav-text">Paper27-1111</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-4"><span class="nav-number">6.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-4"><span class="nav-number">6.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-4"><span class="nav-number">6.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-4"><span class="nav-number">6.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-4"><span class="nav-number">6.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-4"><span class="nav-number">6.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-4"><span class="nav-number">6.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper26-1112"><span class="nav-number">7.</span> <span class="nav-text">Paper26-1112</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-5"><span class="nav-number">7.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-5"><span class="nav-number">7.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-5"><span class="nav-number">7.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-5"><span class="nav-number">7.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-5"><span class="nav-number">7.5.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-5"><span class="nav-number">7.6.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper25-1117"><span class="nav-number">8.</span> <span class="nav-text">Paper25-1117</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-6"><span class="nav-number">8.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-6"><span class="nav-number">8.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-6"><span class="nav-number">8.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-6"><span class="nav-number">8.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-5"><span class="nav-number">8.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-6"><span class="nav-number">8.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-6"><span class="nav-number">8.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper24-1110"><span class="nav-number">9.</span> <span class="nav-text">Paper24-1110</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-7"><span class="nav-number">9.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-7"><span class="nav-number">9.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-7"><span class="nav-number">9.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-7"><span class="nav-number">9.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-6"><span class="nav-number">9.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-7"><span class="nav-number">9.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-7"><span class="nav-number">9.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper23-1115"><span class="nav-number">10.</span> <span class="nav-text">Paper23-1115</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-8"><span class="nav-number">10.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-8"><span class="nav-number">10.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-8"><span class="nav-number">10.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-8"><span class="nav-number">10.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-7"><span class="nav-number">10.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-8"><span class="nav-number">10.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-8"><span class="nav-number">10.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper22-1116"><span class="nav-number">11.</span> <span class="nav-text">Paper22-1116</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-9"><span class="nav-number">11.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-9"><span class="nav-number">11.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-9"><span class="nav-number">11.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-9"><span class="nav-number">11.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-8"><span class="nav-number">11.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-9"><span class="nav-number">11.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-9"><span class="nav-number">11.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper21-1118"><span class="nav-number">12.</span> <span class="nav-text">Paper21-1118</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-10"><span class="nav-number">12.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-10"><span class="nav-number">12.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-10"><span class="nav-number">12.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-10"><span class="nav-number">12.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-9"><span class="nav-number">12.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-10"><span class="nav-number">12.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-10"><span class="nav-number">12.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper20-1119"><span class="nav-number">13.</span> <span class="nav-text">Paper20-1119</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-11"><span class="nav-number">13.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-11"><span class="nav-number">13.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-11"><span class="nav-number">13.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-11"><span class="nav-number">13.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-10"><span class="nav-number">13.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-11"><span class="nav-number">13.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-11"><span class="nav-number">13.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper19-1120"><span class="nav-number">14.</span> <span class="nav-text">Paper19-1120</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-12"><span class="nav-number">14.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-12"><span class="nav-number">14.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-12"><span class="nav-number">14.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-12"><span class="nav-number">14.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-11"><span class="nav-number">14.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-12"><span class="nav-number">14.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-12"><span class="nav-number">14.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper18-1121"><span class="nav-number">15.</span> <span class="nav-text">Paper18-1121</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-13"><span class="nav-number">15.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-13"><span class="nav-number">15.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-13"><span class="nav-number">15.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-13"><span class="nav-number">15.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-12"><span class="nav-number">15.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-13"><span class="nav-number">15.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-13"><span class="nav-number">15.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper17-1122"><span class="nav-number">16.</span> <span class="nav-text">Paper17-1122</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-14"><span class="nav-number">16.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-14"><span class="nav-number">16.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-14"><span class="nav-number">16.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-14"><span class="nav-number">16.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-13"><span class="nav-number">16.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-14"><span class="nav-number">16.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-14"><span class="nav-number">16.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper16-1125"><span class="nav-number">17.</span> <span class="nav-text">Paper16-1125</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-15"><span class="nav-number">17.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-15"><span class="nav-number">17.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-15"><span class="nav-number">17.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-15"><span class="nav-number">17.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-14"><span class="nav-number">17.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-15"><span class="nav-number">17.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-15"><span class="nav-number">17.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper15"><span class="nav-number">18.</span> <span class="nav-text">Paper15</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-16"><span class="nav-number">18.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-16"><span class="nav-number">18.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-16"><span class="nav-number">18.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-16"><span class="nav-number">18.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-15"><span class="nav-number">18.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-16"><span class="nav-number">18.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-16"><span class="nav-number">18.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper14"><span class="nav-number">19.</span> <span class="nav-text">Paper14</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-17"><span class="nav-number">19.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-17"><span class="nav-number">19.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-17"><span class="nav-number">19.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-17"><span class="nav-number">19.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-16"><span class="nav-number">19.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-17"><span class="nav-number">19.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-17"><span class="nav-number">19.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper13"><span class="nav-number">20.</span> <span class="nav-text">Paper13</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-18"><span class="nav-number">20.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-18"><span class="nav-number">20.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-18"><span class="nav-number">20.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-18"><span class="nav-number">20.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-17"><span class="nav-number">20.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-18"><span class="nav-number">20.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-18"><span class="nav-number">20.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper12-1109"><span class="nav-number">21.</span> <span class="nav-text">Paper12-1109</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-19"><span class="nav-number">21.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-19"><span class="nav-number">21.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-19"><span class="nav-number">21.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-19"><span class="nav-number">21.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-18"><span class="nav-number">21.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-19"><span class="nav-number">21.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-19"><span class="nav-number">21.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper11"><span class="nav-number">22.</span> <span class="nav-text">Paper11</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-20"><span class="nav-number">22.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-20"><span class="nav-number">22.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-20"><span class="nav-number">22.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-20"><span class="nav-number">22.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-19"><span class="nav-number">22.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-20"><span class="nav-number">22.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-20"><span class="nav-number">22.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper10"><span class="nav-number">23.</span> <span class="nav-text">Paper10</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-21"><span class="nav-number">23.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-21"><span class="nav-number">23.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-21"><span class="nav-number">23.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-21"><span class="nav-number">23.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-20"><span class="nav-number">23.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-21"><span class="nav-number">23.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-21"><span class="nav-number">23.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper9"><span class="nav-number">24.</span> <span class="nav-text">Paper9</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-22"><span class="nav-number">24.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-22"><span class="nav-number">24.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-22"><span class="nav-number">24.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-22"><span class="nav-number">24.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-21"><span class="nav-number">24.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-22"><span class="nav-number">24.6.</span> <span class="nav-text">Comments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-22"><span class="nav-number">24.7.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper8-1108"><span class="nav-number">25.</span> <span class="nav-text">Paper8-1108</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-23"><span class="nav-number">25.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-23"><span class="nav-number">25.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-23"><span class="nav-number">25.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-23"><span class="nav-number">25.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords"><span class="nav-number">25.5.</span> <span class="nav-text">Keywords</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-23"><span class="nav-number">25.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper7-1107"><span class="nav-number">26.</span> <span class="nav-text">Paper7-1107</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-24"><span class="nav-number">26.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-24"><span class="nav-number">26.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-24"><span class="nav-number">26.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-24"><span class="nav-number">26.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-1"><span class="nav-number">26.5.</span> <span class="nav-text">Keywords</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-24"><span class="nav-number">26.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper6-1105"><span class="nav-number">27.</span> <span class="nav-text">Paper6-1105</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-25"><span class="nav-number">27.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-25"><span class="nav-number">27.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-25"><span class="nav-number">27.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-25"><span class="nav-number">27.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-Index-Terms-22"><span class="nav-number">27.5.</span> <span class="nav-text">Keywords &#x2F; Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-25"><span class="nav-number">27.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper5-1104"><span class="nav-number">28.</span> <span class="nav-text">Paper5-1104</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-26"><span class="nav-number">28.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-26"><span class="nav-number">28.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-26"><span class="nav-number">28.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-26"><span class="nav-number">28.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-2"><span class="nav-number">28.5.</span> <span class="nav-text">Keywords</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-26"><span class="nav-number">28.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Paper4-1106"><span class="nav-number">29.</span> <span class="nav-text">Paper4-1106</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-27"><span class="nav-number">29.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-27"><span class="nav-number">29.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-27"><span class="nav-number">29.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-27"><span class="nav-number">29.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keywords-3"><span class="nav-number">29.5.</span> <span class="nav-text">Keywords</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-27"><span class="nav-number">29.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#paper3-1103"><span class="nav-number">30.</span> <span class="nav-text">paper3-1103</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-28"><span class="nav-number">30.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-28"><span class="nav-number">30.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-28"><span class="nav-number">30.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-28"><span class="nav-number">30.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-28"><span class="nav-number">30.5.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#paper2-1102"><span class="nav-number">31.</span> <span class="nav-text">paper2-1102</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-29"><span class="nav-number">31.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-29"><span class="nav-number">31.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-29"><span class="nav-number">31.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-29"><span class="nav-number">31.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Index-Terms"><span class="nav-number">31.5.</span> <span class="nav-text">Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-29"><span class="nav-number">31.6.</span> <span class="nav-text">Comments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#paper1-1101"><span class="nav-number">32.</span> <span class="nav-text">paper1-1101</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Published-in-30"><span class="nav-number">32.1.</span> <span class="nav-text">Published in</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Title-30"><span class="nav-number">32.2.</span> <span class="nav-text">Title</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Authors-30"><span class="nav-number">32.3.</span> <span class="nav-text">Authors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-30"><span class="nav-number">32.4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Index-Terms-1"><span class="nav-number">32.5.</span> <span class="nav-text">Index Terms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comments-30"><span class="nav-number">32.6.</span> <span class="nav-text">Comments</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Haoran Geng"
      src="/images/paperboat.jpg">
  <p class="site-author-name" itemprop="name">Haoran Geng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/njughr" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;njughr" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hrgeng@smail.nju.edu.cn" title="E-Mail → mailto:hrgeng@smail.nju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1444528490&auto=0&height=66"></iframe>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Haoran Geng</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script color='105,7,90' opacity='0.6' zIndex='-1' count='100' src="/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>


  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
